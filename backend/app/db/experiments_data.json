{
  "experiments": [
    {
      "id": 1,
      "title": "银行信贷风险控制",
      "description": "使用机器学习模型预测信用卡客户违约风险",
      "category": "bank",
      "difficulty_level": 3,
      "estimated_duration": 120,
      "prerequisites": "基本的数据分析和机器学习知识",
      "data_source": "台湾信用卡数据集",
      "steps": [
        {
          "id": 1,
          "title": "数据探索",
          "description": "探索信用卡数据集，了解特征和目标变量",
          "example_code": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n# 使用通用字体设置，避免中文字体问题\nimport matplotlib as mpl\nmpl.rcParams['font.family'] = 'DejaVu Sans'\nimport seaborn as sns\nfrom app.services.bank_analysis import bank_service\n\n# 加载数据\ndf = bank_service.load_credit_data()\n\n# 1. 基本统计量\nprint(\"=== Basic Statistics ===\")\nprint(df.describe())\n\n# 2. 数据类型和基本信息\nprint(\"\\n=== Data Types ===\")\nprint(df.info())\n\n# 3. 目标变量分布\ntarget_counts = df['DEFAULT'].value_counts()\nprint(\"\\n=== Target Distribution ===\")\nprint(f\"Non-Default: {target_counts[0]} ({target_counts[0]/len(df)*100:.2f}%)\")\nprint(f\"Default: {target_counts[1]} ({target_counts[1]/len(df)*100:.2f}%)\")\n\n# 4. 异常值统计\nprint(\"\\n=== Outlier Statistics ===\")\nnumeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\nfor col in numeric_cols:\n    Q1 = df[col].quantile(0.25)\n    Q3 = df[col].quantile(0.75)\n    IQR = Q3 - Q1\n    lower_bound = Q1 - 1.5 * IQR\n    upper_bound = Q3 + 1.5 * IQR\n    outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)][col]\n    if not outliers.empty:\n        print(f\"{col} has {len(outliers)} outliers ({len(outliers)/len(df)*100:.2f}%)\")\n\n# 5. 重复值统计\nduplicates = df.duplicated().sum()\nprint(f\"\\n=== Duplicate Statistics ===\")\nprint(f\"Dataset has {duplicates} duplicated rows ({duplicates/len(df)*100:.2f}%)\")\n\n# 6. 缺失值统计\nmissing_values = df.isnull().sum()\nprint(\"\\n=== Missing Value Statistics ===\")\nif missing_values.sum() == 0:\n    print(\"No missing values in the dataset\")\nelse:\n    print(missing_values[missing_values > 0])\n\n# 7. 可视化部分特征分布\nplt.figure(figsize=(15, 10))\n\n# 年龄分布\nplt.subplot(2, 3, 1)\nsns.histplot(df['AGE'], kde=True)\nplt.title('Age Distribution')\n\n# 信用额度分布\nplt.subplot(2, 3, 2)\nsns.histplot(df['LIMIT_BAL'], kde=True)\nplt.title('Credit Limit Distribution')\n\n# 违约率与年龄的关系\nplt.subplot(2, 3, 3)\nsns.boxplot(x='DEFAULT', y='AGE', data=df)\nplt.title('Default Rate vs Age')\nplt.xlabel('Default Status (0=No, 1=Yes)')\n\n# 相关性分析\nplt.figure(figsize=(12, 10))\ncorrelation_matrix = df.corr()\nsns.heatmap(correlation_matrix, annot=False, cmap='coolwarm', linewidths=0.5)\nplt.title('Feature Correlation Heatmap')\nplt.show() "
        },
        {
          "id": 2,
          "title": "数据预处理",
          "description": "处理缺失值和异常值，标准化特征",
          "example_code": "import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\n# 使用通用字体设置，避免中文字体问题\nimport matplotlib as mpl\nmpl.rcParams['font.family'] = 'DejaVu Sans'\nfrom app.services.bank_analysis import bank_service\n\n# 加载数据\ndf = bank_service.load_credit_data()\nprint(\"Original data shape:\", df.shape)\n\n# 创建原始数据的副本\ndf_cleaned = df.copy()\n\n# 1. 检查并处理重复值\nduplicates = df_cleaned.duplicated().sum()\nif duplicates > 0:\n    print(f\"Found {duplicates} duplicate rows, removing...\")\n    df_cleaned = df_cleaned.drop_duplicates()\n\n# 2. 处理缺失值\nmissing_values = df_cleaned.isnull().sum()\nif missing_values.sum() > 0:\n    print(f\"Found {missing_values.sum()} missing values, processing...\")\n    # 对数值型变量用中位数填充\n    numeric_cols = df_cleaned.select_dtypes(include=[np.number]).columns\n    for col in numeric_cols:\n        if df_cleaned[col].isnull().sum() > 0:\n            median_value = df_cleaned[col].median()\n            df_cleaned[col].fillna(median_value, inplace=True)\nelse:\n    print(\"No missing values in the dataset\")\n\n# 3. 处理异常值\nprint(\"\\nProcessing outliers...\")\nnumeric_cols = df_cleaned.select_dtypes(include=[np.number]).columns.tolist()\nnumeric_cols.remove('DEFAULT')  # 不处理目标变量\n\nfor col in numeric_cols:\n    Q1 = df_cleaned[col].quantile(0.25)\n    Q3 = df_cleaned[col].quantile(0.75)\n    IQR = Q3 - Q1\n    lower_bound = Q1 - 1.5 * IQR\n    upper_bound = Q3 + 1.5 * IQR\n    \n    # 统计异常值\n    outliers = df_cleaned[(df_cleaned[col] < lower_bound) | (df_cleaned[col] > upper_bound)][col]\n    \n    if len(outliers) > 0:\n        print(f\"  - {col} has {len(outliers)} outliers ({len(outliers)/len(df_cleaned)*100:.2f}%)\")\n        \n        # 将异常值替换为边界值\n        df_cleaned.loc[df_cleaned[col] < lower_bound, col] = lower_bound\n        df_cleaned.loc[df_cleaned[col] > upper_bound, col] = upper_bound\n\n# 4. 数据标准化\nprint(\"\\nStart data standardization...\")\n# 分离特征和标签\nX = df_cleaned.drop('DEFAULT', axis=1)\ny = df_cleaned['DEFAULT']\n\n# 对数值类型的特征进行标准化\nnumeric_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n\n# 使用StandardScaler进行标准化\nscaler = StandardScaler()\nX_scaled = X.copy()\nX_scaled[numeric_cols] = scaler.fit_transform(X[numeric_cols])\n\nprint(f\"Standardized {len(numeric_cols)} numerical features\")\n\n# 合并回DataFrame\ndf_standardized = pd.concat([X_scaled, y], axis=1)\n\n# 对比标准化前后的描述性统计\nprint(\"\\nStatistics before standardization:\")\nprint(df_cleaned[numeric_cols].describe())\nprint(\"\\nStatistics after standardization:\")\nprint(df_standardized[numeric_cols].describe())\n\nprint(f\"\\nCleaning completed! Original data: {df.shape}, After cleaning: {df_standardized.shape}\")\n\n# 可视化标准化前后\nplt.figure(figsize=(15, 6))\n\n# 标准化前\nplt.subplot(1, 2, 1)\nplt.boxplot(df_cleaned[['LIMIT_BAL', 'AGE', 'BILL_AMT1']])\nplt.title('Before Standardization')\nplt.xticks([1, 2, 3], ['LIMIT_BAL', 'AGE', 'BILL_AMT1'])\n\n# 标准化后\nplt.subplot(1, 2, 2)\nplt.boxplot(df_standardized[['LIMIT_BAL', 'AGE', 'BILL_AMT1']])\nplt.title('After Standardization')\nplt.xticks([1, 2, 3], ['LIMIT_BAL', 'AGE', 'BILL_AMT1'])\n\nplt.tight_layout()\nplt.show() "
        },
        {
          "id": 3,
          "title": "特征工程",
          "description": "创建新特征，选择重要特征",
          "example_code": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.feature_selection import SelectKBest, f_classif\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.decomposition import PCA\n# 使用通用字体设置，避免中文字体问题\nimport matplotlib as mpl\nmpl.rcParams['font.family'] = 'DejaVu Sans'\nfrom app.services.bank_analysis import bank_service\n\n# 加载数据\ndf = bank_service.load_credit_data()\n\n# 创建数据副本\ndf_fe = df.copy()\n\n# 1. 特征创建\nprint(\"Creating new features...\")\n\n# 计算平均账单金额\ndf_fe['AVG_BILL_AMT'] = (df_fe['BILL_AMT1'] + df_fe['BILL_AMT2'] + df_fe['BILL_AMT3'] + \n                         df_fe['BILL_AMT4'] + df_fe['BILL_AMT5'] + df_fe['BILL_AMT6']) / 6\n\n# 计算平均支付金额\ndf_fe['AVG_PAY_AMT'] = (df_fe['PAY_AMT1'] + df_fe['PAY_AMT2'] + df_fe['PAY_AMT3'] + \n                        df_fe['PAY_AMT4'] + df_fe['PAY_AMT5'] + df_fe['PAY_AMT6']) / 6\n\n# 计算账单与支付比率\ndf_fe['BILL_PAY_RATIO'] = df_fe['AVG_PAY_AMT'] / df_fe['AVG_BILL_AMT'].replace(0, 0.01)\n\n# 计算最大延迟\ndelay_cols = ['PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6']\ndf_fe['MAX_DELAY'] = df_fe[delay_cols].max(axis=1)\n\n# 创建延迟标志\ndf_fe['HAS_DELAY'] = (df_fe[delay_cols] > 0).any(axis=1).astype(int)\n\n# 计算信用额度使用率\ndf_fe['CREDIT_UTILIZATION'] = df_fe['BILL_AMT1'] / df_fe['LIMIT_BAL'].replace(0, 0.01)\n\n# 输出新特征的描述性统计\nprint(\"\\nNew features statistics:\")\nnew_features = ['AVG_BILL_AMT', 'AVG_PAY_AMT', 'BILL_PAY_RATIO', 'MAX_DELAY', 'HAS_DELAY', 'CREDIT_UTILIZATION']\nprint(df_fe[new_features].describe())\n\n# 2. 特征交互 (多项式特征)\nprint(\"\\nCreating polynomial features...\")\n# 选择部分数值特征\nnumeric_feat = ['LIMIT_BAL', 'AGE', 'AVG_BILL_AMT', 'AVG_PAY_AMT']\n\n# 创建二阶多项式特征\npoly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\npoly_features = poly.fit_transform(df_fe[numeric_feat])\n\n# 创建特征名称\npoly_feat_names = poly.get_feature_names_out(numeric_feat)\n\n# 添加多项式特征到数据集\npoly_df = pd.DataFrame(poly_features, columns=poly_feat_names)\npoly_df = poly_df.iloc[:, len(numeric_feat):]  # 移除原特征，只保留交互项\n\n# 重置索引并连接\ndf_fe.reset_index(drop=True, inplace=True)\npoly_df.reset_index(drop=True, inplace=True)\ndf_fe = pd.concat([df_fe, poly_df], axis=1)\n\nprint(f\"Added {poly_df.shape[1]} interaction features\")\n\n# 3. 特征选择\nprint(\"\\nPerforming feature selection...\")\n# 准备特征和目标变量\nX = df_fe.drop('DEFAULT', axis=1)\ny = df_fe['DEFAULT']\n\n# 使用ANOVA F-value进行特征选择\nselector = SelectKBest(f_classif, k=15)  # 选择前15个最重要的特征\nX_new = selector.fit_transform(X, y)\n\n# 获取特征得分\nfeature_scores = pd.DataFrame()\nfeature_scores['Feature'] = X.columns\nfeature_scores['Score'] = selector.scores_\nfeature_scores = feature_scores.sort_values(by='Score', ascending=False)\n\nprint(\"\\nTop 15 important features:\")\nprint(feature_scores.head(15))\n\n# 选择的特征\nselected_features = X.columns[selector.get_support()].tolist()\nprint(\"\\nSelected features:\", selected_features)\n\n# 使用选择的特征创建最终数据集\nX_selected = X[selected_features]\nfinal_df = pd.concat([X_selected, y], axis=1)\n\nprint(f\"\\nFinal dataset shape: {final_df.shape}\")\n\n# 4. 可视化重要特征的分布\nplt.figure(figsize=(15, 10))\n\n# 展示前6个最重要的特征\ntop_features = feature_scores['Feature'].head(6).tolist()\n\nfor i, feature in enumerate(top_features):\n    plt.subplot(2, 3, i+1)\n    sns.histplot(data=df_fe, x=feature, hue='DEFAULT', bins=20, kde=True, palette=['blue', 'red'])\n    plt.title(f'Feature: {feature}')\n    plt.xlabel(feature)\n    plt.ylabel('Frequency')\n\nplt.tight_layout()\nplt.show()\n\n# 5. 特征降维 (PCA)\nplt.figure(figsize=(12, 5))\n\n# 选择数值特征进行PCA\nnum_features = X.select_dtypes(include=['float64', 'int64']).columns.tolist()\nX_num = X[num_features]\n\n# 标准化数据\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X_num)\n\n# 应用PCA\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X_scaled)\n\n# 创建PCA数据框\npca_df = pd.DataFrame(X_pca, columns=['PC1', 'PC2'])\npca_df['DEFAULT'] = y.values\n\n# 可视化PCA结果\nplt.subplot(1, 2, 1)\nsns.scatterplot(data=pca_df, x='PC1', y='PC2', hue='DEFAULT', palette=['blue', 'red'])\nplt.title('PCA: First two principal components')\n\n# 计算特征对主成分的贡献\nloadings = pd.DataFrame(\n    pca.components_.T,\n    columns=['PC1', 'PC2'],\n    index=num_features\n)\n\n# 可视化特征负荷\nplt.subplot(1, 2, 2)\nsns.heatmap(loadings, cmap='viridis')\nplt.title('Feature contributions to PCs')\n\nplt.tight_layout()\nplt.show()\n\n# 输出解释方差比例\nprint(f\"\\nVariance explained by PC1: {pca.explained_variance_ratio_[0]:.2f}\")\nprint(f\"Variance explained by PC2: {pca.explained_variance_ratio_[1]:.2f}\")\nprint(f\"Total variance explained: {sum(pca.explained_variance_ratio_):.2f}\")"
        },
        {
          "id": 4,
          "title": "模型训练",
          "description": "训练多种机器学习模型来预测违约风险",
          "example_code": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# 使用通用字体设置，避免中文字体问题\nimport matplotlib as mpl\nmpl.rcParams['font.family'] = 'DejaVu Sans'\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.preprocessing import StandardScaler\nfrom app.services.bank_analysis import bank_service\n\n# 加载数据\ndf = bank_service.load_credit_data()\n\n# 数据预处理\nprint(\"Data preprocessing...\")\n# 准备特征和目标变量\nX = df.drop('DEFAULT', axis=1)\ny = df['DEFAULT']\n\n# 标准化特征\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# 划分训练集和测试集\nX_train, X_test, y_train, y_test = train_test_split(\n    X_scaled, y, test_size=0.25, random_state=42, stratify=y\n)\n\nprint(f\"Training set: {X_train.shape}, Test set: {X_test.shape}\")\n\n# 定义评估模型的函数\ndef evaluate_model(model, X_train, X_test, y_train, y_test):\n    # 训练模型\n    model.fit(X_train, y_train)\n    \n    # 预测\n    y_pred = model.predict(X_test)\n    \n    # 评估指标\n    accuracy = accuracy_score(y_test, y_pred)\n    report = classification_report(y_test, y_pred)\n    cm = confusion_matrix(y_test, y_pred)\n    \n    # ROC曲线\n    if hasattr(model, 'predict_proba'):\n        y_proba = model.predict_proba(X_test)[:, 1]\n        fpr, tpr, _ = roc_curve(y_test, y_proba)\n        roc_auc = auc(fpr, tpr)\n    else:\n        fpr, tpr, roc_auc = None, None, None\n    \n    return {\n        'model': model,\n        'accuracy': accuracy,\n        'report': report,\n        'confusion_matrix': cm,\n        'roc': (fpr, tpr, roc_auc)\n    }\n\n# 训练和评估多个模型\nprint(\"\\nTraining models...\")\n\n# 1. 逻辑回归\nprint(\"Training Logistic Regression...\")\nlr = LogisticRegression(max_iter=1000, class_weight='balanced')\nlr_result = evaluate_model(lr, X_train, X_test, y_train, y_test)\n\n# 2. 随机森林\nprint(\"Training Random Forest...\")\nrf = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\nrf_result = evaluate_model(rf, X_train, X_test, y_train, y_test)\n\n# 3. 梯度提升树\nprint(\"Training Gradient Boosting...\")\ngb = GradientBoostingClassifier(n_estimators=100, random_state=42)\ngb_result = evaluate_model(gb, X_train, X_test, y_train, y_test)\n\n# 4. 支持向量机\nprint(\"Training SVM...\")\nsvm = SVC(probability=True, class_weight='balanced')\nsvm_result = evaluate_model(svm, X_train, X_test, y_train, y_test)\n\n# 输出模型结果\nmodels = [lr_result, rf_result, gb_result, svm_result]\nmodel_names = ['Logistic Regression', 'Random Forest', 'Gradient Boosting', 'SVM']\n\nprint(\"\\nModel comparison:\")\nfor name, result in zip(model_names, models):\n    print(f\"\\n{name}:\")\n    print(f\"Accuracy: {result['accuracy']:.4f}\")\n    print(\"Classification Report:\")\n    print(result['report'])\n\n# 可视化比较模型性能\nprint(\"\\nVisualizing model performance...\")\n\n# 1. 准确率比较\naccuracies = [result['accuracy'] for result in models]\n\nplt.figure(figsize=(10, 6))\nplt.bar(model_names, accuracies, color=['blue', 'green', 'red', 'purple'])\nplt.title('Model Accuracy Comparison')\nplt.ylabel('Accuracy')\nplt.ylim(0.5, 1.0)  # 设置y轴范围，更好地显示差异\nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.show()\n\n# 2. 混淆矩阵\nplt.figure(figsize=(12, 10))\n\nfor i, (name, result) in enumerate(zip(model_names, models)):\n    plt.subplot(2, 2, i+1)\n    cm = result['confusion_matrix']\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n    plt.title(f'{name} Confusion Matrix')\n    plt.xlabel('Predicted')\n    plt.ylabel('Actual')\n\nplt.tight_layout()\nplt.show()\n\n# 3. ROC曲线比较\nplt.figure(figsize=(10, 8))\n\nfor name, result in zip(model_names, models):\n    fpr, tpr, roc_auc = result['roc']\n    if fpr is not None:\n        plt.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.3f})')\n\nplt.plot([0, 1], [0, 1], 'k--')  # 对角线\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curves Comparison')\nplt.legend(loc='lower right')\nplt.grid(alpha=0.3)\nplt.show()\n\n# 4. 特征重要性（仅针对随机森林模型）\nrandom_forest = rf_result['model']\n\n# 获取特征重要性\nfeature_importances = random_forest.feature_importances_\n\n# 创建DataFrame用于可视化\nimportance_df = pd.DataFrame({\n    'Feature': X.columns,\n    'Importance': feature_importances\n})\nimportance_df = importance_df.sort_values('Importance', ascending=False).head(15)\n\nplt.figure(figsize=(12, 6))\nsns.barplot(x='Importance', y='Feature', data=importance_df, palette='viridis')\nplt.title('Random Forest Feature Importance')\nplt.tight_layout()\nplt.show()\n\n# 超参数调优（以随机森林为例）\nprint(\"\\nPerforming hyperparameter tuning for Random Forest...\")\n\n# 定义参数网格\nparam_grid = {\n    'n_estimators': [50, 100],\n    'max_depth': [None, 10, 20],\n    'min_samples_split': [2, 5],\n    'min_samples_leaf': [1, 2]\n}\n\n# 使用网格搜索和交叉验证\ngrid_search = GridSearchCV(\n    RandomForestClassifier(random_state=42, class_weight='balanced'),\n    param_grid=param_grid,\n    cv=5,\n    scoring='accuracy',\n    n_jobs=-1\n)\n\ngrid_search.fit(X_train, y_train)\n\nprint(f\"Best parameters: {grid_search.best_params_}\")\nprint(f\"Best cross-validation score: {grid_search.best_score_:.4f}\")\n\n# 使用最佳参数的模型\nbest_rf = grid_search.best_estimator_\nbest_rf_result = evaluate_model(best_rf, X_train, X_test, y_train, y_test)\n\nprint(\"\\nOptimized Random Forest:\")\nprint(f\"Accuracy: {best_rf_result['accuracy']:.4f}\")\nprint(\"Classification Report:\")\nprint(best_rf_result['report'])\n\n# 保存训练好的模型\nprint(\"\\nModel training completed!\")\nprint(f\"Best model: Optimized Random Forest (Accuracy: {best_rf_result['accuracy']:.4f})\")"
        },
        {
          "id": 5,
          "title": "模型评估",
          "description": "评估模型性能，解释模型决策",
          "example_code": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# 使用通用字体设置，避免中文字体问题\nimport matplotlib as mpl\nmpl.rcParams['font.family'] = 'DejaVu Sans'\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\nfrom sklearn.metrics import confusion_matrix, roc_curve, precision_recall_curve, average_precision_score\nfrom sklearn.ensemble import RandomForestClassifier\nimport shap\nfrom app.services.bank_analysis import bank_service\n\n# 加载数据\ndf = bank_service.load_credit_data()\n\n# 数据预处理\nprint(\"Preprocessing data...\")\nX = df.drop('DEFAULT', axis=1)\ny = df['DEFAULT']\n\n# 标准化数据\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# 划分数据集\nX_train, X_test, y_train, y_test = train_test_split(\n    X_scaled, y, test_size=0.25, random_state=42, stratify=y\n)\n\n# 训练模型\nprint(\"Training model for evaluation...\")\nrf = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\nrf.fit(X_train, y_train)\n\n# 预测\ny_pred = rf.predict(X_test)\ny_proba = rf.predict_proba(X_test)[:, 1]\n\n# 计算各种评估指标\nprint(\"\\nCalculating evaluation metrics...\")\naccuracy = accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred)\nrecall = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\nroc_auc = roc_auc_score(y_test, y_proba)\navg_precision = average_precision_score(y_test, y_proba)\n\n# 打印评估指标\nprint(\"Model Performance Metrics:\")\nprint(f\"Accuracy: {accuracy:.4f}\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall: {recall:.4f}\")\nprint(f\"F1 Score: {f1:.4f}\")\nprint(f\"ROC AUC: {roc_auc:.4f}\")\nprint(f\"Average Precision: {avg_precision:.4f}\")\n\n# 混淆矩阵\ncm = confusion_matrix(y_test, y_pred)\n\n# 计算其他混淆矩阵指标\ntn, fp, fn, tp = cm.ravel()\ntotal = tn + fp + fn + tp\nprint(\"\\nConfusion Matrix Analysis:\")\nprint(f\"True Negatives: {tn} ({tn/total:.2%})\")\nprint(f\"False Positives: {fp} ({fp/total:.2%})\")\nprint(f\"False Negatives: {fn} ({fn/total:.2%})\")\nprint(f\"True Positives: {tp} ({tp/total:.2%})\")\n\n# 计算特定业务指标\nprint(\"\\nBusiness Metrics:\")\ncost_fp = 1  # 假阳性的成本（错误地识别为违约的成本）\ncost_fn = 5  # 假阴性的成本（错误地识别为非违约的成本，通常更高）\ntotal_cost = (fp * cost_fp) + (fn * cost_fn)\nprint(f\"Total Error Cost: {total_cost} (FP: {fp*cost_fp}, FN: {fn*cost_fn})\")\n\n# 可视化混淆矩阵\nplt.figure(figsize=(10, 8))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\nplt.title('Confusion Matrix')\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.show()\n\n# 交叉验证评估\nprint(\"\\nPerforming cross-validation...\")\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\ncv_scores_accuracy = cross_val_score(rf, X_scaled, y, cv=cv, scoring='accuracy')\ncv_scores_auc = cross_val_score(rf, X_scaled, y, cv=cv, scoring='roc_auc')\n\nprint(f\"CV Accuracy: {cv_scores_accuracy.mean():.4f} (+/- {cv_scores_accuracy.std()*2:.4f})\")\nprint(f\"CV ROC AUC: {cv_scores_auc.mean():.4f} (+/- {cv_scores_auc.std()*2:.4f})\")\n\n# ROC 曲线\nfpr, tpr, thresholds = roc_curve(y_test, y_proba)\n\nplt.figure(figsize=(10, 6))\nplt.plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc:.3f})')\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic (ROC) Curve')\nplt.legend(loc='lower right')\nplt.grid(alpha=0.3)\nplt.show()\n\n# Precision-Recall 曲线\nprecision_curve, recall_curve, _ = precision_recall_curve(y_test, y_proba)\n\nplt.figure(figsize=(10, 6))\nplt.plot(recall_curve, precision_curve, label=f'AP = {avg_precision:.3f}')\nplt.axhline(y=y_test.mean(), color='r', linestyle='--', label=f'Baseline: {y_test.mean():.3f}')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Precision-Recall Curve')\nplt.legend()\nplt.grid(alpha=0.3)\nplt.show()\n\n# 阈值分析\nthresholds_df = pd.DataFrame({\n    'Threshold': thresholds,\n    'TPR': tpr[:-1],\n    'FPR': fpr[:-1]\n})\n\n# 计算每个阈值的精确度和召回率\nthresholds_plot = np.arange(0.1, 1.0, 0.1)\nmetrics = []\n\nfor threshold in thresholds_plot:\n    y_pred_threshold = (y_proba >= threshold).astype(int)\n    \n    precision_t = precision_score(y_test, y_pred_threshold)\n    recall_t = recall_score(y_test, y_pred_threshold)\n    f1_t = f1_score(y_test, y_pred_threshold)\n    \n    metrics.append({\n        'Threshold': threshold,\n        'Precision': precision_t,\n        'Recall': recall_t,\n        'F1': f1_t\n    })\n\nmetrics_df = pd.DataFrame(metrics)\n\n# 绘制不同阈值下的指标\nplt.figure(figsize=(12, 6))\n\nplt.plot(metrics_df['Threshold'], metrics_df['Precision'], marker='o', label='Precision')\nplt.plot(metrics_df['Threshold'], metrics_df['Recall'], marker='s', label='Recall')\nplt.plot(metrics_df['Threshold'], metrics_df['F1'], marker='^', label='F1')\n\nplt.xlabel('Classification Threshold')\nplt.ylabel('Score')\nplt.title('Performance Metrics vs. Classification Threshold')\nplt.legend()\nplt.grid(alpha=0.3)\nplt.show()\n\n# Feature Importance and SHAP Analysis\nprint(\"\\nAnalyzing feature importance...\")\n\n# 获取特征重要性\nfeature_importances = rf.feature_importances_\n\n# 创建DataFrame并排序\nfeature_importance_df = pd.DataFrame({\n    'Feature': X.columns,\n    'Importance': feature_importances\n})\nfeature_importance_df = feature_importance_df.sort_values('Importance', ascending=False)\n\n# 绘制特征重要性\nplt.figure(figsize=(12, 6))\nsns.barplot(x='Importance', y='Feature', data=feature_importance_df.head(15))\nplt.title('Random Forest Feature Importance')\nplt.show()\n\n# SHAP值分析\nprint(\"\\nCalculating SHAP values (this may take a moment)...\")\n# 使用原始特征数据而不是标准化数据来计算SHAP值以提高可解释性\nX_sample = X.iloc[:100]  # 使用一个样本子集进行SHAP分析，以减少计算时间\nmodel_for_shap = RandomForestClassifier(n_estimators=100, random_state=42)\nmodel_for_shap.fit(X, y)  # 在完整数据上训练\n\n# 创建SHAP解释器\nexplainer = shap.TreeExplainer(model_for_shap)\nshap_values = explainer.shap_values(X_sample)\n\n# 绘制SHAP摘要图\nshap.summary_plot(shap_values[1], X_sample, feature_names=X.columns.tolist())\n\n# 绘制前3个特征的SHAP依赖图\ntop_features = feature_importance_df['Feature'].head(3).tolist()\nfor feature in top_features:\n    plt.figure()\n    shap.dependence_plot(feature, shap_values[1], X_sample, feature_names=X.columns.tolist())\n\n# 模型校准曲线\nfrom sklearn.calibration import calibration_curve\nplt.figure(figsize=(10, 6))\n\nprobability_true, probability_pred = calibration_curve(y_test, y_proba, n_bins=10)\nplt.plot(probability_pred, probability_true, marker='o', label='Random Forest')\nplt.plot([0, 1], [0, 1], 'k--', label='Perfectly Calibrated')\n\nplt.xlabel('Mean Predicted Probability')\nplt.ylabel('Fraction of Positives')\nplt.title('Calibration Curve')\nplt.legend()\nplt.grid(alpha=0.3)\nplt.show()\n\n# 输出总结和建议\nprint(\"\\nModel Evaluation Summary:\")\nprint(f\"Overall Performance: {accuracy:.2%} accuracy, {roc_auc:.2%} ROC AUC\")\n\n# 根据性能指标给出建议\nif roc_auc > 0.8:\n    print(\"The model shows good discriminative ability.\")\nelif roc_auc > 0.7:\n    print(\"The model shows acceptable discriminative ability, but could be improved.\")\nelse:\n    print(\"The model's discriminative ability is limited. Consider feature engineering or trying different algorithms.\")\n\nif precision < 0.7:\n    print(\"Consider increasing the classification threshold to improve precision, if required for business purposes.\")\n\nif recall < 0.7:\n    print(\"Consider decreasing the classification threshold to improve recall, if required for business purposes.\")"
        },
        {
          "id": 6,
          "title": "风险控制策略",
          "description": "设计和实施信用卡风险控制策略",
          "example_code": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# 使用通用字体设置，避免中文字体问题\nimport matplotlib as mpl\nmpl.rcParams['font.family'] = 'DejaVu Sans'\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom app.services.bank_analysis import bank_service\n\n# 加载数据\ndf = bank_service.load_credit_data()\n\n# 数据准备\nprint(\"Preparing data...\")\nX = df.drop('DEFAULT', axis=1)\ny = df['DEFAULT']\n\n# 标准化数据\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# 划分训练集和测试集\nX_train, X_test, y_train, y_test = train_test_split(\n    X_scaled, y, test_size=0.25, random_state=42, stratify=y\n)\n\n# 训练模型（使用随机森林为例）\nprint(\"Training model...\")\nmodel = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\nmodel.fit(X_train, y_train)\n\n# 获取预测概率\ny_proba = model.predict_proba(X_test)[:, 1]\n\n# 创建测试数据集的副本，并添加预测概率\ntest_df = pd.DataFrame(X_test, columns=X.columns)\ntest_df['ACTUAL_DEFAULT'] = y_test.values\ntest_df['DEFAULT_PROBABILITY'] = y_proba\n\n# 1. 风险分层策略\nprint(\"\\nImplementing risk stratification strategy...\")\n\n# 基于预测概率对客户进行风险分层\nrisk_bins = [0, 0.2, 0.4, 0.6, 0.8, 1.0]\nrisk_labels = ['Very Low', 'Low', 'Medium', 'High', 'Very High']\n\ntest_df['RISK_LEVEL'] = pd.cut(\n    test_df['DEFAULT_PROBABILITY'], \n    bins=risk_bins, \n    labels=risk_labels\n)\n\n# 计算每个风险等级的实际违约率\nrisk_level_stats = test_df.groupby('RISK_LEVEL')['ACTUAL_DEFAULT'].agg(\n    ['count', 'mean']\n).rename(columns={'count': 'Number_of_Customers', 'mean': 'Actual_Default_Rate'})\n\n# 添加预测的违约概率平均值\nrisk_level_stats['Avg_Predicted_Probability'] = test_df.groupby('RISK_LEVEL')['DEFAULT_PROBABILITY'].mean()\n\nprint(\"Risk Stratification Results:\")\nprint(risk_level_stats)\n\n# 可视化风险分层\nplt.figure(figsize=(12, 6))\n\n# 客户数量分布\nplt.subplot(1, 2, 1)\nsns.countplot(x='RISK_LEVEL', data=test_df, palette='YlOrRd')\nplt.title('Number of Customers by Risk Level')\nplt.xlabel('Risk Level')\nplt.ylabel('Count')\n\n# 各风险等级的实际违约率\nplt.subplot(1, 2, 2)\nsns.barplot(\n    x=risk_level_stats.index, \n    y=risk_level_stats['Actual_Default_Rate'],\n    palette='YlOrRd'\n)\nplt.title('Actual Default Rate by Risk Level')\nplt.xlabel('Risk Level')\nplt.ylabel('Default Rate')\nplt.ylim(0, 1)\n\nplt.tight_layout()\nplt.show()\n\n# 2. 设计信用审批策略\nprint(\"\\nDesigning credit approval strategy...\")\n\n# 设置不同风险等级的信用额度调整系数\ncredit_limit_adjustments = {\n    'Very Low': 1.2,   # 增加20%\n    'Low': 1.1,        # 增加10%\n    'Medium': 1.0,     # 保持不变\n    'High': 0.8,       # 减少20%\n    'Very High': 0.6   # 减少40%\n}\n\n# 假设我们有一个基准信用额度为10000\nbase_credit_limit = 10000\n\n# 计算调整后的信用额度\ntest_df['ADJUSTED_CREDIT_LIMIT'] = test_df['RISK_LEVEL'].map(credit_limit_adjustments) * base_credit_limit\n\n# 3. 拒绝策略\n# 设置拒绝阈值 - 高于此阈值的申请将被拒绝\nrejection_threshold = 0.7\n\ntest_df['APPLICATION_STATUS'] = np.where(\n    test_df['DEFAULT_PROBABILITY'] >= rejection_threshold,\n    'Rejected',\n    'Approved'\n)\n\n# 计算拒绝率和批准率\nrejection_rate = (test_df['APPLICATION_STATUS'] == 'Rejected').mean()\napproval_rate = (test_df['APPLICATION_STATUS'] == 'Approved').mean()\n\nprint(f\"Rejection Rate: {rejection_rate:.2%}\")\nprint(f\"Approval Rate: {approval_rate:.2%}\")\n\n# 计算拒绝和批准客户中的实际违约率\nrejected_default_rate = test_df[test_df['APPLICATION_STATUS'] == 'Rejected']['ACTUAL_DEFAULT'].mean()\napproved_default_rate = test_df[test_df['APPLICATION_STATUS'] == 'Approved']['ACTUAL_DEFAULT'].mean()\n\nprint(f\"Default Rate among Rejected Customers: {rejected_default_rate:.2%}\")\nprint(f\"Default Rate among Approved Customers: {approved_default_rate:.2%}\")\n\n# 可视化拒绝策略\nplt.figure(figsize=(12, 5))\n\n# 批准 vs. 拒绝的客户数量\nplt.subplot(1, 2, 1)\nsns.countplot(x='APPLICATION_STATUS', data=test_df)\nplt.title('Application Approval Status')\nplt.xlabel('Status')\nplt.ylabel('Count')\n\n# 批准 vs. 拒绝的违约率\nstatus_default_rates = test_df.groupby('APPLICATION_STATUS')['ACTUAL_DEFAULT'].mean()\nplt.subplot(1, 2, 2)\nsns.barplot(x=status_default_rates.index, y=status_default_rates.values)\nplt.title('Default Rate by Application Status')\nplt.xlabel('Status')\nplt.ylabel('Default Rate')\nplt.ylim(0, 1)\n\nplt.tight_layout()\nplt.show()\n\n# 4. 利润模拟\nprint(\"\\nSimulating profit impact...\")\n\n# 设置业务参数\nannual_interest_rate = 0.18  # 年利率18%\naverage_transaction_amount = 5000  # 平均交易金额\nfee_percentage = 0.03  # 交易费用3%\ncost_of_funds = 0.05  # 资金成本5%\ncost_per_default = 10000  # 每个违约客户的平均损失\n\n# 计算每个客户的预期收益\ntest_df['EXPECTED_PROFIT'] = np.where(\n    test_df['ACTUAL_DEFAULT'] == 1,\n    -cost_per_default,  # 违约客户造成损失\n    (annual_interest_rate - cost_of_funds) * average_transaction_amount + \n        fee_percentage * average_transaction_amount  # 非违约客户产生收益\n)\n\n# 计算不同策略下的总收益\n# 1. 接受所有客户\ntotal_profit_accept_all = test_df['EXPECTED_PROFIT'].sum()\n\n# 2. 使用我们的拒绝策略\ntotal_profit_with_strategy = test_df[test_df['APPLICATION_STATUS'] == 'Approved']['EXPECTED_PROFIT'].sum()\n\n# 3. 拒绝所有客户\ntotal_profit_reject_all = 0  # 假设拒绝所有客户没有收益也没有损失\n\nprint(f\"Profit if Accept All: ${total_profit_accept_all:.2f}\")\nprint(f\"Profit with Rejection Strategy: ${total_profit_with_strategy:.2f}\")\nprint(f\"Profit if Reject All: ${total_profit_reject_all:.2f}\")\n\nprofit_improvement = ((total_profit_with_strategy - total_profit_accept_all) / abs(total_profit_accept_all)) * 100\nprint(f\"Profit Improvement with Strategy: {profit_improvement:.2f}%\")\n\n# 5. 阈值优化\nprint(\"\\nOptimizing rejection threshold...\")\n\n# 测试不同的拒绝阈值\nthresholds = np.arange(0.1, 1.0, 0.1)\nthreshold_results = []\n\nfor threshold in thresholds:\n    # 应用拒绝策略\n    application_status = np.where(\n        test_df['DEFAULT_PROBABILITY'] >= threshold,\n        'Rejected',\n        'Approved'\n    )\n    \n    # 计算批准客户的总收益\n    approved_indices = [i for i, status in enumerate(application_status) if status == 'Approved']\n    profit = test_df.iloc[approved_indices]['EXPECTED_PROFIT'].sum()\n    \n    # 计算拒绝率\n    rejection_rate = (application_status == 'Rejected').mean()\n    \n    # 计算批准客户中的违约率\n    approved_default_rate = test_df.iloc[approved_indices]['ACTUAL_DEFAULT'].mean() if approved_indices else 0\n    \n    threshold_results.append({\n        'Threshold': threshold,\n        'Profit': profit,\n        'Rejection_Rate': rejection_rate,\n        'Approved_Default_Rate': approved_default_rate\n    })\n\n# 转换为DataFrame\nthreshold_df = pd.DataFrame(threshold_results)\n\n# 找出利润最高的阈值\noptimal_threshold = threshold_df.loc[threshold_df['Profit'].idxmax(), 'Threshold']\nmax_profit = threshold_df['Profit'].max()\n\nprint(f\"Optimal Rejection Threshold: {optimal_threshold}\")\nprint(f\"Maximum Profit: ${max_profit:.2f}\")\n\n# 可视化不同阈值下的结果\nplt.figure(figsize=(15, 6))\n\n# 利润 vs. 阈值\nplt.subplot(1, 3, 1)\nplt.plot(threshold_df['Threshold'], threshold_df['Profit'], marker='o')\nplt.axvline(x=optimal_threshold, color='r', linestyle='--', label=f'Optimal={optimal_threshold}')\nplt.title('Profit vs. Threshold')\nplt.xlabel('Rejection Threshold')\nplt.ylabel('Profit ($)')\nplt.legend()\n\n# 拒绝率 vs. 阈值\nplt.subplot(1, 3, 2)\nplt.plot(threshold_df['Threshold'], threshold_df['Rejection_Rate'], marker='o')\nplt.axvline(x=optimal_threshold, color='r', linestyle='--')\nplt.title('Rejection Rate vs. Threshold')\nplt.xlabel('Rejection Threshold')\nplt.ylabel('Rejection Rate')\n\n# 批准客户中的违约率 vs. 阈值\nplt.subplot(1, 3, 3)\nplt.plot(threshold_df['Threshold'], threshold_df['Approved_Default_Rate'], marker='o')\nplt.axvline(x=optimal_threshold, color='r', linestyle='--')\nplt.title('Approved Default Rate vs. Threshold')\nplt.xlabel('Rejection Threshold')\nplt.ylabel('Default Rate among Approved')\n\nplt.tight_layout()\nplt.show()\n\n# 6. 最终风险控制策略建议\nprint(\"\\nFinal Risk Control Strategy Recommendations:\")\n\nprint(f\"1. Implement a risk-based approval system with an optimal rejection threshold of {optimal_threshold}\")\nprint(\"2. Apply risk-based credit limit adjustments:\")\nfor risk_level, adjustment in credit_limit_adjustments.items():\n    adjustment_percentage = (adjustment - 1) * 100 if adjustment > 1 else (adjustment - 1) * 100\n    print(f\"   - {risk_level} risk: {adjustment_percentage:+.0f}% credit limit adjustment\")\n\nprint(\"3. Implement additional monitoring for high-risk approved customers\")\nprint(\"4. Consider implementing risk-based pricing for different risk segments\")\n\n# 7. 策略实施后的预期业务影响\nprint(\"\\nExpected Business Impact:\")\nprint(f\"- Expected Profit Improvement: {profit_improvement:.2f}%\")\nprint(f\"- Expected Approval Rate: {1 - threshold_df.loc[threshold_df['Threshold'] == optimal_threshold, 'Rejection_Rate'].values[0]:.2%}\")\nprint(f\"- Expected Default Rate among Approved: {threshold_df.loc[threshold_df['Threshold'] == optimal_threshold, 'Approved_Default_Rate'].values[0]:.2%}\")"
        }
      ]
    },
    {
      "id": 2,
      "title": "银行客户流失预警模型",
      "description": "使用机器学习方法预测和分析银行客户流失风险因素",
      "category": "bank",
      "difficulty_level": 4,
      "estimated_duration": 120,
      "prerequisites": "统计分析基础和数据挖掘知识",
      "data_source": "银行客户交易与行为数据",
      "steps": [
        {
          "id": 1,
          "title": "客户行为分析",
          "description": "分析客户交易频率、金额和渠道偏好等行为特征",
          "example_code": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport seaborn as sns\n# 使用通用字体设置，避免中文字体问题\nfrom app.services.docker_matplotlib_fix import configure_matplotlib_fonts\n\n# 配置字体以支持中文显示\nconfigure_matplotlib_fonts()\nfrom app.services.bank_analysis import bank_service\n\n# 加载客户数据\ndf = bank_service.load_customer_churn_data()\n\n# 1. 数据概览\nprint(\"=== Data Overview ===\")\nprint(f\"Dataset shape: {df.shape}\")\nprint(\"\\nFirst few rows:\")\nprint(df.head())\n\n# 2. 数据基本信息\nprint(\"\\n=== Basic Data Information ===\")\nprint(df.info())\n\n# 3. 基本统计描述\nprint(\"\\n=== Descriptive Statistics ===\")\nprint(df.describe())\n\n# 4. 缺失值情况\nprint(\"\\n=== Missing Values Analysis ===\")\nmissing_values = df.isnull().sum()\nif missing_values.sum() > 0:\n    print(missing_values[missing_values > 0])\n    missing_percentage = (missing_values / len(df)) * 100\n    print(\"\\nMissing percentage:\")\n    print(missing_percentage[missing_percentage > 0])\nelse:\n    print(\"No missing values found.\")\n\n# 5. 客户流失情况分析\nif 'Churn' in df.columns:\n    print(\"\\n=== Churn Distribution ===\")\n    churn_distribution = df['Churn'].value_counts(normalize=True) * 100\n    print(f\"Non-churned customers: {churn_distribution[0]:.2f}%\")\n    print(f\"Churned customers: {churn_distribution[1]:.2f}%\")\n    \n    plt.figure(figsize=(10, 5))\n    plt.subplot(1, 2, 1)\n    sns.countplot(x='Churn', data=df, palette=['lightblue', 'salmon'])\n    plt.title('Customer Churn Distribution')\n    plt.xlabel('Churn (0=No, 1=Yes)')\n    plt.ylabel('Count')\n    \n    plt.subplot(1, 2, 2)\n    plt.pie(churn_distribution, labels=['Retained', 'Churned'], autopct='%1.1f%%', colors=['lightblue', 'salmon'], startangle=90)\n    plt.title('Churn Percentage')\n    plt.axis('equal')\n    plt.tight_layout()\n    plt.show()\n\n# 6. 交易频率分析\nif 'TransactionFrequency' in df.columns:\n    print(\"\\n=== Transaction Frequency Analysis ===\")\n    print(df.groupby('Churn')['TransactionFrequency'].describe())\n    \n    plt.figure(figsize=(10, 6))\n    sns.histplot(data=df, x='TransactionFrequency', hue='Churn', bins=20, kde=True, palette=['lightblue', 'salmon'])\n    plt.title('Transaction Frequency Distribution by Churn Status')\n    plt.xlabel('Transaction Frequency (per month)')\n    plt.ylabel('Count')\n    plt.show()\n    \n    plt.figure(figsize=(8, 6))\n    sns.boxplot(x='Churn', y='TransactionFrequency', data=df, palette=['lightblue', 'salmon'])\n    plt.title('Transaction Frequency by Churn Status')\n    plt.xlabel('Churn (0=No, 1=Yes)')\n    plt.ylabel('Transaction Frequency')\n    plt.show()\n\n# 7. 交易金额分析\nif 'TransactionAmount' in df.columns:\n    print(\"\\n=== Transaction Amount Analysis ===\")\n    print(df.groupby('Churn')['TransactionAmount'].describe())\n    \n    plt.figure(figsize=(10, 6))\n    sns.histplot(data=df, x='TransactionAmount', hue='Churn', bins=20, kde=True, palette=['lightblue', 'salmon'])\n    plt.title('Transaction Amount Distribution by Churn Status')\n    plt.xlabel('Average Transaction Amount')\n    plt.ylabel('Count')\n    plt.show()\n\n# 8. 渠道偏好分析\nif 'PreferredChannel' in df.columns:\n    print(\"\\n=== Channel Preference Analysis ===\")\n    channel_counts = df.groupby(['PreferredChannel', 'Churn']).size().unstack()\n    print(channel_counts)\n    \n    # 计算每个渠道的流失率\n    channel_churn_rate = channel_counts[1] / (channel_counts[0] + channel_counts[1]) * 100\n    print(\"\\nChurn rate by channel:\")\n    print(channel_churn_rate)\n    \n    plt.figure(figsize=(12, 5))\n    plt.subplot(1, 2, 1)\n    channel_counts.plot(kind='bar', stacked=True, color=['lightblue', 'salmon'])\n    plt.title('Customer Count by Channel and Churn Status')\n    plt.xlabel('Preferred Channel')\n    plt.ylabel('Number of Customers')\n    plt.legend(['Retained', 'Churned'])\n    \n    plt.subplot(1, 2, 2)\n    channel_churn_rate.plot(kind='bar', color='salmon')\n    plt.title('Churn Rate by Channel')\n    plt.xlabel('Preferred Channel')\n    plt.ylabel('Churn Rate (%)')\n    plt.tight_layout()\n    plt.show()\n\n# 9. 客户活跃度分析\nif 'DaysSinceLastTransaction' in df.columns:\n    print(\"\\n=== Customer Activity Analysis ===\")\n    print(df.groupby('Churn')['DaysSinceLastTransaction'].describe())\n    \n    plt.figure(figsize=(10, 6))\n    sns.boxplot(x='Churn', y='DaysSinceLastTransaction', data=df, palette=['lightblue', 'salmon'])\n    plt.title('Days Since Last Transaction by Churn Status')\n    plt.xlabel('Churn (0=No, 1=Yes)')\n    plt.ylabel('Days Since Last Transaction')\n    plt.show()\n\n# 10. 客户价值分析\nif 'CustomerValue' in df.columns and 'Tenure' in df.columns:\n    print(\"\\n=== Customer Value Analysis ===\")\n    # 计算每个客户的生命周期价值\n    df['LifetimeValue'] = df['CustomerValue'] * df['Tenure']\n    print(df.groupby('Churn')['LifetimeValue'].describe())\n    \n    plt.figure(figsize=(10, 6))\n    sns.scatterplot(data=df, x='Tenure', y='CustomerValue', hue='Churn', palette=['lightblue', 'salmon'], alpha=0.7)\n    plt.title('Customer Value vs Tenure by Churn Status')\n    plt.xlabel('Tenure (months)')\n    plt.ylabel('Monthly Customer Value')\n    plt.show()\n\n# 11. 数值特征间的相关性分析\nnumeric_df = df.select_dtypes(include=[np.number])\nif len(numeric_df.columns) > 1:\n    print(\"\\n=== Correlation Analysis ===\")\n    correlation_matrix = numeric_df.corr()\n    print(correlation_matrix['Churn'].sort_values(ascending=False) if 'Churn' in numeric_df.columns else correlation_matrix)\n    \n    plt.figure(figsize=(12, 10))\n    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n    plt.title('Correlation Matrix of Numerical Features')\n    plt.tight_layout()\n    plt.show()\n\n# 12. 关键行为特征的分布比较\nkey_features = [col for col in df.columns if col not in ['CustomerID', 'Name', 'Churn']]\nif len(key_features) > 0:\n    print(\"\\n=== Key Feature Distributions ===\")\n    \n    # 显示主要特征的分布\n    num_cols = min(6, len(key_features))\n    num_rows = (len(key_features) + num_cols - 1) // num_cols\n    plt.figure(figsize=(15, num_rows * 4))\n    \n    for i, feature in enumerate(key_features[:num_cols * num_rows]):\n        if feature in df.columns and df[feature].dtype in [np.int64, np.float64]:\n            plt.subplot(num_rows, num_cols, i+1)\n            sns.histplot(data=df, x=feature, hue='Churn', kde=True, palette=['lightblue', 'salmon'], bins=20)\n            plt.title(f'Distribution of {feature}')\n    \n    plt.tight_layout()\n    plt.show()\n\n# 13. 客户分群和行为模式识别\nif 'TransactionFrequency' in df.columns and 'TransactionAmount' in df.columns:\n    print(\"\\n=== Customer Segmentation Based on Transaction Behavior ===\")\n    \n    plt.figure(figsize=(10, 8))\n    sns.scatterplot(data=df, x='TransactionFrequency', y='TransactionAmount', hue='Churn', size='Tenure', sizes=(50, 200), palette=['lightblue', 'salmon'], alpha=0.7)\n    plt.title('Customer Segmentation by Transaction Behavior')\n    plt.xlabel('Transaction Frequency')\n    plt.ylabel('Average Transaction Amount')\n    plt.grid(True, linestyle='--', alpha=0.7)\n    plt.show()\n\n# 14. 总结关键发现\nprint(\"\\n=== Key Findings Summary ===\")\nif 'Churn' in df.columns:\n    # 计算流失率\n    churn_rate = df['Churn'].mean() * 100\n    print(f\"Overall Churn Rate: {churn_rate:.2f}%\")\n    \n    # 找出与流失最相关的特征\n    if 'Churn' in numeric_df.columns:\n        churn_correlations = correlation_matrix['Churn'].drop('Churn').abs().sort_values(ascending=False)\n        print(\"\\nTop features correlated with churn:\")\n        print(churn_correlations.head(5))\n    \n    # 分析流失客户和留存客户之间的主要区别\n    print(\"\\nKey differences between churned and non-churned customers:\")\n    for feature in key_features[:5]:\n        if feature in df.columns and df[feature].dtype in [np.int64, np.float64]:\n            churned_mean = df[df['Churn'] == 1][feature].mean()\n            non_churned_mean = df[df['Churn'] == 0][feature].mean()\n            diff_pct = ((churned_mean - non_churned_mean) / non_churned_mean) * 100\n            print(f\"- {feature}: {diff_pct:.1f}% difference between churned and non-churned customers\")"
        },
        {
          "id": 2,
          "title": "流失指标定义",
          "description": "确定客户流失的定义和关键指标",
          "example_code": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport matplotlib as mpl\nfrom datetime import datetime, timedelta\nfrom app.services.docker_matplotlib_fix import configure_matplotlib_fonts\n\n# 配置字体以支持中文显示\nconfigure_matplotlib_fonts()\nfrom app.services.bank_analysis import bank_service\n\n# 加载客户数据\ndf = bank_service.load_customer_churn_data()\nprint(f\"加载数据集，共{df.shape[0]}行，{df.shape[1]}列\")\n\n# 1. 探索现有的流失标签\nif 'Churn' in df.columns:\n    print(\"\\n=== 现有的流失标签分析 ===\")\n    churn_counts = df['Churn'].value_counts()\n    print(\"流失标签分布:\")\n    print(f\"未流失客户: {churn_counts[0]} ({churn_counts[0]/len(df)*100:.2f}%)\")\n    print(f\"已流失客户: {churn_counts[1]} ({churn_counts[1]/len(df)*100:.2f}%)\")\n    \n    # 可视化现有的流失分布\n    plt.figure(figsize=(10, 5))\n    plt.subplot(1, 2, 1)\n    sns.countplot(x='Churn', data=df, palette=['lightblue', 'salmon'])\n    plt.title('客户流失分布')\n    plt.xlabel('流失状态 (0=未流失, 1=已流失)')\n    plt.ylabel('客户数量')\n    \n    plt.subplot(1, 2, 2)\n    plt.pie(churn_counts, labels=['未流失', '已流失'], autopct='%1.1f%%', colors=['lightblue', 'salmon'], startangle=90)\n    plt.title('流失比例')\n    plt.axis('equal')\n    plt.tight_layout()\n    plt.show()\n\n# 2. 定义流失的方法\nprint(\"\\n=== 流失定义方法 ===\")\nprint(\"1. 账户关闭: 客户主动关闭账户\")\nprint(\"2. 非活跃流失: 客户长时间没有活动\")\nprint(\"3. 价值流失: 客户交易额显著下降\")\nprint(\"4. 参与度流失: 客户交易频率显著下降\")\nprint(\"5. 渠道转换: 客户完全停止使用某一渠道\")\n\n# 3. 基于交易不活跃定义流失\nprint(\"\\n=== 基于交易不活跃定义流失 ===\")\ninactivity_threshold = 90  # 90天不活跃定义为流失\n\nif 'LastTransactionDate' in df.columns:\n    # 转换日期字段为日期类型\n    df['LastTransactionDate'] = pd.to_datetime(df['LastTransactionDate'])\n    \n    # 计算当前日期（使用数据集中最近的交易日期再加7天作为\"当前日期\"）\n    current_date = df['LastTransactionDate'].max() + timedelta(days=7)\n    print(f\"分析的'当前日期': {current_date.strftime('%Y-%m-%d')}\")\n    \n    # 计算自上次交易以来的天数\n    df['DaysSinceLastTransaction'] = (current_date - df['LastTransactionDate']).dt.days\n    \n    # 基于不活跃天数定义流失\n    df['InactivityChurn'] = (df['DaysSinceLastTransaction'] > inactivity_threshold).astype(int)\n    \n    inactive_churn_rate = df['InactivityChurn'].mean() * 100\n    print(f\"基于{inactivity_threshold}天不活跃定义的流失率: {inactive_churn_rate:.2f}%\")\n    \n    # 与现有流失标签对比\n    if 'Churn' in df.columns:\n        churn_match = (df['Churn'] == df['InactivityChurn']).mean() * 100\n        print(f\"不活跃定义的流失与现有流失标签匹配度: {churn_match:.2f}%\")\n        \n        # 计算现有流失标签的不同类型\n        days_inactive_churned = df[df['Churn'] == 1]['DaysSinceLastTransaction'].mean()\n        days_inactive_retained = df[df['Churn'] == 0]['DaysSinceLastTransaction'].mean()\n        print(f\"已流失客户的平均不活跃天数: {days_inactive_churned:.1f}天\")\n        print(f\"未流失客户的平均不活跃天数: {days_inactive_retained:.1f}天\")\n\n# 4. 基于交易金额下降定义流失\nprint(\"\\n=== 基于交易金额下降定义流失 ===\")\nvalue_decline_threshold = -0.5  # 交易金额下降50%定义为价值流失\n\nif 'TransactionAmountDecline' in df.columns:\n    # 基于交易金额下降定义流失\n    df['ValueChurn'] = (df['TransactionAmountDecline'] <= value_decline_threshold).astype(int)\n    \n    value_churn_rate = df['ValueChurn'].mean() * 100\n    print(f\"基于交易金额下降{abs(value_decline_threshold)*100}%定义的流失率: {value_churn_rate:.2f}%\")\n    \n    # 与现有流失标签对比\n    if 'Churn' in df.columns:\n        value_churn_match = (df['Churn'] == df['ValueChurn']).mean() * 100\n        print(f\"价值流失与现有流失标签匹配度: {value_churn_match:.2f}%\")\n\n# 5. 基于交易频率下降定义流失\nprint(\"\\n=== 基于交易频率下降定义流失 ===\")\nfrequency_decline_threshold = -0.6  # 交易频率下降60%定义为参与度流失\n\nif 'TransactionFrequencyDecline' in df.columns:\n    # 基于交易频率下降定义流失\n    df['FrequencyChurn'] = (df['TransactionFrequencyDecline'] <= frequency_decline_threshold).astype(int)\n    \n    frequency_churn_rate = df['FrequencyChurn'].mean() * 100\n    print(f\"基于交易频率下降{abs(frequency_decline_threshold)*100}%定义的流失率: {frequency_churn_rate:.2f}%\")\n    \n    # 与现有流失标签对比\n    if 'Churn' in df.columns:\n        freq_churn_match = (df['Churn'] == df['FrequencyChurn']).mean() * 100\n        print(f\"参与度流失与现有流失标签匹配度: {freq_churn_match:.2f}%\")\n\n# 6. 综合指标流失定义\nprint(\"\\n=== 综合指标流失定义 ===\")\n# 创建综合流失指标\nchurn_indicators = []\n\nif 'InactivityChurn' in df.columns:\n    churn_indicators.append('InactivityChurn')\nif 'ValueChurn' in df.columns:\n    churn_indicators.append('ValueChurn')\nif 'FrequencyChurn' in df.columns:\n    churn_indicators.append('FrequencyChurn')\n\nif churn_indicators:\n    # 如果任一指标显示流失，则认为客户已流失\n    df['CompositeChurn'] = df[churn_indicators].max(axis=1)\n    \n    composite_churn_rate = df['CompositeChurn'].mean() * 100\n    print(f\"基于综合指标定义的流失率: {composite_churn_rate:.2f}%\")\n    \n    # 各种流失类型的重叠情况\n    if len(churn_indicators) > 1:\n        print(\"\\n不同流失类型的占比:\")\n        for indicator in churn_indicators:\n            indicator_rate = df[indicator].mean() * 100\n            print(f\"- {indicator}: {indicator_rate:.2f}%\")\n        \n        # 计算纯流失类型的客户\n        for i, indicator1 in enumerate(churn_indicators):\n            only_this_type = df[indicator1] == 1\n            for j, indicator2 in enumerate(churn_indicators):\n                if i != j:\n                    only_this_type &= df[indicator2] == 0\n            \n            only_this_type_pct = only_this_type.mean() * 100\n            print(f\"- 仅{indicator1}类型流失: {only_this_type_pct:.2f}%\")\n    \n    # 与现有流失标签对比\n    if 'Churn' in df.columns:\n        composite_match = (df['Churn'] == df['CompositeChurn']).mean() * 100\n        print(f\"\\n综合流失定义与现有流失标签匹配度: {composite_match:.2f}%\")\n\n# 7. 流失预警信号定义\nprint(\"\\n=== 流失预警信号定义 ===\")\n# 定义预警等级\ndf['ChurnRiskScore'] = 0\n\n# 根据不同指标分配风险分数\nif 'DaysSinceLastTransaction' in df.columns:\n    df.loc[df['DaysSinceLastTransaction'] > 60, 'ChurnRiskScore'] += 1\n    df.loc[df['DaysSinceLastTransaction'] > 75, 'ChurnRiskScore'] += 1\n\nif 'TransactionAmountDecline' in df.columns:\n    df.loc[df['TransactionAmountDecline'] < -0.3, 'ChurnRiskScore'] += 1\n    df.loc[df['TransactionAmountDecline'] < -0.4, 'ChurnRiskScore'] += 1\n\nif 'TransactionFrequencyDecline' in df.columns:\n    df.loc[df['TransactionFrequencyDecline'] < -0.4, 'ChurnRiskScore'] += 1\n    df.loc[df['TransactionFrequencyDecline'] < -0.5, 'ChurnRiskScore'] += 1\n\n# 其他可能的风险信号\nif 'SupportCallsCount' in df.columns:\n    df.loc[df['SupportCallsCount'] > 3, 'ChurnRiskScore'] += 1\n\nif 'ComplaintsCount' in df.columns:\n    df.loc[df['ComplaintsCount'] > 0, 'ChurnRiskScore'] += 1\n\n# 定义风险等级\ndf['ChurnRiskLevel'] = pd.cut(df['ChurnRiskScore'], \n                          bins=[-1, 0, 2, 4, 100], \n                          labels=['无风险', '低风险', '中风险', '高风险'])\n\n# 显示风险等级分布\nrisk_distribution = df['ChurnRiskLevel'].value_counts(normalize=True) * 100\nprint(\"流失风险等级分布:\")\nfor level, percentage in risk_distribution.items():\n    print(f\"- {level}: {percentage:.2f}%\")\n\n# 可视化风险分数分布\nplt.figure(figsize=(15, 5))\nplt.subplot(1, 2, 1)\nsns.countplot(x='ChurnRiskScore', data=df, palette='YlOrRd')\nplt.title('流失风险分数分布')\nplt.xlabel('风险分数')\nplt.ylabel('客户数量')\n\nplt.subplot(1, 2, 2)\nsns.countplot(x='ChurnRiskLevel', data=df, palette='YlOrRd')\nplt.title('流失风险等级分布')\nplt.xlabel('风险等级')\nplt.ylabel('客户数量')\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n\n# 8. 流失指标的业务解释\nprint(\"\\n=== 流失指标的业务解释 ===\")\nprint(\"根据分析，我们建议使用以下流失定义:\")\nprint(\"1. 活跃度流失: 客户连续90天未进行任何交易\")\nprint(\"2. 价值流失: 客户近期交易金额较过往平均下降50%以上\")\nprint(\"3. 频率流失: 客户近期交易频率较过往平均下降60%以上\")\nprint(\"4. 综合流失指标: 满足上述任一条件即视为流失风险客户\")\n\nprint(\"\\n预警信号层级:\")\nprint(\"- 低风险: 出现1-2个轻微预警信号，建议进行常规客户维护\")\nprint(\"- 中风险: 出现2-4个预警信号，建议进行针对性的客户挽留措施\")\nprint(\"- 高风险: 出现4个以上预警信号，建议立即实施紧急挽留计划\")\n\n# 9. 流失率的时间趋势（假设有历史数据）\nprint(\"\\n=== 流失率时间趋势模拟 ===\")\n# 创建假设的月度流失率数据\nmonths = pd.date_range(start='2023-01-01', periods=12, freq='M')\nmonthly_churn_rates = [5.2, 5.3, 5.1, 5.4, 5.6, 5.8, 6.1, 6.3, 6.0, 5.7, 5.5, 5.3]\n\n# 可视化流失率趋势\nplt.figure(figsize=(12, 6))\nplt.plot(months, monthly_churn_rates, marker='o', linestyle='-', color='salmon')\nplt.title('月度客户流失率趋势')\nplt.xlabel('月份')\nplt.ylabel('流失率 (%)')\nplt.grid(True, linestyle='--', alpha=0.7)\nplt.tight_layout()\nplt.show()\n\n# 10. 流失客户的财务影响\nprint(\"\\n=== 流失客户的财务影响分析 ===\")\n# 假设的客户价值数据\navg_customer_value = 5000  # 平均客户年价值\ncustomer_acquisition_cost = 500  # 获取新客户成本\ntotal_customers = len(df)\n\n# 计算不同流失率下的财务影响\nchurn_rates = [0.05, 0.10, 0.15, 0.20]\nfinancial_impact = []\n\nfor rate in churn_rates:\n    churned_customers = total_customers * rate\n    lost_revenue = churned_customers * avg_customer_value\n    replacement_cost = churned_customers * customer_acquisition_cost\n    total_impact = lost_revenue + replacement_cost\n    \n    financial_impact.append({\n        'Churn_Rate': rate * 100,\n        'Churned_Customers': churned_customers,\n        'Lost_Revenue': lost_revenue,\n        'Replacement_Cost': replacement_cost,\n        'Total_Impact': total_impact\n    })\n\nimpact_df = pd.DataFrame(financial_impact)\nprint(impact_df)\n\n# 可视化财务影响\nplt.figure(figsize=(12, 6))\nx = impact_df['Churn_Rate']\n\nplt.bar(x, impact_df['Lost_Revenue'], label='损失收入', color='salmon', alpha=0.7)\nplt.bar(x, impact_df['Replacement_Cost'], bottom=impact_df['Lost_Revenue'], label='替换成本', color='lightblue', alpha=0.7)\n\nplt.title('不同流失率下的财务影响')\nplt.xlabel('流失率 (%)')\nplt.ylabel('财务影响 (元)')\nplt.legend()\nplt.grid(True, axis='y', linestyle='--', alpha=0.7)\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n通过明确定义流失指标和预警信号，银行可以更早地识别潜在流失客户，并采取相应措施降低流失率，从而减少财务损失。\")"
        },
        {
          "id": 3,
          "title": "预警变量筛选",
          "description": "识别和筛选有预测价值的客户特征变量",
          "example_code": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif, RFE\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom scipy.stats import pointbiserialr\nimport matplotlib as mpl\nfrom app.services.docker_matplotlib_fix import configure_matplotlib_fonts\n\n# 配置字体以支持中文显示\nconfigure_matplotlib_fonts()\nfrom app.services.bank_analysis import bank_service\n\n# 加载客户数据\ndf = bank_service.load_customer_churn_data()\nprint(f\"加载数据集，共{df.shape[0]}行，{df.shape[1]}列\")\n\n# 1. 数据预处理\nprint(\"\\n=== 数据预处理 ===\")\n\n# 检查目标变量\nif 'Churn' not in df.columns:\n    print(\"错误: 数据集中缺少目标变量'Churn'\")\n    exit()\n\n# 移除不需要的列\nnon_feature_cols = ['CustomerID', 'Name', 'Email', 'Address']\nfeature_cols = [col for col in df.columns if col != 'Churn' and col not in non_feature_cols]\n\n# 检查并处理缺失值\nmissing_values = df[feature_cols].isnull().sum()\nif missing_values.sum() > 0:\n    print(\"\\n存在缺失值的特征:\")\n    print(missing_values[missing_values > 0])\n    \n    # 使用中位数填充数值型缺失值\n    numeric_cols = df[feature_cols].select_dtypes(include=['int64', 'float64']).columns\n    for col in numeric_cols:\n        if df[col].isnull().sum() > 0:\n            median_val = df[col].median()\n            df[col].fillna(median_val, inplace=True)\n    \n    # 使用众数填充类别型缺失值\n    categorical_cols = df[feature_cols].select_dtypes(include=['object', 'category']).columns\n    for col in categorical_cols:\n        if df[col].isnull().sum() > 0:\n            mode_val = df[col].mode()[0]\n            df[col].fillna(mode_val, inplace=True)\n\n# 处理类别特征\ncategorical_cols = df[feature_cols].select_dtypes(include=['object', 'category']).columns.tolist()\nif categorical_cols:\n    print(f\"\\n对{len(categorical_cols)}个类别特征进行独热编码\")\n    df_encoded = pd.get_dummies(df, columns=categorical_cols, drop_first=True)\n    \n    # 更新特征列列表\n    feature_cols = [col for col in df_encoded.columns if col != 'Churn' and col not in non_feature_cols]\nelse:\n    df_encoded = df.copy()\n\n# 获取X和y\nX = df_encoded[feature_cols]\ny = df_encoded['Churn']\n\n# 分割训练集和测试集\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n\n# 标准化数值特征\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nprint(f\"\\n预处理后的特征数量: {len(feature_cols)}\")\nprint(f\"训练集: {X_train.shape[0]}行, 测试集: {X_test.shape[0]}行\")\n\n# 2. 单变量特征筛选\nprint(\"\\n=== 单变量特征重要性分析 ===\")\n\n# 2.1 点二列相关系数(连续特征)\nprint(\"\\n连续特征的点二列相关系数:\")\nnumeric_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n\nif numeric_cols:\n    corr_results = []\n    \n    for col in numeric_cols:\n        corr, p_value = pointbiserialr(y, X[col])\n        corr_results.append({\n            'Feature': col,\n            'Correlation': abs(corr),\n            'P_Value': p_value\n        })\n    \n    corr_df = pd.DataFrame(corr_results)\n    corr_df = corr_df.sort_values('Correlation', ascending=False)\n    \n    # 显示相关系数最高的特征\n    print(corr_df.head(10))\n    \n    # 可视化相关系数\n    plt.figure(figsize=(12, 8))\n    top_features = corr_df.head(15)['Feature'].tolist()\n    sns.barplot(x='Correlation', y='Feature', data=corr_df[corr_df['Feature'].isin(top_features)])\n    plt.title('特征与流失的点二列相关系数（绝对值）')\n    plt.xlabel('绝对相关系数')\n    plt.tight_layout()\n    plt.show()\n\n# 2.2 ANOVA F-value\nprint(\"\\nANOVA F-value特征筛选:\")\nanova_selector = SelectKBest(f_classif, k='all')\nanova_selector.fit(X_train_scaled, y_train)\n\n# 计算ANOVA F-value和P-value\nanova_scores = pd.DataFrame()\nanova_scores['Feature'] = X.columns\nanova_scores['F_Value'] = anova_selector.scores_\nanova_scores['P_Value'] = anova_selector.pvalues_\nanova_scores = anova_scores.sort_values('F_Value', ascending=False)\n\n# 显示得分最高的特征\nprint(anova_scores.head(10))\n\n# 可视化得分最高的特征\nplt.figure(figsize=(12, 8))\ntop_features = anova_scores.head(15)['Feature'].tolist()\nsns.barplot(x='F_Value', y='Feature', data=anova_scores[anova_scores['Feature'].isin(top_features)])\nplt.title('ANOVA F-value特征重要性')\nplt.xlabel('F-value')\nplt.tight_layout()\nplt.show()\n\n# 2.3 互信息筛选\nprint(\"\\n互信息特征筛选:\")\nmi_selector = SelectKBest(mutual_info_classif, k='all')\nmi_selector.fit(X_train_scaled, y_train)\n\n# 计算互信息分数\nmi_scores = pd.DataFrame()\nmi_scores['Feature'] = X.columns\nmi_scores['MI_Score'] = mi_selector.scores_\nmi_scores = mi_scores.sort_values('MI_Score', ascending=False)\n\n# 显示得分最高的特征\nprint(mi_scores.head(10))\n\n# 可视化得分最高的特征\nplt.figure(figsize=(12, 8))\ntop_features = mi_scores.head(15)['Feature'].tolist()\nsns.barplot(x='MI_Score', y='Feature', data=mi_scores[mi_scores['Feature'].isin(top_features)])\nplt.title('互信息特征重要性')\nplt.xlabel('互信息分数')\nplt.tight_layout()\nplt.show()\n\n# 3. 基于模型的特征筛选\nprint(\"\\n=== 基于模型的特征重要性分析 ===\")\n\n# 3.1 随机森林特征重要性\nprint(\"\\n随机森林特征重要性:\")\nrf_model = RandomForestClassifier(n_estimators=100, random_state=42)\nrf_model.fit(X_train_scaled, y_train)\n\n# 计算特征重要性\nrf_importances = pd.DataFrame()\nrf_importances['Feature'] = X.columns\nrf_importances['Importance'] = rf_model.feature_importances_\nrf_importances = rf_importances.sort_values('Importance', ascending=False)\n\n# 显示最重要的特征\nprint(rf_importances.head(10))\n\n# 可视化特征重要性\nplt.figure(figsize=(12, 8))\ntop_features = rf_importances.head(15)['Feature'].tolist()\nsns.barplot(x='Importance', y='Feature', data=rf_importances[rf_importances['Feature'].isin(top_features)])\nplt.title('随机森林特征重要性')\nplt.xlabel('重要性')\nplt.tight_layout()\nplt.show()\n\n# 3.2 递归特征消除(RFE)\nprint(\"\\n递归特征消除(RFE):\")\nlogit_model = LogisticRegression(max_iter=1000, random_state=42)\nrfe = RFE(estimator=logit_model, n_features_to_select=15, step=1)\nrfe.fit(X_train_scaled, y_train)\n\n# 获取RFE结果\nrfe_results = pd.DataFrame()\nrfe_results['Feature'] = X.columns\nrfe_results['Selected'] = rfe.support_\nrfe_results['Rank'] = rfe.ranking_\nrfe_results = rfe_results.sort_values('Rank')\n\n# 显示被选中的特征\nprint(rfe_results[rfe_results['Selected'] == True])\n\n# 4. 特征重要性综合分析\nprint(\"\\n=== 特征重要性综合分析 ===\")\n\n# 创建一个综合得分\nfeature_importance = pd.DataFrame({'Feature': X.columns})\n\n# 添加各种特征选择方法的排名\nif numeric_cols:\n    corr_df['Corr_Rank'] = corr_df['Correlation'].rank(ascending=False)\n    feature_importance = feature_importance.merge(corr_df[['Feature', 'Corr_Rank']], on='Feature', how='left')\n\nanova_scores['Anova_Rank'] = anova_scores['F_Value'].rank(ascending=False)\nfeature_importance = feature_importance.merge(anova_scores[['Feature', 'Anova_Rank']], on='Feature', how='left')\n\nmi_scores['MI_Rank'] = mi_scores['MI_Score'].rank(ascending=False)\nfeature_importance = feature_importance.merge(mi_scores[['Feature', 'MI_Rank']], on='Feature', how='left')\n\nrf_importances['RF_Rank'] = rf_importances['Importance'].rank(ascending=False)\nfeature_importance = feature_importance.merge(rf_importances[['Feature', 'RF_Rank']], on='Feature', how='left')\n\nfeature_importance = feature_importance.merge(rfe_results[['Feature', 'Rank']], on='Feature', how='left')\nfeature_importance.rename(columns={'Rank': 'RFE_Rank'}, inplace=True)\n\n# 计算平均排名\nrank_columns = [col for col in feature_importance.columns if col.endswith('_Rank')]\nfeature_importance['Avg_Rank'] = feature_importance[rank_columns].mean(axis=1)\nfeature_importance = feature_importance.sort_values('Avg_Rank')\n\n# 显示综合排名前20的特征\nprint(\"综合特征重要性排名(前20名):\")\nprint(feature_importance.head(20))\n\n# 可视化综合排名靠前的特征\nplt.figure(figsize=(12, 8))\ntop_features = feature_importance.head(15)['Feature'].tolist()\nsns.barplot(y='Feature', x='Avg_Rank', data=feature_importance[feature_importance['Feature'].isin(top_features)])\nplt.title('特征重要性综合排名')\nplt.xlabel('平均排名(数值越小越重要)')\nplt.tight_layout()\nplt.show()\n\n# 5. 最终特征选择\nprint(\"\\n=== 最终特征选择 ===\")\n# 选择排名前15的特征\ntop_n_features = 15\nselected_features = feature_importance.head(top_n_features)['Feature'].tolist()\n\nprint(f\"选择的{top_n_features}个最重要特征:\")\nfor i, feature in enumerate(selected_features, 1):\n    print(f\"{i}. {feature}\")\n\n# 检查所选特征间的相关性\nif len(numeric_cols) > 0:\n    correlation_matrix = X[selected_features].corr()\n    \n    # 可视化相关性矩阵\n    plt.figure(figsize=(12, 10))\n    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n    plt.title('所选特征间的相关性矩阵')\n    plt.tight_layout()\n    plt.show()\n    \n    # 检测高度相关的特征对\n    print(\"\\n高度相关的特征对(相关性绝对值>0.8):\")\n    high_corr_found = False\n    \n    for i in range(len(selected_features)):\n        for j in range(i+1, len(selected_features)):\n            corr = correlation_matrix.iloc[i, j]\n            if abs(corr) > 0.8:\n                print(f\"- {selected_features[i]} 与 {selected_features[j]}: {corr:.3f}\")\n                high_corr_found = True\n    \n    if not high_corr_found:\n        print(\"未发现高度相关的特征对\")\n\n# 6. 特征与流失的可视化分析\nprint(\"\\n=== 特征与流失的关系可视化 ===\")\n\n# 选择前几个最重要的特征进行可视化\nn_features_to_plot = min(6, len(selected_features))  # 最多展示6个特征\n\n# 为分类特征生成箱型图，为连续特征生成直方图\nplt.figure(figsize=(15, n_features_to_plot * 4))\n\nfor i, feature in enumerate(selected_features[:n_features_to_plot]):\n    if feature in df.columns:  # 确保特征在原始数据框中\n        feature_data = df[[feature, 'Churn']]\n        \n        # 移除可能的缺失值\n        feature_data = feature_data.dropna()\n        \n        # 根据特征类型选择不同的可视化方法\n        if feature_data[feature].dtype in ['int64', 'float64'] and feature_data[feature].nunique() > 10:\n            # 连续数值特征 - 使用直方图和密度图\n            plt.subplot(n_features_to_plot, 2, 2*i+1)\n            sns.histplot(data=feature_data, x=feature, hue='Churn', kde=True, bins=30, palette=['lightblue', 'salmon'])\n            plt.title(f'{feature}的分布与流失关系')\n            \n            # 箱形图\n            plt.subplot(n_features_to_plot, 2, 2*i+2)\n            sns.boxplot(x='Churn', y=feature, data=feature_data, palette=['lightblue', 'salmon'])\n            plt.title(f'{feature}在不同流失状态下的分布')\n            \n        else:\n            # 分类特征或低基数特征 - 使用计数图和百分比堆叠图\n            plt.subplot(n_features_to_plot, 2, 2*i+1)\n            sns.countplot(x=feature, hue='Churn', data=feature_data, palette=['lightblue', 'salmon'])\n            plt.title(f'{feature}类别与流失关系')\n            plt.xticks(rotation=45)\n            \n            # 按类别的流失率\n            plt.subplot(n_features_to_plot, 2, 2*i+2)\n            category_churn = feature_data.groupby(feature)['Churn'].mean() * 100\n            sns.barplot(x=category_churn.index, y=category_churn.values, palette='YlOrRd')\n            plt.title(f'按{feature}分类的流失率(%)')\n            plt.xticks(rotation=45)\n            plt.ylabel('流失率 (%)')\n\nplt.tight_layout()\nplt.show()\n\n# 7. 结论与建议\nprint(\"\\n=== 结论与建议 ===\")\nprint(\"基于预警变量筛选的结论:\")\nprint(f\"1. 我们从{len(feature_cols)}个初始特征中筛选出{len(selected_features)}个最具预测价值的特征\")\nprint(\"2. 主要的预警指标包括:\")\n\n# 展示前5个重要特征及其解释\ntop_5_features = feature_importance.head(5)['Feature'].tolist()\nfor i, feature in enumerate(top_5_features, 1):\n    # 这里可以添加对每个特征的业务解释\n    print(f\"   {i}. {feature}\")\n\nprint(\"\\n3. 预警模型构建建议:\")\nprint(\"   - 使用筛选的特征集构建预警模型\")\nprint(\"   - 重点关注交易行为的变化趋势特征\")\nprint(\"   - 考虑客户生命周期的不同阶段特征\")\nprint(\"   - 对相关性高的特征考虑进行特征工程或只选择其中一个\")\n\nprint(\"\\n4. 业务应用建议:\")\nprint(\"   - 建立以上述关键指标为基础的客户流失风险监控仪表板\")\nprint(\"   - 设置差异化的预警阈值，并定期对阈值进行校准\")\nprint(\"   - 将预警结果与客户价值结合，优先关注高价值客户\")"
        },
        {
          "id": 4,
          "title": "预警模型构建",
          "description": "使用随机森林、逻辑回归等算法构建流失预警模型",
          "example_code": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\nfrom sklearn.metrics import confusion_matrix, classification_report, roc_curve, precision_recall_curve\nfrom sklearn.pipeline import Pipeline\nfrom imblearn.over_sampling import SMOTE\nimport matplotlib as mpl\nfrom app.services.docker_matplotlib_fix import configure_matplotlib_fonts\n\n# 配置字体以支持中文显示\nconfigure_matplotlib_fonts()\nfrom app.services.bank_analysis import bank_service\n\n# 加载客户数据\ndf = bank_service.load_customer_churn_data()\nprint(f\"加载数据集，共{df.shape[0]}行，{df.shape[1]}列\")\n\n# 1. 数据准备\nprint(\"\\n=== 数据准备 ===\")\n\n# 检查目标变量\nif 'Churn' not in df.columns:\n    print(\"错误: 数据集中缺少目标变量'Churn'\")\n    exit()\n\n# 检查流失率\nchurn_rate = df['Churn'].mean() * 100\nprint(f\"客户流失率: {churn_rate:.2f}%\")\n\n# 使用之前步骤中筛选出的重要特征\n# 注意：这里假设我们已经从上一步中得到了重要特征列表\n# 在实际应用中，可以从上一步中加载或者重新筛选特征\nimportant_features = [\n    'DaysSinceLastTransaction', 'TransactionFrequencyDecline', 'TransactionAmountDecline',\n    'Tenure', 'CustomerValue', 'TransactionFrequency', 'TransactionAmount',\n    'SupportCallsCount', 'ProductCount', 'Age', 'ComplaintsCount',\n    'IsDigitalActive', 'IsMobileActive', 'HasCreditCard', 'HasLoan'\n]\n\n# 检查所有重要特征是否在数据集中\nmissing_features = [feature for feature in important_features if feature not in df.columns]\nif missing_features:\n    print(f\"警告: 以下特征不在数据集中: {missing_features}\")\n    important_features = [feature for feature in important_features if feature in df.columns]\n\n# 如果没有足够的特征，使用所有可用的数值特征\nif len(important_features) < 5:\n    print(\"特征太少，将使用所有数值特征\")\n    important_features = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n    if 'Churn' in important_features:\n        important_features.remove('Churn')\n\nprint(f\"使用{len(important_features)}个特征构建预警模型:\")\nfor i, feature in enumerate(important_features, 1):\n    print(f\"{i}. {feature}\")\n\n# 准备特征和目标变量\nX = df[important_features]\ny = df['Churn']\n\n# 划分训练集和测试集\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=42, stratify=y\n)\nprint(f\"训练集: {X_train.shape[0]}行, 测试集: {X_test.shape[0]}行\")\n\n# 检查类别不平衡\nneg_count = (y_train == 0).sum()\npos_count = (y_train == 1).sum()\nratio = neg_count / pos_count\nprint(f\"训练集中未流失客户: {neg_count}, 流失客户: {pos_count}, 比例: {ratio:.2f}:1\")\n\n# 2. 创建评估模型的函数\ndef evaluate_model(model, X_train, X_test, y_train, y_test, model_name, use_smote=False):\n    # 复制数据，避免修改原始数据\n    X_train_copy = X_train.copy()\n    y_train_copy = y_train.copy()\n    \n    # 如果使用SMOTE处理类别不平衡\n    if use_smote:\n        print(f\"\\n对{model_name}使用SMOTE处理类别不平衡...\")\n        smote = SMOTE(random_state=42)\n        X_train_copy, y_train_copy = smote.fit_resample(X_train_copy, y_train_copy)\n        print(f\"SMOTE后，训练集形状: {X_train_copy.shape}\")\n        print(f\"流失占比: {y_train_copy.mean():.2f}\")\n    \n    # 训练模型\n    print(f\"\\n训练{model_name}...\")\n    model.fit(X_train_copy, y_train_copy)\n    \n    # 预测\n    y_pred = model.predict(X_test)\n    \n    # 获取预测概率（如果模型支持）\n    if hasattr(model, 'predict_proba'):\n        y_prob = model.predict_proba(X_test)[:, 1]\n    else:\n        y_prob = model.decision_function(X_test) if hasattr(model, 'decision_function') else y_pred\n    \n    # 计算评估指标\n    accuracy = accuracy_score(y_test, y_pred)\n    precision = precision_score(y_test, y_pred)\n    recall = recall_score(y_test, y_pred)\n    f1 = f1_score(y_test, y_pred)\n    auc = roc_auc_score(y_test, y_prob)\n    \n    # 输出评估结果\n    print(f\"\\n{model_name}评估结果:\")\n    print(f\"准确率(Accuracy): {accuracy:.4f}\")\n    print(f\"精确率(Precision): {precision:.4f}\")\n    print(f\"召回率(Recall): {recall:.4f}\")\n    print(f\"F1分数: {f1:.4f}\")\n    print(f\"ROC AUC: {auc:.4f}\")\n    \n    # 混淆矩阵\n    cm = confusion_matrix(y_test, y_pred)\n    tn, fp, fn, tp = cm.ravel()\n    print(f\"\\n混淆矩阵:\")\n    print(f\"真负例(TN): {tn}\")\n    print(f\"假正例(FP): {fp}\")\n    print(f\"假负例(FN): {fn}\")\n    print(f\"真正例(TP): {tp}\")\n    \n    # 计算特定业务指标\n    print(\"\\n业务指标:\")\n    # 假设流失客户的平均损失是5000元，误判非流失的成本是1000元\n    cost_fn = 5000  # 假负例成本 (错过流失客户的损失)\n    cost_fp = 1000  # 假正例成本 (对非流失客户采取不必要措施的成本)\n    total_cost = (fn * cost_fn) + (fp * cost_fp)\n    print(f\"总错误成本: {total_cost}元 (FN: {fn*cost_fn}元, FP: {fp*cost_fp}元)\")\n    \n    return {\n        'model': model,\n        'name': model_name,\n        'accuracy': accuracy,\n        'precision': precision,\n        'recall': recall,\n        'f1': f1,\n        'auc': auc,\n        'confusion_matrix': cm,\n        'y_pred': y_pred,\n        'y_prob': y_prob,\n        'cost': total_cost\n    }\n\n# 3. 训练和评估多个模型\nprint(\"\\n=== 训练和评估模型 ===\")\n\n# 3.1 逻辑回归模型\npipeline_lr = Pipeline([\n    ('scaler', StandardScaler()),\n    ('classifier', LogisticRegression(max_iter=1000, random_state=42))\n])\nresults_lr = evaluate_model(\n    pipeline_lr, X_train, X_test, y_train, y_test, \"逻辑回归\", use_smote=True\n)\n\n# 3.2 随机森林模型\npipeline_rf = Pipeline([\n    ('scaler', StandardScaler()),\n    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))\n])\nresults_rf = evaluate_model(\n    pipeline_rf, X_train, X_test, y_train, y_test, \"随机森林\", use_smote=True\n)\n\n# 3.3 梯度提升树模型\npipeline_gbt = Pipeline([\n    ('scaler', StandardScaler()),\n    ('classifier', GradientBoostingClassifier(n_estimators=100, random_state=42))\n])\nresults_gbt = evaluate_model(\n    pipeline_gbt, X_train, X_test, y_train, y_test, \"梯度提升树\", use_smote=True\n)\n\n# 3.4 支持向量机模型\npipeline_svm = Pipeline([\n    ('scaler', StandardScaler()),\n    ('classifier', SVC(probability=True, random_state=42))\n])\nresults_svm = evaluate_model(\n    pipeline_svm, X_train, X_test, y_train, y_test, \"支持向量机\", use_smote=True\n)\n\n# 4. 比较模型性能\nprint(\"\\n=== 模型性能比较 ===\")\nresults = [results_lr, results_rf, results_gbt, results_svm]\nmodel_names = [result['name'] for result in results]\naccuracies = [result['accuracy'] for result in results]\nprecisions = [result['precision'] for result in results]\nrecalls = [result['recall'] for result in results]\nf1_scores = [result['f1'] for result in results]\naucs = [result['auc'] for result in results]\ncosts = [result['cost'] for result in results]\n\n# 创建比较表格\ncomparison_df = pd.DataFrame({\n    '模型': model_names,\n    '准确率': accuracies,\n    '精确率': precisions,\n    '召回率': recalls,\n    'F1分数': f1_scores,\n    'ROC AUC': aucs,\n    '错误成本': costs\n})\nprint(comparison_df)\n\n# 可视化比较结果\nplt.figure(figsize=(15, 10))\n\n# 准确率比较\nplt.subplot(2, 2, 1)\nsns.barplot(x='模型', y='准确率', data=comparison_df)\nplt.title('模型准确率比较')\nplt.ylim(0.7, 1.0)\n\n# 精确率和召回率比较\nplt.subplot(2, 2, 2)\ncomparison_plot = pd.melt(comparison_df, id_vars=['模型'], value_vars=['精确率', '召回率'], var_name='指标', value_name='值')\nsns.barplot(x='模型', y='值', hue='指标', data=comparison_plot)\nplt.title('模型精确率和召回率比较')\nplt.ylim(0.0, 1.0)\n\n# F1和AUC比较\nplt.subplot(2, 2, 3)\ncomparison_plot = pd.melt(comparison_df, id_vars=['模型'], value_vars=['F1分数', 'ROC AUC'], var_name='指标', value_name='值')\nsns.barplot(x='模型', y='值', hue='指标', data=comparison_plot)\nplt.title('模型F1分数和ROC AUC比较')\nplt.ylim(0.0, 1.0)\n\n# 成本比较\nplt.subplot(2, 2, 4)\nsns.barplot(x='模型', y='错误成本', data=comparison_df, palette='YlOrRd_r')\nplt.title('模型错误成本比较')\nplt.ylabel('错误成本(元)')\n\nplt.tight_layout()\nplt.show()\n\n# 5. 模型调优 (以最佳模型为例)\n# 假设随机森林是初步评估中的最佳模型\nprint(\"\\n=== 随机森林模型调优 ===\")\n\n# 设定参数网格\nparam_grid = {\n    'classifier__n_estimators': [50, 100, 200],\n    'classifier__max_depth': [None, 10, 20, 30],\n    'classifier__min_samples_split': [2, 5, 10],\n    'classifier__min_samples_leaf': [1, 2, 4],\n    'classifier__class_weight': [None, 'balanced', 'balanced_subsample']\n}\n\n# 使用网格搜索和交叉验证\nprint(\"执行网格搜索交叉验证来优化随机森林参数 (这可能需要一些时间)...\")\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\ngrid_search = GridSearchCV(\n    pipeline_rf,\n    param_grid=param_grid,\n    scoring='f1',  # 使用F1分数作为评估指标\n    cv=cv,\n    n_jobs=-1,  # 使用所有可用CPU\n    verbose=1\n)\n\n# 使用SMOTE处理过的数据进行训练\nsmote = SMOTE(random_state=42)\nX_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\ngrid_search.fit(X_train_smote, y_train_smote)\n\n# 输出最佳参数\nprint(f\"\\n最佳参数: {grid_search.best_params_}\")\nprint(f\"最佳交叉验证F1分数: {grid_search.best_score_:.4f}\")\n\n# 使用最佳参数的模型进行评估\nbest_rf = grid_search.best_estimator_\nresults_best_rf = evaluate_model(\n    best_rf, X_train, X_test, y_train, y_test, \"调优后的随机森林\", use_smote=True\n)\n\n# 6. ROC曲线和PR曲线分析\nprint(\"\\n=== ROC曲线和PR曲线分析 ===\")\nplt.figure(figsize=(15, 6))\n\n# ROC曲线\nplt.subplot(1, 2, 1)\nfor result in results + [results_best_rf]:\n    fpr, tpr, _ = roc_curve(y_test, result['y_prob'])\n    plt.plot(fpr, tpr, label=f'{result[\"name\"]} (AUC = {result[\"auc\"]:.3f})')\n\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlabel('假正例率(False Positive Rate)')\nplt.ylabel('真正例率(True Positive Rate)')\nplt.title('ROC曲线比较')\nplt.legend()\nplt.grid(alpha=0.3)\n\n# PR曲线\nplt.subplot(1, 2, 2)\nfor result in results + [results_best_rf]:\n    precision_curve, recall_curve, _ = precision_recall_curve(y_test, result['y_prob'])\n    plt.plot(recall_curve, precision_curve, label=f'{result[\"name\"]} (F1 = {result[\"f1\"]:.3f})')\n\nplt.xlabel('召回率(Recall)')\nplt.ylabel('精确率(Precision)')\nplt.title('精确率-召回率曲线比较')\nplt.legend()\nplt.grid(alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# 7. 阈值分析 (以最佳模型为例)\nprint(\"\\n=== 预测阈值分析 ===\")\n# 获取最佳模型的预测概率\nbest_model = results_best_rf\ny_prob = best_model['y_prob']\n\n# 测试不同的阈值\nthresholds = np.arange(0.1, 1.0, 0.1)\nthreshold_metrics = []\n\nfor threshold in thresholds:\n    # 根据阈值生成预测\n    y_pred_threshold = (y_prob >= threshold).astype(int)\n    \n    # 计算指标\n    accuracy = accuracy_score(y_test, y_pred_threshold)\n    precision = precision_score(y_test, y_pred_threshold)\n    recall = recall_score(y_test, y_pred_threshold)\n    f1 = f1_score(y_test, y_pred_threshold)\n    \n    # 计算成本\n    cm = confusion_matrix(y_test, y_pred_threshold)\n    tn, fp, fn, tp = cm.ravel()\n    cost_fn = 5000  # 假负例成本\n    cost_fp = 1000  # 假正例成本\n    total_cost = (fn * cost_fn) + (fp * cost_fp)\n    \n    threshold_metrics.append({\n        '阈值': threshold,\n        '准确率': accuracy,\n        '精确率': precision,\n        '召回率': recall,\n        'F1分数': f1,\n        '假正例(FP)': fp,\n        '假负例(FN)': fn,\n        '错误成本': total_cost\n    })\n\n# 创建阈值分析表格\nthreshold_df = pd.DataFrame(threshold_metrics)\nprint(threshold_df)\n\n# 可视化阈值分析\nplt.figure(figsize=(15, 10))\n\n# 绘制准确率、精确率、召回率和F1随阈值的变化\nplt.subplot(2, 2, 1)\nplt.plot(threshold_df['阈值'], threshold_df['准确率'], marker='o', label='准确率')\nplt.plot(threshold_df['阈值'], threshold_df['精确率'], marker='s', label='精确率')\nplt.plot(threshold_df['阈值'], threshold_df['召回率'], marker='^', label='召回率')\nplt.plot(threshold_df['阈值'], threshold_df['F1分数'], marker='*', label='F1分数')\nplt.xlabel('预测阈值')\nplt.ylabel('指标值')\nplt.title('性能指标与预测阈值的关系')\nplt.legend()\nplt.grid(alpha=0.3)\n\n# 绘制FP和FN随阈值的变化\nplt.subplot(2, 2, 2)\nplt.plot(threshold_df['阈值'], threshold_df['假正例(FP)'], marker='o', label='假正例(FP)')\nplt.plot(threshold_df['阈值'], threshold_df['假负例(FN)'], marker='s', label='假负例(FN)')\nplt.xlabel('预测阈值')\nplt.ylabel('错误计数')\nplt.title('错误类型与预测阈值的关系')\nplt.legend()\nplt.grid(alpha=0.3)\n\n# 绘制错误成本随阈值的变化\nplt.subplot(2, 2, 3)\nplt.plot(threshold_df['阈值'], threshold_df['错误成本'], marker='o', color='red')\nplt.xlabel('预测阈值')\nplt.ylabel('错误成本(元)')\nplt.title('错误成本与预测阈值的关系')\nplt.grid(alpha=0.3)\n\n# 找出成本最低的阈值\nmin_cost_threshold = threshold_df.loc[threshold_df['错误成本'].idxmin(), '阈值']\nmin_cost = threshold_df['错误成本'].min()\nplt.axvline(min_cost_threshold, color='green', linestyle='--', label=f'最优阈值={min_cost_threshold}')\nplt.legend()\n\n# 显示不同阈值下的混淆矩阵热图\nplt.subplot(2, 2, 4)\nopt_index = threshold_df['阈值'].tolist().index(min_cost_threshold)\ny_pred_opt = (y_prob >= min_cost_threshold).astype(int)\ncm_opt = confusion_matrix(y_test, y_pred_opt)\nsns.heatmap(cm_opt, annot=True, fmt='d', cmap='Blues')\nplt.title(f'最优阈值({min_cost_threshold})下的混淆矩阵')\nplt.xlabel('预测标签')\nplt.ylabel('真实标签')\n\nplt.tight_layout()\nplt.show()\n\n# 8. 特征重要性分析（针对随机森林模型）\nprint(\"\\n=== 特征重要性分析 ===\")\n\n# 获取最佳随机森林模型的特征重要性\nbest_rf_classifier = best_rf.named_steps['classifier']\nfeature_importances = best_rf_classifier.feature_importances_\n\n# 创建特征重要性数据框\nimportance_df = pd.DataFrame({\n    '特征': important_features,\n    '重要性': feature_importances\n})\nimportance_df = importance_df.sort_values('重要性', ascending=False)\n\n# 显示特征重要性\nprint(\"随机森林特征重要性:\")\nprint(importance_df)\n\n# 可视化特征重要性\nplt.figure(figsize=(12, 8))\nsns.barplot(x='重要性', y='特征', data=importance_df)\nplt.title('随机森林特征重要性')\nplt.xlabel('重要性')\nplt.tight_layout()\nplt.show()\n\n# 9. 最终模型保存和使用示例\nprint(\"\\n=== 最终模型 ===\")\n\n# 使用最优阈值\noptimal_threshold = min_cost_threshold\nprint(f\"最优预测阈值: {optimal_threshold}\")\n\n# 应用最终模型进行预测的示例\ndef predict_churn_risk(model, threshold, customer_data):\n    \"\"\"使用训练好的模型和优化的阈值预测客户流失风险\"\"\"\n    # 确保数据格式正确\n    if isinstance(customer_data, pd.DataFrame):\n        # 确保包含所有需要的特征\n        missing_cols = set(important_features) - set(customer_data.columns)\n        if missing_cols:\n            raise ValueError(f\"客户数据缺少以下特征: {missing_cols}\")\n        \n        # 仅选择模型所需特征\n        X = customer_data[important_features]\n    else:\n        raise ValueError(\"客户数据必须是pandas DataFrame格式\")\n    \n    # 预测概率\n    churn_proba = model.predict_proba(X)[:, 1]\n    \n    # 根据阈值确定预测结果\n    predictions = (churn_proba >= threshold).astype(int)\n    \n    # 返回预测结果和概率\n    results = pd.DataFrame({\n        'customer_id': customer_data.index if customer_data.index.name else range(len(X)),\n        'churn_probability': churn_proba,\n        'churn_prediction': predictions\n    })\n    \n    # 添加风险等级\n    results['risk_level'] = pd.cut(\n        results['churn_probability'],\n        bins=[0, 0.3, 0.6, 0.8, 1.0],\n        labels=['低风险', '中风险', '高风险', '极高风险']\n    )\n    \n    return results\n\n# 演示最终模型预测\nprint(\"\\n预测示例:\")\n# 使用测试集的前10行作为示例\nsample_customers = X_test.head(10).copy()\n\n# 预测这些客户的流失风险\nprediction_results = predict_churn_risk(best_rf, optimal_threshold, sample_customers)\n\n# 添加实际标签用于比较\nprediction_results['actual_churn'] = y_test.head(10).values\n\n# 显示预测结果\nprint(prediction_results[['churn_probability', 'churn_prediction', 'risk_level', 'actual_churn']])\n\n# 10. 模型总结与业务建议\nprint(\"\\n=== 模型总结与业务建议 ===\")\nprint(\"客户流失预警模型总结:\")\nprint(f\"1. 最佳模型: {best_model['name']}\")\nprint(f\"2. 模型性能: F1分数 = {best_model['f1']:.4f}, ROC AUC = {best_model['auc']:.4f}\")\nprint(f\"3. 最优预测阈值: {optimal_threshold} (基于错误成本最小化)\")\nprint(\"4. 关键预警特征:\")\nfor i, row in importance_df.head(5).iterrows():\n    print(f\"   - {row['特征']}: {row['重要性']:.4f}\")\n\nprint(\"\\n客户挽留策略建议:\")\nprint(\"1. 对于高风险和极高风险客户，实施主动挽留措施\")\nprint(\"2. 针对不同风险等级的客户制定差异化的挽留策略\")\nprint(\"3. 定期更新和评估模型，调整预测阈值以适应业务变化\")\nprint(\"4. 结合客户价值，优先挽留高价值且高流失风险的客户\")\nprint(\"5. 将流失风险预测集成到客户关系管理系统中，支持业务决策\")"
        },
        {
          "id": 5,
          "title": "模型验证",
          "description": "使用ROC曲线、混淆矩阵等方法评估模型效果",
          "example_code": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\nfrom sklearn.metrics import confusion_matrix, classification_report, roc_curve, precision_recall_curve\nfrom sklearn.calibration import calibration_curve\nfrom sklearn.pipeline import Pipeline\nfrom imblearn.over_sampling import SMOTE\nimport matplotlib as mpl\nfrom app.services.docker_matplotlib_fix import configure_matplotlib_fonts\n\n# 配置字体以支持中文显示\nconfigure_matplotlib_fonts()\nfrom app.services.bank_analysis import bank_service\n\n# 从上一步骤中加载已训练的模型和相关数据\n# 在实际应用中，可能需要重新加载数据和模型或者从上一步骤中获取\n\n# 加载客户数据\ndf = bank_service.load_customer_churn_data()\nprint(f\"加载数据集，共{df.shape[0]}行，{df.shape[1]}列\")\n\n# 确定重要特征\nimportant_features = [\n    'DaysSinceLastTransaction', 'TransactionFrequencyDecline', 'TransactionAmountDecline',\n    'Tenure', 'CustomerValue', 'TransactionFrequency', 'TransactionAmount',\n    'SupportCallsCount', 'ProductCount', 'Age', 'ComplaintsCount',\n    'IsDigitalActive', 'IsMobileActive', 'HasCreditCard', 'HasLoan'\n]\n\n# 检查所有重要特征是否在数据集中\nmissing_features = [feature for feature in important_features if feature not in df.columns]\nif missing_features:\n    print(f\"警告: 以下特征不在数据集中: {missing_features}\")\n    important_features = [feature for feature in important_features if feature in df.columns]\n\n# 准备特征和目标变量\nX = df[important_features]\ny = df['Churn']\n\n# 划分训练集和测试集\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=42, stratify=y\n)\n\n# 1. 模型训练 (重新训练简化版模型，用于演示验证过程)\nprint(\"\\n=== 模型训练 ===\")\n\n# 使用随机森林作为示例模型\nfrom sklearn.ensemble import RandomForestClassifier\n\n# 应用SMOTE处理类别不平衡\nsmote = SMOTE(random_state=42)\nX_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n\n# 创建数据处理和模型训练管道\npipeline = Pipeline([\n    ('scaler', StandardScaler()),\n    ('classifier', RandomForestClassifier(n_estimators=100, max_depth=20, random_state=42))\n])\n\n# 训练模型\npipeline.fit(X_train_smote, y_train_smote)\n\n# 在测试集上进行预测\ny_pred = pipeline.predict(X_test)\ny_prob = pipeline.predict_proba(X_test)[:, 1]\n\n# 2. 基本性能评估\nprint(\"\\n=== 基本性能评估 ===\")\n\n# 计算常用评估指标\naccuracy = accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred)\nrecall = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\nauc = roc_auc_score(y_test, y_prob)\n\nprint(f\"准确率: {accuracy:.4f}\")\nprint(f\"精确率: {precision:.4f}\")\nprint(f\"召回率: {recall:.4f}\")\nprint(f\"F1分数: {f1:.4f}\")\nprint(f\"ROC AUC: {auc:.4f}\")\n\n# 3. 混淆矩阵分析\nprint(\"\\n=== 混淆矩阵分析 ===\")\n\n# 计算混淆矩阵\ncm = confusion_matrix(y_test, y_pred)\ntn, fp, fn, tp = cm.ravel()\n\nprint(f\"真负例(TN): {tn}\")\nprint(f\"假正例(FP): {fp}\")\nprint(f\"假负例(FN): {fn}\")\nprint(f\"真正例(TP): {tp}\")\n\n# 计算更详细的指标\nsensitivity = recall  # 敏感度 = 召回率 = TP/(TP+FN)\nspecificity = tn / (tn + fp)  # 特异度 = TN/(TN+FP)\nprecision = tp / (tp + fp)  # 精确率 = TP/(TP+FP)\nnpv = tn / (tn + fn)  # 负预测值 = TN/(TN+FN)\nfallout = fp / (fp + tn)  # 误诊率 = FP/(FP+TN)\n\nprint(f\"敏感度(Sensitivity/Recall): {sensitivity:.4f}\")\nprint(f\"特异度(Specificity): {specificity:.4f}\")\nprint(f\"精确率(Precision): {precision:.4f}\")\nprint(f\"负预测值(NPV): {npv:.4f}\")\nprint(f\"误诊率(Fall-out): {fallout:.4f}\")\n\n# 可视化混淆矩阵\nplt.figure(figsize=(10, 8))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\nplt.title('混淆矩阵')\nplt.xlabel('预测标签')\nplt.ylabel('真实标签')\nplt.xticks([0.5, 1.5], ['未流失(0)', '已流失(1)'])\nplt.yticks([0.5, 1.5], ['未流失(0)', '已流失(1)'])\nplt.tight_layout()\nplt.show()\n\n# 4. ROC曲线分析\nprint(\"\\n=== ROC曲线分析 ===\")\n\n# 计算ROC曲线\nfpr, tpr, thresholds = roc_curve(y_test, y_prob)\n\n# 计算每个阈值下的特异度\nspecificity = 1 - fpr\n\n# 可视化ROC曲线\nplt.figure(figsize=(10, 8))\nplt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC曲线 (AUC = {auc:.3f})')\nplt.plot([0, 1], [0, 1], color='gray', linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('假正例率 (1 - 特异度)')\nplt.ylabel('真正例率 (敏感度)')\nplt.title('ROC曲线')\nplt.legend(loc='lower right')\nplt.grid(alpha=0.3)\n\n# 在ROC曲线上标记几个关键点\nidx_max_youden = np.argmax(tpr - fpr)  # Youden指数最大点\nidx_min_dist = np.argmin((1-tpr)**2 + fpr**2)  # 距离左上角最近点\n\nplt.scatter(fpr[idx_max_youden], tpr[idx_max_youden], marker='o', color='red', \n           label=f'最大Youden指数 (阈值={thresholds[idx_max_youden]:.2f})')\nplt.scatter(fpr[idx_min_dist], tpr[idx_min_dist], marker='s', color='green', \n           label=f'最佳平衡点 (阈值={thresholds[idx_min_dist]:.2f})')\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n# 5. 精确率-召回率曲线分析\nprint(\"\\n=== 精确率-召回率曲线分析 ===\")\n\n# 计算精确率-召回率曲线\nprecision_curve, recall_curve, pr_thresholds = precision_recall_curve(y_test, y_prob)\n\n# 可视化精确率-召回率曲线\nplt.figure(figsize=(10, 8))\nplt.plot(recall_curve, precision_curve, color='green', lw=2, label='PR曲线')\n\n# 计算平均精确率\navg_precision = np.mean(precision_curve)\nplt.axhline(y=avg_precision, color='gray', linestyle='--', label=f'平均精确率 = {avg_precision:.3f}')\n\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('召回率')\nplt.ylabel('精确率')\nplt.title('精确率-召回率曲线')\nplt.legend(loc='lower left')\nplt.grid(alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n# 6. 阈值与不同指标的关系分析\nprint(\"\\n=== 阈值分析 ===\")\n\n# 收集不同阈值下的各种指标\nthreshold_metrics = []\n\nfor i in range(len(thresholds)):\n    # 使用当前阈值生成预测\n    y_pred_threshold = (y_prob >= thresholds[i]).astype(int)\n    \n    # 计算指标\n    acc = accuracy_score(y_test, y_pred_threshold)\n    prec = precision_score(y_test, y_pred_threshold, zero_division=0)\n    rec = recall_score(y_test, y_pred_threshold)\n    f1 = f1_score(y_test, y_pred_threshold)\n    \n    # 记录结果\n    threshold_metrics.append({\n        '阈值': thresholds[i],\n        '准确率': acc,\n        '精确率': prec,\n        '召回率': rec,\n        'F1分数': f1,\n        '特异度': specificity[i]\n    })\n\n# 转换为DataFrame\nthreshold_df = pd.DataFrame(threshold_metrics)\n\n# 可视化阈值与各指标的关系\nplt.figure(figsize=(12, 8))\nplt.plot(threshold_df['阈值'], threshold_df['准确率'], marker='o', markersize=4, label='准确率')\nplt.plot(threshold_df['阈值'], threshold_df['精确率'], marker='s', markersize=4, label='精确率')\nplt.plot(threshold_df['阈值'], threshold_df['召回率'], marker='^', markersize=4, label='召回率')\nplt.plot(threshold_df['阈值'], threshold_df['F1分数'], marker='*', markersize=4, label='F1分数')\nplt.plot(threshold_df['阈值'], threshold_df['特异度'], marker='d', markersize=4, label='特异度')\n\nplt.axvline(x=thresholds[idx_max_youden], color='red', linestyle='--', label=f'最大Youden指数阈值 = {thresholds[idx_max_youden]:.2f}')\nplt.axvline(x=thresholds[idx_min_dist], color='green', linestyle='--', label=f'最佳平衡点阈值 = {thresholds[idx_min_dist]:.2f}')\n\nplt.xlabel('预测阈值')\nplt.ylabel('指标值')\nplt.title('不同阈值下的模型性能指标')\nplt.legend(loc='center right')\nplt.grid(alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n# 7. 校准曲线分析\nprint(\"\\n=== 概率校准分析 ===\")\n\n# 计算校准曲线\nprob_true, prob_pred = calibration_curve(y_test, y_prob, n_bins=10)\n\n# 可视化校准曲线\nplt.figure(figsize=(10, 8))\nplt.plot([0, 1], [0, 1], 'k--', label='完美校准')\nplt.plot(prob_pred, prob_true, 's-', label='模型校准曲线')\nplt.xlabel('预测概率')\nplt.ylabel('实际频率')\nplt.title('校准曲线 - 预测概率与实际频率的对比')\nplt.legend()\nplt.grid(alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n# 8. 交叉验证评估\nprint(\"\\n=== 交叉验证评估 ===\")\n\n# 定义交叉验证策略\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n# 执行交叉验证，针对不同评估指标\nscoring_metrics = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']\ncv_results = {}\n\nprint(\"执行5折交叉验证...\")\nfor metric in scoring_metrics:\n    cv_scores = cross_val_score(pipeline, X, y, cv=cv, scoring=metric)\n    cv_results[metric] = {\n        'mean': cv_scores.mean(),\n        'std': cv_scores.std(),\n        'scores': cv_scores\n    }\n    print(f\"{metric}: {cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")\n\n# 可视化交叉验证结果\nplt.figure(figsize=(12, 6))\nmean_scores = [cv_results[m]['mean'] for m in scoring_metrics]\nstd_scores = [cv_results[m]['std'] for m in scoring_metrics]\n\n# 创建条形图\nbars = plt.bar(scoring_metrics, mean_scores, yerr=std_scores, capsize=10, color='skyblue')\n\n# 在条形上添加具体数值\nfor i, bar in enumerate(bars):\n    height = bar.get_height()\n    plt.text(bar.get_x() + bar.get_width()/2., height + 0.01, f'{mean_scores[i]:.4f}', \n             ha='center', va='bottom', rotation=0, fontsize=9)\n\nplt.xlabel('评估指标')\nplt.ylabel('分数')\nplt.title('交叉验证评估结果')\nplt.ylim(0, 1.05)\nplt.grid(axis='y', alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n# 9. 特征贡献分析 (SHAP值)\nprint(\"\\n=== 特征贡献分析 ===\")\nprint(\"注: 完整的SHAP分析可能需要安装shap库，此处仅展示随机森林的特征重要性\")\n\n# 获取模型的特征重要性\nfeature_importance = pipeline.named_steps['classifier'].feature_importances_\n\n# 创建特征重要性数据框\nimportance_df = pd.DataFrame({\n    '特征': important_features,\n    '重要性': feature_importance\n})\nimportance_df = importance_df.sort_values('重要性', ascending=False)\n\n# 可视化特征重要性\nplt.figure(figsize=(12, 8))\nsns.barplot(x='重要性', y='特征', data=importance_df)\nplt.title('特征重要性分析')\nplt.xlabel('重要性')\nplt.tight_layout()\nplt.show()\n\n# 10. 结论与建议\nprint(\"\\n=== 模型验证结论 ===\")\n\nprint(\"基于模型验证的结论:\")\nprint(f\"1. 模型的整体性能: ROC AUC = {auc:.4f}, F1分数 = {f1:.4f}\")\n\n# 确定最佳阈值\nbest_f1_idx = threshold_df['F1分数'].idxmax()\nbest_f1_threshold = threshold_df.loc[best_f1_idx, '阈值']\n\n# Youden指数阈值\nyouden_threshold = thresholds[idx_max_youden]\n\nprint(f\"2. 最佳预测阈值:\")\nprint(f\"   - 基于F1分数: {best_f1_threshold:.4f}\")\nprint(f\"   - 基于Youden指数: {youden_threshold:.4f}\")\n\nprint(\"3. 模型优势:\")\nif sensitivity > 0.7:\n    print(f\"   - 较高的敏感度({sensitivity:.4f})，能够有效识别潜在流失客户\")\nif specificity > 0.7:\n    print(f\"   - 较高的特异度({specificity:.4f})，减少误报\")\nif auc > 0.8:\n    print(f\"   - 优秀的区分能力(AUC = {auc:.4f})\")\n\nprint(\"4. 模型不足:\")\nif sensitivity < 0.7:\n    print(f\"   - 敏感度较低({sensitivity:.4f})，可能漏检部分流失客户\")\nif specificity < 0.7:\n    print(f\"   - 特异度较低({specificity:.4f})，可能产生过多误报\")\nif np.abs(prob_true - prob_pred).mean() > 0.1:\n    print(\"   - 预测概率校准不足，需要进一步调整\")\n\nprint(\"\\n5. 建议:\")\nprint(\"   - 针对业务需求选择合适的预测阈值\")\nprint(\"   - 若优先考虑流失客户的召回，可选择较低阈值\")\nprint(\"   - 若优先考虑干预措施的精确性，可选择较高阈值\")\nprint(\"   - 在应用中结合客户价值等因素确定最终干预策略\")"
        },
        {
          "id": 6,
          "title": "客户挽留策略",
          "description": "基于模型结果设计针对性的客户挽留方案",
          "example_code": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nimport matplotlib as mpl\nfrom datetime import datetime, timedelta\nfrom app.services.docker_matplotlib_fix import configure_matplotlib_fonts\n\n# 配置字体以支持中文显示\nconfigure_matplotlib_fonts()\nfrom app.services.bank_analysis import bank_service\n\n# 从前面步骤加载预测结果和客户数据\n# 在实际应用中，需要加载前面步骤中的模型预测结果\n\n# 加载客户数据\ndf = bank_service.load_customer_churn_data()\nprint(f\"加载数据集，共{df.shape[0]}行，{df.shape[1]}列\")\n\n# 1. 创建模拟的流失风险预测结果\nprint(\"\\n=== 生成模拟的流失风险预测结果 ===\")\n\n# 假设我们有一个随机森林模型预测的流失概率\n# 这里我们模拟一个预测结果\nnp.random.seed(42)\ndf['churn_probability'] = np.clip(df['Churn'].astype(float) + np.random.normal(0, 0.2, size=len(df)), 0, 1)\n\n# 根据概率分配风险等级\ndf['risk_level'] = pd.cut(\n    df['churn_probability'], \n    bins=[0, 0.3, 0.6, 0.8, 1.0], \n    labels=['低风险', '中风险', '高风险', '极高风险']\n)\n\n# 查看风险分布情况\nrisk_counts = df['risk_level'].value_counts().sort_index()\nrisk_percentage = risk_counts / len(df) * 100\n\nprint(\"客户流失风险分布:\")\nfor level, count in risk_counts.items():\n    percentage = risk_percentage[level]\n    print(f\"{level}: {count}人 ({percentage:.2f}%)\")\n\n# 可视化风险分布\nplt.figure(figsize=(12, 6))\nsns.countplot(x='risk_level', data=df, palette='YlOrRd')\nplt.title('客户流失风险等级分布')\nplt.xlabel('风险等级')\nplt.ylabel('客户数量')\nplt.tight_layout()\nplt.show()\n\n# 2. 客户价值分析\nprint(\"\\n=== 客户价值分析 ===\")\n\n# 假设CustomerValue字段代表客户价值分数\nif 'CustomerValue' in df.columns:\n    # 将客户按价值划分为高、中、低三组\n    df['value_segment'] = pd.qcut(\n        df['CustomerValue'],\n        q=3,\n        labels=['低价值', '中价值', '高价值']\n    )\n    \n    # 查看价值分布情况\n    value_counts = df['value_segment'].value_counts().sort_index()\n    value_percentage = value_counts / len(df) * 100\n    \n    print(\"客户价值分布:\")\n    for level, count in value_counts.items():\n        percentage = value_percentage[level]\n        print(f\"{level}: {count}人 ({percentage:.2f}%)\")\n    \n    # 查看风险等级与价值等级的交叉分布\n    risk_value_cross = pd.crosstab(\n        df['risk_level'], \n        df['value_segment'], \n        normalize='all'*100\n    )\n    \n    print(\"\\n风险等级与价值等级交叉分布 (占总客户百分比%):\")\n    print(risk_value_cross)\n    \n    # 可视化风险-价值矩阵\n    plt.figure(figsize=(12, 8))\n    sns.heatmap(risk_value_cross, annot=True, fmt='.2f', cmap='YlGnBu', cbar_kws={'label': '占比(%)'}, vmin=0)\n    plt.title('客户风险-价值矩阵 (占总体客户百分比)')\n    plt.tight_layout()\n    plt.show()\nelse:\n    print(\"数据集中缺少客户价值信息\")\n    # 创建模拟的客户价值\n    df['CustomerValue'] = np.random.uniform(100, 10000, size=len(df))\n    df['value_segment'] = pd.qcut(\n        df['CustomerValue'],\n        q=3,\n        labels=['低价值', '中价值', '高价值']\n    )\n\n# 3. 客户细分与画像\nprint(\"\\n=== 客户细分与画像 ===\")\n\n# 选择用于聚类的特征\nclustering_features = ['Age', 'Tenure', 'CustomerValue', 'TransactionFrequency']\nfor feature in clustering_features:\n    if feature not in df.columns:\n        print(f\"警告: 特征 {feature} 不在数据集中\")\n\n# 使用可用的特征进行聚类\navailable_features = [f for f in clustering_features if f in df.columns]\nif not available_features:\n    # 如果没有可用特征，创建一些模拟特征\n    print(\"使用模拟特征进行客户细分\")\n    df['Age'] = np.random.randint(18, 80, size=len(df))\n    df['Tenure'] = np.random.randint(1, 20, size=len(df))\n    df['TransactionFrequency'] = np.random.randint(1, 50, size=len(df))\n    available_features = ['Age', 'Tenure', 'CustomerValue', 'TransactionFrequency']\n\n# 准备用于聚类的数据\nX_cluster = df[available_features].copy()\n\n# 标准化特征\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X_cluster)\n\n# 使用K-means进行聚类 (假设分为4个客户群体)\nkmeans = KMeans(n_clusters=4, random_state=42, n_init=10)\ndf['cluster'] = kmeans.fit_predict(X_scaled)\n\n# 分析各客户群体的特征\ncluster_summary = df.groupby('cluster').agg({\n    'churn_probability': 'mean',\n    'CustomerValue': 'mean',\n    'risk_level': lambda x: x.value_counts().index[0],  # 最常见的风险等级\n    'value_segment': lambda x: x.value_counts().index[0],  # 最常见的价值等级\n})\n\n# 添加其他可用的特征到摘要中\nfor feature in available_features:\n    if feature != 'CustomerValue':  # 已经包含了\n        cluster_summary[feature] = df.groupby('cluster')[feature].mean()\n\n# 为每个群体定义一个标签\ncluster_labels = {\n    0: '群体A',\n    1: '群体B', \n    2: '群体C',\n    3: '群体D'\n}\ncluster_summary['群体标签'] = [cluster_labels[i] for i in cluster_summary.index]\nprint(\"客户群体特征摘要:\")\nprint(cluster_summary)\n\n# 可视化客户群体\n# 使用PCA降维以便可视化\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X_scaled)\n\n# 添加降维结果到数据框\ndf['PCA1'] = X_pca[:, 0]\ndf['PCA2'] = X_pca[:, 1]\n\n# 可视化聚类结果\nplt.figure(figsize=(14, 10))\n\n# 散点图\nplt.subplot(2, 2, 1)\nsns.scatterplot(x='PCA1', y='PCA2', hue='cluster', data=df, palette='viridis', s=50, alpha=0.7)\nplt.title('客户群体聚类可视化 (PCA降维)')\nplt.legend(title='客户群体')\n\n# 风险等级分布\nplt.subplot(2, 2, 2)\nrisk_by_cluster = pd.crosstab(df['cluster'], df['risk_level'])\nrisk_by_cluster.plot(kind='bar', stacked=True, colormap='YlOrRd', ax=plt.gca())\nplt.title('各客户群体的风险等级分布')\nplt.xlabel('客户群体')\nplt.ylabel('客户数量')\nplt.legend(title='风险等级')\n\n# 客户价值分布\nplt.subplot(2, 2, 3)\nvalue_by_cluster = pd.crosstab(df['cluster'], df['value_segment'])\nvalue_by_cluster.plot(kind='bar', stacked=True, colormap='YlGnBu', ax=plt.gca())\nplt.title('各客户群体的价值等级分布')\nplt.xlabel('客户群体')\nplt.ylabel('客户数量')\nplt.legend(title='价值等级')\n\n# 流失概率分布\nplt.subplot(2, 2, 4)\nsns.boxplot(x='cluster', y='churn_probability', data=df, palette='viridis')\nplt.title('各客户群体的流失概率分布')\nplt.xlabel('客户群体')\nplt.ylabel('流失概率')\n\nplt.tight_layout()\nplt.show()\n\n# 4. 设计针对性的挽留策略矩阵\nprint(\"\\n=== 客户挽留策略矩阵 ===\")\n\n# 定义基于风险和价值的挽留策略矩阵\nretention_matrix = pd.DataFrame(\n    index=['低风险', '中风险', '高风险', '极高风险'],\n    columns=['低价值', '中价值', '高价值']\n)\n\n# 填充策略矩阵\nretention_matrix.loc['低风险', '低价值'] = \"常规服务维护\"\nretention_matrix.loc['低风险', '中价值'] = \"定期回访+产品推荐\"\nretention_matrix.loc['低风险', '高价值'] = \"VIP专属服务+增值服务\"\n\nretention_matrix.loc['中风险', '低价值'] = \"电子邮件活动+优惠券\"\nretention_matrix.loc['中风险', '中价值'] = \"个性化推荐+专属优惠\"\nretention_matrix.loc['中风险', '高价值'] = \"客户经理专访+金融咨询\"\n\nretention_matrix.loc['高风险', '低价值'] = \"短信提醒+小额优惠\"\nretention_matrix.loc['高风险', '中价值'] = \"电话回访+专属优惠包\"\nretention_matrix.loc['高风险', '高价值'] = \"1对1沟通+专属解决方案\"\n\nretention_matrix.loc['极高风险', '低价值'] = \"退出调查+社交媒体互动\"\nretention_matrix.loc['极高风险', '中价值'] = \"紧急挽留方案+产品升级\"\nretention_matrix.loc['极高风险', '高价值'] = \"总监级拜访+定制化挽留计划\"\n\nprint(\"客户挽留策略矩阵:\")\nprint(retention_matrix)\n\n# 可视化策略矩阵\nplt.figure(figsize=(12, 8))\nax = plt.subplot(111)\n\n# 创建表格\ntable = ax.table(\n    cellText=retention_matrix.values,\n    rowLabels=retention_matrix.index,\n    colLabels=retention_matrix.columns,\n    cellLoc='center',\n    rowLoc='center',\n    loc='center'\n)\n\n# 设置表格样式\ntable.auto_set_font_size(False)\ntable.set_fontsize(10)\ntable.scale(1, 2)\n\n# 设置表格颜色\ncmap = plt.cm.get_cmap('YlGnBu')\n\nfor i in range(len(retention_matrix.index)):\n    for j in range(len(retention_matrix.columns)):\n        # 根据位置设置颜色深浅\n        intensity = (i+1) * (j+1) / ((len(retention_matrix.index)) * (len(retention_matrix.columns)))\n        table[(i+1, j)].set_facecolor(cmap(intensity))\n\nax.set_title('客户挽留策略矩阵', fontsize=16)\nax.axis('off')\nplt.tight_layout()\nplt.show()\n\n# 5. 针对不同客户群体的挽留策略\nprint(\"\\n=== 客户群体挽留策略 ===\")\n\n# 定义各客户群体的挽留策略\ncluster_strategies = {\n    0: {\n        'name': cluster_labels[0],\n        'description': '以下策略针对' + cluster_labels[0] + '客户群体',\n        'strategies': [\n            \"策略1: 提供便捷的数字化服务体验\",\n            \"策略2: 简化产品使用流程，降低使用门槛\",\n            \"策略3: 提供专属的教育内容和使用指南\",\n            \"策略4: 设计阶梯式成长奖励计划\"\n        ]\n    },\n    1: {\n        'name': cluster_labels[1],\n        'description': '以下策略针对' + cluster_labels[1] + '客户群体',\n        'strategies': [\n            \"策略1: 开发专属的增值服务包\",\n            \"策略2: 提供个性化的财务顾问服务\",\n            \"策略3: 建立会员积分兑换制度\",\n            \"策略4: 定期举办客户交流活动\"\n        ]\n    },\n    2: {\n        'name': cluster_labels[2],\n        'description': '以下策略针对' + cluster_labels[2] + '客户群体',\n        'strategies': [\n            \"策略1: 定制专属的高端服务方案\",\n            \"策略2: 提供一站式财富管理服务\",\n            \"策略3: 邀请参与新产品开发意见征集\",\n            \"策略4: 建立专属客户经理对接机制\"\n        ]\n    },\n    3: {\n        'name': cluster_labels[3],\n        'description': '以下策略针对' + cluster_labels[3] + '客户群体',\n        'strategies': [\n            \"策略1: 开展特定主题的金融知识讲座\",\n            \"策略2: 提供针对性的产品组合推荐\",\n            \"策略3: 优化服务流程，提升响应速度\",\n            \"策略4: 建立客户反馈闭环处理机制\"\n        ]\n    }\n}\n\n# 打印每个群体的策略\nfor cluster_id, strategy in cluster_strategies.items():\n    print(f\"\\n{strategy['name']} ({strategy['description']})\")\n    avg_prob = df[df['cluster'] == cluster_id]['churn_probability'].mean()\n    print(f\"平均流失概率: {avg_prob:.4f}\")\n    \n    # 获取该群体的主要风险等级和价值等级\n    main_risk = df[df['cluster'] == cluster_id]['risk_level'].mode()[0]\n    main_value = df[df['cluster'] == cluster_id]['value_segment'].mode()[0]\n    print(f\"主要风险等级: {main_risk}, 主要价值等级: {main_value}\")\n    \n    print(\"挽留策略:\")\n    for s in strategy['strategies']:\n        print(f\"- {s}\")\n\n# 6. 挽留策略的实施计划\nprint(\"\\n=== 挽留策略实施计划 ===\")\n\n# 示例的策略实施计划时间表\ntoday = datetime.now()\nstart_date = today + timedelta(days=7)  # 从下周开始\n\n# 定义实施时间表\nimplementation_plan = [\n    {\n        'phase': '第一阶段: 紧急挽留',\n        'start_date': start_date,\n        'end_date': start_date + timedelta(days=30),\n        'target': '极高风险客户 (所有价值等级)',\n        'actions': [\n            \"1. 建立紧急挽留响应团队\",\n            \"2. 设计针对极高风险客户的挽留话术和优惠方案\",\n            \"3. 实施1对1沟通和专属解决方案\",\n            \"4. 跟踪并分析客户反馈和挽留效果\"\n        ]\n    },\n    {\n        'phase': '第二阶段: 高风险干预',\n        'start_date': start_date + timedelta(days=15),\n        'end_date': start_date + timedelta(days=60),\n        'target': '高风险客户 (中高价值)',\n        'actions': [\n            \"1. 客户经理主动联系和回访\",\n            \"2. 提供定制化的产品优化方案\",\n            \"3. 针对性解决客户痛点问题\",\n            \"4. 建立客户关系强化计划\"\n        ]\n    },\n    {\n        'phase': '第三阶段: 中风险预防',\n        'start_date': start_date + timedelta(days=30),\n        'end_date': start_date + timedelta(days=90),\n        'target': '中风险客户 (所有价值等级)',\n        'actions': [\n            \"1. 推出客户忠诚度提升活动\",\n            \"2. 开展产品使用教育和培训\",\n            \"3. 针对不同客户群体开展个性化营销\",\n            \"4. 完善客户服务体验流程\"\n        ]\n    },\n    {\n        'phase': '第四阶段: 低风险维护',\n        'start_date': start_date + timedelta(days=45),\n        'end_date': start_date + timedelta(days=120),\n        'target': '低风险客户 (中高价值)',\n        'actions': [\n            \"1. 定期客户满意度调查\",\n            \"2. 提供增值服务和专属福利\",\n            \"3. 建立长期客户关系维护机制\",\n            \"4. 发展交叉销售和向上销售策略\"\n        ]\n    }\n]\n\n# 打印实施计划\nfor phase in implementation_plan:\n    print(f\"\\n{phase['phase']}\")\n    print(f\"实施时间: {phase['start_date'].strftime('%Y-%m-%d')} 至 {phase['end_date'].strftime('%Y-%m-%d')}\")\n    print(f\"目标客户: {phase['target']}\")\n    print(\"主要行动:\")\n    for action in phase['actions']:\n        print(f\"- {action}\")\n\n# 7. 挽留效果评估指标设计\nprint(\"\\n=== 挽留效果评估指标 ===\")\n\n# 定义评估指标\nevaluation_metrics = {\n    '流失率指标': [\n        {'name': '整体流失率', 'description': '全行客户的整体流失率变化'},\n        {'name': '目标群体流失率', 'description': '各干预群体的流失率变化'},\n        {'name': '高价值客户流失率', 'description': '高价值客户群体的流失率变化'},\n        {'name': '新增流失率', 'description': '新增客户的流失率'}\n    ],\n    '客户行为指标': [\n        {'name': '交易频率', 'description': '客户交易频率的变化'},\n        {'name': '交易金额', 'description': '客户交易金额的变化'},\n        {'name': '产品持有数', 'description': '客户持有产品数量的变化'},\n        {'name': '渠道活跃度', 'description': '客户在各渠道的活跃程度变化'}\n    ],\n    '客户关系指标': [\n        {'name': '客户满意度', 'description': 'NPS或满意度调查结果'},\n        {'name': '投诉率', 'description': '客户投诉率的变化'},\n        {'name': '问题解决率', 'description': '客户问题的解决效率和质量'},\n        {'name': '客户互动率', 'description': '客户对营销活动的响应率'}\n    ],\n    '财务绩效指标': [\n        {'name': '挽留成本', 'description': '每挽留一个客户的平均成本'},\n        {'name': '挽留ROI', 'description': '挽留投资的回报率'},\n        {'name': '客户终身价值', 'description': '被挽留客户的预期终身价值变化'},\n        {'name': '交叉销售收入', 'description': '通过挽留活动产生的额外产品销售收入'}\n    ]\n}\n\n# 打印评估指标\nfor category, metrics in evaluation_metrics.items():\n    print(f\"\\n{category}:\")\n    for metric in metrics:\n        print(f\"- {metric['name']}: {metric['description']}\")\n\n# 8. 模拟挽留效果\nprint(\"\\n=== 挽留效果模拟预测 ===\")\n\n# 模拟不同挽留策略的效果\nretention_scenarios = {\n    '无干预': {'retention_rate': 0.0, 'cost_per_customer': 0},\n    '基础干预': {'retention_rate': 0.2, 'cost_per_customer': 200},\n    '标准干预': {'retention_rate': 0.4, 'cost_per_customer': 500},\n    '强化干预': {'retention_rate': 0.6, 'cost_per_customer': 1000},\n    '全方位干预': {'retention_rate': 0.7, 'cost_per_customer': 2000}\n}\n\n# 假设的客户价值和挽留成本\navg_customer_value = 5000  # 平均客户年价值\n\n# 高风险及以上客户数量\nhigh_risk_customers = df[df['risk_level'].isin(['高风险', '极高风险'])].shape[0]\n\n# 计算不同情景下的效果\nresults = []\nfor scenario, params in retention_scenarios.items():\n    # 被挽留的客户数\n    retained_customers = high_risk_customers * params['retention_rate']\n    \n    # 挽留总成本\n    total_cost = high_risk_customers * params['cost_per_customer']\n    \n    # 挽留带来的价值\n    retained_value = retained_customers * avg_customer_value\n    \n    # 净收益\n    net_benefit = retained_value - total_cost\n    \n    # ROI\n    roi = (net_benefit / total_cost) if total_cost > 0 else float('inf')\n    \n    results.append({\n        '情景': scenario,\n        '挽留率': params['retention_rate'] * 100,\n        '挽留客户数': retained_customers,\n        '总成本(元)': total_cost,\n        '挽留价值(元)': retained_value,\n        '净收益(元)': net_benefit,\n        'ROI': roi\n    })\n\n# 转换为DataFrame\nresults_df = pd.DataFrame(results)\nprint(\"不同挽留情景的效果比较:\")\nprint(results_df[['情景', '挽留率', '挽留客户数', '总成本(元)', '挽留价值(元)', '净收益(元)', 'ROI']])\n\n# 可视化挽留效果\nplt.figure(figsize=(14, 8))\n\n# 净收益比较\nplt.subplot(1, 2, 1)\nsns.barplot(x='情景', y='净收益(元)', data=results_df, palette='viridis')\nplt.title('不同情景下的净收益比较')\nplt.xticks(rotation=45)\nplt.ylabel('净收益(元)')\n\n# ROI比较\nplt.subplot(1, 2, 2)\nsns.barplot(x='情景', y='ROI', data=results_df[results_df['情景'] != '无干预'], palette='viridis')  # 排除无干预情景，避免无穷大ROI\nplt.title('不同情景下的ROI比较')\nplt.xticks(rotation=45)\nplt.ylabel('ROI(倍)')\n\nplt.tight_layout()\nplt.show()\n\n# 9. 长期客户关系管理建议\nprint(\"\\n=== 长期客户关系管理建议 ===\")\n\nlong_term_recommendations = [\n    {\n        'category': '客户洞察与理解',\n        'recommendations': [\n            \"1. 建立客户360度视图，整合多渠道数据\",\n            \"2. 定期进行客户需求调研和满意度调查\",\n            \"3. 利用高级分析技术预测客户生命周期变化\",\n            \"4. 建立客户行为模式库，支持精准营销\"\n        ]\n    },\n    {\n        'category': '产品与服务优化',\n        'recommendations': [\n            \"1. 基于客户反馈持续优化产品功能与体验\",\n            \"2. 开发针对不同客户群体的差异化产品\",\n            \"3. 简化服务流程，提升客户便捷性\",\n            \"4. 建立产品使用辅导和教育机制\"\n        ]\n    },\n    {\n        'category': '沟通与互动策略',\n        'recommendations': [\n            \"1. 建立多渠道、个性化的客户沟通机制\",\n            \"2. 优化触点管理，确保一致的品牌体验\",\n            \"3. 发展基于价值的内容营销策略\",\n            \"4. 建立客户社区，促进客户间交流与分享\"\n        ]\n    },\n    {\n        'category': '组织能力建设',\n        'recommendations': [\n            \"1. 建立以客户为中心的文化和考核机制\",\n            \"2. 提升员工客户服务技能和意识\",\n            \"3. 优化数据和技术基础设施\",\n            \"4. 建立客户体验管理团队和流程\"\n        ]\n    }\n]\n\n# 打印长期建议\nfor category in long_term_recommendations:\n    print(f\"\\n{category['category']}:\")\n    for recommendation in category['recommendations']:\n        print(f\"- {recommendation}\")\n\n# 10. 挽留策略的总结\nprint(\"\\n=== 客户挽留策略总结 ===\")\nprint(\"基于预警模型的客户挽留策略框架:\")\nprint(\"1. 通过机器学习预警模型，及时识别有流失风险的客户\")\nprint(\"2. 结合客户价值，制定差异化的挽留优先级矩阵\")\nprint(\"3. 针对不同客户群体，设计个性化的挽留措施\")\nprint(\"4. 分阶段实施挽留计划，优先处理高价值高风险客户\")\nprint(\"5. 建立科学的评估指标体系，持续监控挽留效果\")\nprint(\"6. 将短期挽留措施与长期客户关系管理相结合\")\nprint(\"7. 通过数据驱动决策，持续优化挽留策略和投入\")\n\nprint(\"\\n挽留策略成功的关键因素:\")\nprint(\"1. 精准的流失风险预测能力\")\nprint(\"2. 客户价值与风险的综合评估\")\nprint(\"3. 个性化的挽留方案设计\")\nprint(\"4. 多部门协作的实施机制\")\nprint(\"5. 科学的效果评估和持续优化\")\nprint(\"6. 高管层的支持与资源投入\")\n\nprint(\"\\n通过实施科学的客户挽留策略，银行可以有效降低客户流失率，提升客户忠诚度和终身价值，增强市场竞争力。\")"
        }
      ]
    },
    {
      "id": 3,
      "title": "银行信用欺诈数据分析",
      "description": "分析并检测银行信用卡交易中的欺诈行为",
      "category": "bank",
      "difficulty_level": 3,
      "estimated_duration": 130,
      "prerequisites": "数据挖掘和异常检测基础知识",
      "data_source": "银行信用卡交易数据",
      "steps": [
        {
          "id": 1,
          "title": "欺诈类型分析",
          "description": "了解不同类型的银行信用欺诈手段和特征",
          "example_code": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime, timedelta\nimport matplotlib as mpl\nfrom app.services.docker_matplotlib_fix import configure_matplotlib_fonts\n\n# 配置字体以支持中文显示\nconfigure_matplotlib_fonts()\nfrom app.services.bank_analysis import bank_service\n\n# 加载信用卡欺诈数据\ndf = bank_service.load_credit_fraud_data()\nprint(f\"加载数据集，共{df.shape[0]}行，{df.shape[1]}列\")\n\n# 1. 基本数据探索\nprint(\"\\n=== 基本数据探索 ===\")\n\n# 查看数据的基本信息\nprint(\"\\n数据集的基本信息:\")\nprint(df.info())\n\n# 查看数据的统计摘要\nprint(\"\\n数据的统计摘要:\")\nprint(df.describe())\n\n# 检查是否有缺失值\nprint(\"\\n缺失值统计:\")\nmissing_values = df.isnull().sum()\nprint(missing_values[missing_values > 0])\n\n# 查看目标变量分布\nprint(\"\\n欺诈与非欺诈交易比例:\")\nfraud_counts = df['Class'].value_counts()\nfraud_percentage = fraud_counts / len(df) * 100\n\nfor label, count in fraud_counts.items():\n    percentage = fraud_percentage[label]\n    is_fraud = \"欺诈\" if label == 1 else \"正常\"\n    print(f\"{is_fraud}交易: {count}笔 ({percentage:.4f}%)\")\n\n# 可视化欺诈与非欺诈交易比例\nplt.figure(figsize=(10, 6))\nsns.countplot(x='Class', data=df, palette=['#2ecc71', '#e74c3c'])\nplt.title('欺诈与非欺诈交易数量比较')\nplt.xlabel('交易类型')\nplt.ylabel('交易数量')\nplt.xticks([0, 1], ['正常交易', '欺诈交易'])\n\n# 添加文本标签\nfor i, count in enumerate(fraud_counts):\n    percentage = fraud_percentage[i if i in fraud_percentage.index else fraud_counts.index[i]]\n    plt.text(i, count + 100, f'{count}\\n({percentage:.4f}%)', ha='center')\n\nplt.tight_layout()\nplt.show()\n\n# 2. 欺诈交易金额分析\nprint(\"\\n=== 欺诈交易金额分析 ===\")\n\n# 分析正常和欺诈交易的金额分布\nprint(\"\\n正常交易金额统计:\")\nprint(df[df['Class'] == 0]['Amount'].describe())\n\nprint(\"\\n欺诈交易金额统计:\")\nprint(df[df['Class'] == 1]['Amount'].describe())\n\n# 可视化正常和欺诈交易金额分布\nplt.figure(figsize=(14, 6))\n\n# 正常交易金额分布\nplt.subplot(1, 2, 1)\nsns.histplot(df[df['Class'] == 0]['Amount'], bins=50, kde=True, color='#2ecc71')\nplt.title('正常交易金额分布')\nplt.xlabel('交易金额')\nplt.ylabel('交易次数')\n\n# 欺诈交易金额分布\nplt.subplot(1, 2, 2)\nsns.histplot(df[df['Class'] == 1]['Amount'], bins=50, kde=True, color='#e74c3c')\nplt.title('欺诈交易金额分布')\nplt.xlabel('交易金额')\nplt.ylabel('交易次数')\n\nplt.tight_layout()\nplt.show()\n\n# 使用箱线图比较欺诈和正常交易的金额差异\nplt.figure(figsize=(10, 6))\nsns.boxplot(x='Class', y='Amount', data=df, palette=['#2ecc71', '#e74c3c'])\nplt.title('欺诈与正常交易金额箱线图比较')\nplt.xlabel('交易类型')\nplt.ylabel('交易金额')\nplt.xticks([0, 1], ['正常交易', '欺诈交易'])\nplt.tight_layout()\nplt.show()\n\n# 3. 欺诈交易时间模式分析\nprint(\"\\n=== 欺诈交易时间模式分析 ===\")\n\n# 假设数据中有Time字段表示时间（以秒为单位）\nif 'Time' in df.columns:\n    # 将Time转换为小时\n    df['Hour'] = (df['Time'] / 3600) % 24\n    \n    # 分析欺诈交易在不同时间段的分布\n    plt.figure(figsize=(14, 6))\n    \n    # 欺诈交易的小时分布\n    plt.subplot(1, 2, 1)\n    sns.histplot(df[df['Class'] == 1]['Hour'], bins=24, kde=True, color='#e74c3c')\n    plt.title('欺诈交易的小时分布')\n    plt.xlabel('小时')\n    plt.ylabel('交易次数')\n    plt.xticks(range(0, 24, 2))\n    \n    # 正常交易的小时分布\n    plt.subplot(1, 2, 2)\n    sns.histplot(df[df['Class'] == 0]['Hour'], bins=24, kde=True, color='#2ecc71')\n    plt.title('正常交易的小时分布')\n    plt.xlabel('小时')\n    plt.ylabel('交易次数')\n    plt.xticks(range(0, 24, 2))\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # 分析各小时欺诈率\n    hour_fraud_rate = df.groupby('Hour')['Class'].mean() * 100\n    \n    plt.figure(figsize=(12, 6))\n    hour_fraud_rate.plot(kind='line', marker='o', color='#3498db')\n    plt.title('各小时的欺诈率')\n    plt.xlabel('小时')\n    plt.ylabel('欺诈率 (%)')\n    plt.xticks(range(0, 24, 2))\n    plt.grid(True, alpha=0.3)\n    plt.tight_layout()\n    plt.show()\n    \n# 4. 欺诈类型聚类分析\nprint(\"\\n=== 欺诈类型聚类分析 ===\")\n\n# 使用特征变量进行欺诈交易的聚类分析\nfraud_data = df[df['Class'] == 1].copy()\n\n# 选择特征变量（不包括Class和Amount）\nfeature_columns = [col for col in df.columns if col.startswith('V')]\n\n# 如果数据量较大，可以随机抽样\nif len(fraud_data) > 1000:\n    fraud_data = fraud_data.sample(1000, random_state=42)\n\n# 标准化特征\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nfraud_data_scaled = scaler.fit_transform(fraud_data[feature_columns])\n\n# 使用t-SNE进行降维可视化\nfrom sklearn.manifold import TSNE\ntsne = TSNE(n_components=2, random_state=42, perplexity=30)\nfraud_data_tsne = tsne.fit_transform(fraud_data_scaled)\n\n# 使用K-means进行聚类\nfrom sklearn.cluster import KMeans\n\n# 确定最佳聚类数（用肘部法则）\ndistortions = []\nK = range(1, 10)\nfor k in K:\n    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n    kmeans.fit(fraud_data_scaled)\n    distortions.append(kmeans.inertia_)\n\n# 可视化肘部法则\nplt.figure(figsize=(10, 6))\nplt.plot(K, distortions, 'bx-')\nplt.xlabel('聚类数量k')\nplt.ylabel('畸变')\nplt.title('肘部法则确定最佳聚类数')\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n# 假设最佳聚类数为4\nbest_k = 4  # 根据肘部法则确定\nkmeans = KMeans(n_clusters=best_k, random_state=42, n_init=10)\nfraud_clusters = kmeans.fit_predict(fraud_data_scaled)\n\n# 添加聚类标签到数据\nfraud_data['Cluster'] = fraud_clusters\n\n# 可视化聚类结果\nplt.figure(figsize=(12, 10))\n\n# t-SNE可视化\nplt.subplot(2, 2, 1)\nplt.scatter(fraud_data_tsne[:, 0], fraud_data_tsne[:, 1], c=fraud_clusters, \n            cmap='viridis', s=50, alpha=0.8)\nplt.title('欺诈交易的聚类分析 (t-SNE降维)')\nplt.xlabel('t-SNE维度1')\nplt.ylabel('t-SNE维度2')\nplt.colorbar(label='聚类')\n\n# 各聚类的交易金额分布\nplt.subplot(2, 2, 2)\nsns.boxplot(x='Cluster', y='Amount', data=fraud_data, palette='viridis')\nplt.title('各欺诈类型的交易金额分布')\nplt.xlabel('欺诈类型聚类')\nplt.ylabel('交易金额')\n\n# 如果有Time字段，分析各聚类的时间分布\nif 'Hour' in fraud_data.columns:\n    plt.subplot(2, 2, 3)\n    sns.boxplot(x='Cluster', y='Hour', data=fraud_data, palette='viridis')\n    plt.title('各欺诈类型的时间分布')\n    plt.xlabel('欺诈类型聚类')\n    plt.ylabel('小时')\n\n# 各聚类的大小\nplt.subplot(2, 2, 4)\ncluster_counts = fraud_data['Cluster'].value_counts().sort_index()\nplt.bar(cluster_counts.index, cluster_counts.values, color='viridis')\nplt.title('各欺诈类型的数量分布')\nplt.xlabel('欺诈类型聚类')\nplt.ylabel('交易数量')\nplt.xticks(range(best_k))\n\n# 在柱状图上添加数值标签\nfor i, count in enumerate(cluster_counts):\n    plt.text(i, count + 5, str(count), ha='center')\n\nplt.tight_layout()\nplt.show()\n\n# 5. 各欺诈类型的特征分析\nprint(\"\\n=== 各欺诈类型的特征分析 ===\")\n\n# 分析各聚类的特征均值\ncluster_features = []\nfor cluster in range(best_k):\n    cluster_mean = fraud_data[fraud_data['Cluster'] == cluster][feature_columns].mean()\n    cluster_features.append(cluster_mean)\n\ncluster_features_df = pd.DataFrame(cluster_features)\n\n# 计算全局特征均值（非欺诈交易）\nnon_fraud_mean = df[df['Class'] == 0][feature_columns].mean()\n\n# 分析每个聚类的特征偏离情况\nfor cluster in range(best_k):\n    # 计算与非欺诈交易的特征差异\n    feature_diff = cluster_features_df.iloc[cluster] - non_fraud_mean\n    \n    # 找出差异最大的10个特征\n    top_features = feature_diff.abs().nlargest(10).index.tolist()\n    \n    print(f\"\\n欺诈类型{cluster}的特征分析:\")\n    print(f\"该类型共有{cluster_counts[cluster]}笔交易\")\n    print(\"与正常交易差异最大的特征:\")\n    \n    for feature in top_features:\n        diff_value = feature_diff[feature]\n        diff_dir = \"高于\" if diff_value > 0 else \"低于\"\n        print(f\"{feature}: {diff_dir}正常交易 {abs(diff_value):.4f}单位\")\n\n# 可视化特征重要性\nplt.figure(figsize=(14, 10))\n\n# 为每个聚类绘制重要特征\nfor cluster in range(min(4, best_k)):  # 最多显示4个聚类\n    plt.subplot(2, 2, cluster + 1)\n    \n    # 计算与非欺诈交易的特征差异\n    feature_diff = cluster_features_df.iloc[cluster] - non_fraud_mean\n    \n    # 找出差异最大的10个特征\n    top_features = feature_diff.abs().nlargest(10)\n    \n    # 创建条形图\n    colors = ['#3498db' if val > 0 else '#e74c3c' for val in top_features]\n    plt.barh(top_features.index, top_features.values, color=colors)\n    plt.title(f'欺诈类型{cluster}的特征重要性')\n    plt.xlabel('与正常交易的差异')\n    plt.grid(True, axis='x', alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# 6. 欺诈类型总结\nprint(\"\\n=== 欺诈类型总结 ===\")\n\n# 根据分析结果，总结各种欺诈类型的特点\nfraud_types = [\n    {\n        \"type\": \"金额小、高频率欺诈\",\n        \"characteristics\": [\n            \"交易金额通常较小\",\n            \"在短时间内可能有多次交易\",\n            \"通常发生在特定的时间段\",\n            \"主要针对商业或零售账户\"\n        ],\n        \"examples\": [\n            \"测试被盗信用卡有效性的小额交易\",\n            \"自动化欺诈工具批量处理的交易\",\n            \"通过多次小额交易规避风控系统\"\n        ]\n    },\n    {\n        \"type\": \"金额大、低频率欺诈\",\n        \"characteristics\": [\n            \"交易金额通常较大\",\n            \"交易频率较低\",\n            \"可能针对高价值商品或服务\",\n            \"通常在被发现之前只有几次交易\"\n        ],\n        \"examples\": [\n            \"购买高价值电子产品或奢侈品\",\n            \"大额现金提取或转账\",\n            \"购买可快速转售的商品\"\n        ]\n    },\n    {\n        \"type\": \"账户接管型欺诈\",\n        \"characteristics\": [\n            \"交易模式与用户历史模式显著不同\",\n            \"可能先有小额测试交易，然后是大额交易\",\n            \"交易地点或商户类型异常\",\n            \"短时间内多种类型交易\"\n        ],\n        \"examples\": [\n            \"通过网络钓鱼获取账户后的欺诈交易\",\n            \"身份盗窃后的非授权使用\",\n            \"账户信息泄露后的欺诈行为\"\n        ]\n    },\n    {\n        \"type\": \"商户欺诈\",\n        \"characteristics\": [\n            \"通常涉及特定商户或商户类型\",\n            \"交易金额可能各不相同\",\n            \"可能有不规则的交易模式\",\n            \"可能涉及虚假退款或重复收费\"\n        ],\n        \"examples\": [\n            \"虚假商户进行的刷卡套现\",\n            \"商户与犯罪分子共谋的欺诈行为\",\n            \"POS终端被篡改后的欺诈交易\"\n        ]\n    }\n]\n\n# 打印欺诈类型总结\nfor i, fraud_type in enumerate(fraud_types):\n    print(f\"\\n欺诈类型{i+1}: {fraud_type['type']}\")\n    print(\"特征:\")\n    for char in fraud_type['characteristics']:\n        print(f\"- {char}\")\n    \n    print(\"\\n典型案例:\")\n    for example in fraud_type['examples']:\n        print(f\"- {example}\")\n    \n    print(\"\\n防范建议:\")\n    if i == 0:  # 金额小、高频率欺诈\n        print(\"- 设置短时间内小额交易次数限制\")\n        print(\"- 对多次小额交易进行风险聚合评估\")\n        print(\"- 建立小额高频交易的行为模型\")\n    elif i == 1:  # 金额大、低频率欺诈\n        print(\"- 对大额交易实施强验证机制\")\n        print(\"- 基于消费者行为分析大额交易风险\")\n        print(\"- 设置基于风险的交易限额\")\n    elif i == 2:  # 账户接管型欺诈\n        print(\"- 建立用户行为基线，检测异常活动\")\n        print(\"- 对异地或异常时间的交易添加验证步骤\")\n        print(\"- 使用多因素认证保护账户安全\")\n    else:  # 商户欺诈\n        print(\"- 对商户进行严格的尽职调查和监控\")\n        print(\"- 分析商户级别的交易模式异常\")\n        print(\"- 建立商户风险评分机制\")\n\n# 7. 反欺诈策略建议\nprint(\"\\n=== 反欺诈策略建议 ===\")\n\n# 分层防御策略\ndefense_layers = {\n    \"数据层\": [\n        \"1. 建立全面的数据收集机制，包括交易、设备、行为等信息\",\n        \"2. 实施实时数据质量检查，确保数据完整性\",\n        \"3. 整合内部和外部数据源，提高欺诈检测能力\"\n    ],\n    \"分析层\": [\n        \"1. 部署规则引擎，实施基本的欺诈检测规则\",\n        \"2. 应用高级分析技术，如机器学习模型检测复杂欺诈模式\",\n        \"3. 建立异常检测系统，识别偏离正常行为的交易\"\n    ],\n    \"操作层\": [\n        \"1. 实施实时交易监控和预警系统\",\n        \"2. 建立分级响应机制，根据风险级别采取相应措施\",\n        \"3. 配置自适应认证策略，对高风险交易增加验证步骤\"\n    ],\n    \"优化层\": [\n        \"1. 定期评估欺诈模式变化和策略有效性\",\n        \"2. 持续优化模型和规则，适应新出现的欺诈手段\",\n        \"3. 建立欺诈案例库，促进知识共享和学习\"\n    ]\n}\n\n# 打印反欺诈策略建议\nfor layer, strategies in defense_layers.items():\n    print(f\"\\n{layer}:\")\n    for strategy in strategies:\n        print(f\"- {strategy}\")\n\n# 总结\nprint(\"\\n=== 总结 ===\")\nprint(\"信用卡欺诈分析的关键发现:\")\nprint(\"1. 欺诈交易在整体交易中占比很小，数据极不平衡\")\nprint(\"2. 欺诈交易的金额分布与正常交易存在显著差异\")\nprint(\"3. 欺诈交易在某些时间段可能更为集中\")\nprint(\"4. 通过聚类分析，可以识别多种不同类型的欺诈模式\")\nprint(\"5. 不同类型的欺诈具有不同的特征表现和处理策略\")\n\nprint(\"\\n下一步建议:\")\nprint(\"1. 基于识别的欺诈类型，设计有针对性的特征工程\")\nprint(\"2. 开发专门针对各类欺诈行为的检测模型\")\nprint(\"3. 实施多层次防御策略，提高欺诈检测效果\")\nprint(\"4. 建立实时监控系统，及时发现和响应欺诈活动\")"
        },
        {
          "id": 2,
          "title": "交易数据处理",
          "description": "清洗和准备交易数据，处理时间特征",
          "example_code": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime, timedelta\nimport matplotlib as mpl\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\nfrom app.services.docker_matplotlib_fix import configure_matplotlib_fonts\n\n# 配置字体以支持中文显示\nconfigure_matplotlib_fonts()\nfrom app.services.bank_analysis import bank_service\n\n# 加载信用卡欺诈数据\ndf = bank_service.load_credit_fraud_data()\nprint(f\"加载数据集，共{df.shape[0]}行，{df.shape[1]}列\")\n\n# 1. 数据概览\nprint(\"\\n=== 数据概览 ===\")\nprint(\"数据集前5行:\")\nprint(df.head())\n\nprint(\"\\n数据集的基本信息:\")\nprint(df.info())\n\n# 检查是否有缺失值\nprint(\"\\n缺失值检查:\")\nmissing_values = df.isnull().sum()\nif missing_values.sum() > 0:\n    print(missing_values[missing_values > 0])\n    print(\"\\n需要处理缺失值...\")\nelse:\n    print(\"数据集中没有缺失值，无需填充处理\")\n\n# 检查重复值\nduplicates = df.duplicated().sum()\nprint(f\"\\n重复行数: {duplicates}\")\nif duplicates > 0:\n    print(\"需要处理重复数据...\")\n    \n# 查看数据集基本统计\nprint(\"\\n数据集的基本统计:\")\nprint(df.describe())\n\n# 2. 处理交易金额\nprint(\"\\n=== 处理交易金额 ===\")\n\n# 检查金额是否有异常值（如负值）\nnegative_amounts = (df['Amount'] < 0).sum()\nprint(f\"负值金额数量: {negative_amounts}\")\n\n# 检查金额为0的交易\nzero_amounts = (df['Amount'] == 0).sum()\nprint(f\"金额为0的交易数量: {zero_amounts}\")\n\n# 可视化金额分布\nplt.figure(figsize=(12, 5))\n\n# 原始金额分布\nplt.subplot(1, 2, 1)\nsns.histplot(df['Amount'], bins=50, kde=True)\nplt.title('原始交易金额分布')\nplt.xlabel('交易金额')\nplt.ylabel('频率')\n\n# 对数变换后的金额分布\ndf['LogAmount'] = np.log1p(df['Amount'])  # log1p处理金额为0的情况\nplt.subplot(1, 2, 2)\nsns.histplot(df['LogAmount'], bins=50, kde=True)\nplt.title('对数变换后的交易金额分布')\nplt.xlabel('log(交易金额 + 1)')\nplt.ylabel('频率')\n\nplt.tight_layout()\nplt.show()\n\n# 3. 处理时间特征\nprint(\"\\n=== 处理时间特征 ===\")\n\n# Time特征处理（通常表示自某个起始时间点的秒数）\nif 'Time' in df.columns:\n    # 基本统计\n    print(\"时间特征的基本统计:\")\n    print(df['Time'].describe())\n    \n    # 确定数据时间跨度（假设Time是按秒计）\n    time_span_days = (df['Time'].max() - df['Time'].min()) / (60 * 60 * 24)\n    print(f\"数据时间跨度约为: {time_span_days:.2f} 天\")\n    \n    # 转换为更有意义的时间特征\n    df['Hour'] = (df['Time'] / 3600) % 24  # 小时（0-23）\n    df['Day'] = (df['Time'] / (3600 * 24)) % 7  # 星期几（0-6，假设0代表星期一）\n    \n    # 创建其他有用的时间特征\n    # 将小时分为早上、下午、晚上和深夜\n    df['TimeOfDay'] = pd.cut(\n        df['Hour'], \n        bins=[0, 6, 12, 18, 24], \n        labels=['深夜', '早上', '下午', '晚上']\n    )\n    \n    # 将星期几划分为工作日和周末\n    df['IsWeekend'] = df['Day'].apply(lambda x: 1 if x >= 5 else 0)  # 假设5,6代表周末\n    \n    # 可视化时间特征\n    plt.figure(figsize=(15, 10))\n    \n    # 小时分布\n    plt.subplot(2, 2, 1)\n    sns.countplot(x='Hour', data=df, palette='viridis')\n    plt.title('交易小时分布')\n    plt.xlabel('小时')\n    plt.ylabel('交易数量')\n    plt.xticks(range(0, 24, 2))\n    \n    # 星期几分布\n    plt.subplot(2, 2, 2)\n    sns.countplot(x='Day', data=df, palette='viridis')\n    plt.title('交易星期分布')\n    plt.xlabel('星期')\n    plt.ylabel('交易数量')\n    plt.xticks(range(7), ['周一', '周二', '周三', '周四', '周五', '周六', '周日'])\n    \n    # 时间段分布\n    plt.subplot(2, 2, 3)\n    sns.countplot(x='TimeOfDay', data=df, palette='viridis')\n    plt.title('交易时间段分布')\n    plt.xlabel('时间段')\n    plt.ylabel('交易数量')\n    \n    # 工作日与周末对比\n    plt.subplot(2, 2, 4)\n    sns.countplot(x='IsWeekend', data=df, palette='viridis')\n    plt.title('工作日与周末交易对比')\n    plt.xlabel('是否周末')\n    plt.ylabel('交易数量')\n    plt.xticks([0, 1], ['工作日', '周末'])\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # 分析不同时间段的欺诈率\n    time_features = ['Hour', 'Day', 'TimeOfDay', 'IsWeekend']\n    plt.figure(figsize=(15, 10))\n    \n    for i, feature in enumerate(time_features):\n        plt.subplot(2, 2, i+1)\n        fraud_rate = df.groupby(feature)['Class'].mean() * 100\n        fraud_rate.plot(kind='bar', color='#3498db')\n        plt.title(f'{feature}的欺诈率')\n        plt.ylabel('欺诈率 (%)')\n        plt.grid(True, alpha=0.3, axis='y')\n        \n        # 为星期几和时间段添加自定义标签\n        if feature == 'Day':\n            plt.xticks(range(7), ['周一', '周二', '周三', '周四', '周五', '周六', '周日'])\n    \n    plt.tight_layout()\n    plt.show()\n\n# 4. 处理特征（V1-V28）\nprint(\"\\n=== 处理特征变量 ===\")\n\n# 假设V1-V28是PCA或其他变换后的特征\nfeature_columns = [col for col in df.columns if col.startswith('V')]\nprint(f\"特征变量数量: {len(feature_columns)}\")\n\n# 检查特征的分布\nplt.figure(figsize=(15, 10))\n\n# 随机选择6个特征进行可视化\nselected_features = np.random.choice(feature_columns, 6, replace=False)\n\nfor i, feature in enumerate(selected_features):\n    plt.subplot(2, 3, i+1)\n    sns.histplot(df[feature], bins=50, kde=True)\n    plt.title(f'{feature}分布')\n    plt.xlabel(feature)\n\nplt.tight_layout()\nplt.show()\n\n# 检查特征相关性\ncorr_matrix = df[feature_columns].corr()\n\n# 绘制相关性热图\nplt.figure(figsize=(15, 12))\nsns.heatmap(corr_matrix, cmap='coolwarm', center=0, annot=False)\nplt.title('特征变量相关性热图')\nplt.tight_layout()\nplt.show()\n\n# 检查是否有高度相关的特征对\nhighly_correlated = []\ncorr_threshold = 0.8\n\nfor i in range(len(feature_columns)):\n    for j in range(i+1, len(feature_columns)):\n        corr = corr_matrix.iloc[i, j]\n        if abs(corr) > corr_threshold:\n            highly_correlated.append((feature_columns[i], feature_columns[j], corr))\n\nif highly_correlated:\n    print(\"高度相关的特征对 (|相关系数| > 0.8):\")\n    for feat1, feat2, corr in highly_correlated:\n        print(f\"{feat1} 和 {feat2}: {corr:.4f}\")\nelse:\n    print(\"没有发现高度相关的特征对\")\n\n# 5. 特征标准化\nprint(\"\\n=== 特征标准化 ===\")\n\n# 使用StandardScaler和RobustScaler进行对比\nfeatures_to_scale = feature_columns + ['Amount']\n\n# 创建一个包含原始数据的副本\ndf_scaled = df.copy()\n\n# 应用StandardScaler\nstd_scaler = StandardScaler()\ndf_scaled[features_to_scale] = std_scaler.fit_transform(df[features_to_scale])\n\n# 应用RobustScaler（对异常值具有更强的鲁棒性）\nrobust_scaler = RobustScaler()\ndf_robust = df.copy()\ndf_robust[features_to_scale] = robust_scaler.fit_transform(df[features_to_scale])\n\n# 比较不同缩放方法的结果\nplt.figure(figsize=(15, 10))\n\n# 随机选择3个特征进行对比可视化\nselected_features_for_scaling = np.random.choice(feature_columns, 3, replace=False)\n\nfor i, feature in enumerate(selected_features_for_scaling):\n    # 原始数据分布\n    plt.subplot(3, 3, i*3+1)\n    sns.histplot(df[feature], bins=50, kde=True)\n    plt.title(f'原始 {feature}')\n    plt.xlabel(feature)\n    \n    # StandardScaler后的分布\n    plt.subplot(3, 3, i*3+2)\n    sns.histplot(df_scaled[feature], bins=50, kde=True)\n    plt.title(f'StandardScaler {feature}')\n    plt.xlabel(feature)\n    \n    # RobustScaler后的分布\n    plt.subplot(3, 3, i*3+3)\n    sns.histplot(df_robust[feature], bins=50, kde=True)\n    plt.title(f'RobustScaler {feature}')\n    plt.xlabel(feature)\n\nplt.tight_layout()\nplt.show()\n\n# 6. 检测和处理异常值\nprint(\"\\n=== 检测和处理异常值 ===\")\n\n# 使用箱线图检测异常值\nplt.figure(figsize=(15, 10))\n\n# 随机选择6个特征进行箱线图可视化\nselected_features_boxplot = np.random.choice(feature_columns, 6, replace=False)\n\nfor i, feature in enumerate(selected_features_boxplot):\n    plt.subplot(2, 3, i+1)\n    sns.boxplot(y=df[feature])\n    plt.title(f'{feature}的箱线图')\n    plt.ylabel(feature)\n\nplt.tight_layout()\nplt.show()\n\n# 计算并显示异常值比例\nprint(\"\\n使用IQR方法识别异常值的比例:\")\n\nfor feature in selected_features_boxplot:\n    q1 = df[feature].quantile(0.25)\n    q3 = df[feature].quantile(0.75)\n    iqr = q3 - q1\n    lower_bound = q1 - 1.5 * iqr\n    upper_bound = q3 + 1.5 * iqr\n    outliers = df[(df[feature] < lower_bound) | (df[feature] > upper_bound)]\n    outlier_percentage = len(outliers) / len(df) * 100\n    print(f\"{feature}: {outlier_percentage:.2f}% 的数据点被识别为异常值\")\n\n# 7. 处理类别不平衡问题\nprint(\"\\n=== 处理类别不平衡问题 ===\")\n\n# 显示类别分布\nclass_counts = df['Class'].value_counts()\nfraud_percentage = class_counts[1] / len(df) * 100\n\nprint(f\"非欺诈交易数量: {class_counts[0]}\")\nprint(f\"欺诈交易数量: {class_counts[1]}\")\nprint(f\"欺诈率: {fraud_percentage:.4f}%\")\n\n# 计算欺诈类别的权重（用于处理不平衡问题）\nfraud_weight = class_counts[0] / class_counts[1]\nprint(f\"欺诈类别的权重: {fraud_weight:.4f}\")\n\n# 8. 数据处理前后对比\nprint(\"\\n=== 数据处理前后对比 ===\")\n\n# 创建表格显示处理前后的数据形状和特征\noriginal_shape = df.shape\nprocessed_shape = df_scaled.shape\nnew_features = [col for col in df_scaled.columns if col not in df.columns]\nmodified_features = ['Amount']  # LogAmount, 标准化的特征等\n\nprint(f\"原始数据: {original_shape[0]}行, {original_shape[1]}列\")\nprint(f\"处理后的数据: {processed_shape[0]}行, {processed_shape[1]}列\")\nprint(f\"新增特征: {', '.join(new_features)}\")\nprint(f\"修改的特征: LogAmount, 标准化的特征等\")\n\n# 9. 最终数据集准备\nprint(\"\\n=== 最终数据集准备 ===\")\n\n# 选择处理后的特征\nprocessed_features = feature_columns + ['LogAmount', 'Hour', 'Day', 'IsWeekend']\n\n# 准备最终数据集\nX = df_robust[processed_features]  # 使用RobustScaler处理的数据集\ny = df['Class']\n\nprint(f\"特征数据(X)形状: {X.shape}\")\nprint(f\"目标变量(y)形状: {y.shape}\")\n\n# 将数据集划分为训练集和测试集\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n\nprint(f\"训练集: X_train: {X_train.shape}, y_train: {y_train.shape}\")\nprint(f\"测试集: X_test: {X_test.shape}, y_test: {y_test.shape}\")\n\n# 检查划分后的类别分布\nprint(\"\\n训练集类别分布:\")\nprint(y_train.value_counts())\nprint(\"\\n测试集类别分布:\")\nprint(y_test.value_counts())\n\n# 10. 数据处理总结\nprint(\"\\n=== 数据处理总结 ===\")\nprint(\"数据处理和准备完成，包含以下步骤:\")\nprint(\"1. 数据加载和初步检查\")\nprint(\"2. 处理交易金额（对数变换）\")\nprint(\"3. 处理时间特征（小时、星期、时间段等）\")\nprint(\"4. 分析特征变量的分布和相关性\")\nprint(\"5. 使用StandardScaler和RobustScaler进行特征标准化\")\nprint(\"6. 检测和分析异常值\")\nprint(\"7. 了解并处理类别不平衡问题\")\nprint(\"8. 准备最终处理后的数据集\")\nprint(\"9. 划分训练集和测试集\")\n\nprint(\"\\n下一步工作:\")\nprint(\"1. 进行更深入的特征工程，如提取异常检测特征\")\nprint(\"2. 应用不平衡数据处理技术如SMOTE、类别权重等\")\nprint(\"3. 构建和训练欺诈检测模型\")"
        },
        {
          "id": 3,
          "title": "异常特征提取",
          "description": "提取能够识别欺诈交易的关键特征",
          "example_code": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif\nfrom sklearn.ensemble import RandomForestClassifier, IsolationForest\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom app.services.docker_matplotlib_fix import configure_matplotlib_fonts\n\n# 配置字体以支持中文显示\nconfigure_matplotlib_fonts()\nfrom app.services.bank_analysis import bank_service\n\n# 加载已经预处理的欺诈数据\ndf = bank_service.load_credit_fraud_data()\nprint(f\"加载数据集，共{df.shape[0]}行，{df.shape[1]}列\")\n\n# 假设已经进行了基本的数据清洗和准备（如前面步骤所示）\n# 这里我们将重点放在特征提取和选择上\n\n# 分离特征和目标变量\nfeature_columns = [col for col in df.columns if col.startswith('V')] + ['Amount', 'Time']\nX = df[feature_columns]\ny = df['Class']\n\n# 1. 基于统计学的特征重要性\nprint(\"\\n=== 基于统计学的特征重要性 ===\")\n\n# 使用ANOVA F-value 评估特征重要性\nk_best = 15  # 选择前15个最重要的特征\nselector = SelectKBest(score_func=f_classif, k=k_best)\nX_selected = selector.fit_transform(X, y)\n\n# 获取每个特征的重要性得分\nfeature_scores = selector.scores_\nfeature_scores_dict = dict(zip(feature_columns, feature_scores))\n\n# 按重要性排序\nsorted_features = sorted(feature_scores_dict.items(), key=lambda x: x[1], reverse=True)\nprint(\"\\n基于F值的前15个重要特征:\")\nfor feature, score in sorted_features[:15]:\n    print(f\"{feature}: {score:.4f}\")\n\n# 可视化特征重要性\nplt.figure(figsize=(12, 8))\ntop_features = [f[0] for f in sorted_features[:15]]\ntop_scores = [f[1] for f in sorted_features[:15]]\n\nplt.barh(range(len(top_features)), top_scores, align='center')\nplt.yticks(range(len(top_features)), top_features)\nplt.xlabel('F值重要性分数')\nplt.title('特征重要性排名 (ANOVA F-test)')\nplt.tight_layout()\nplt.show()\n\n# 2. 基于机器学习的特征重要性\nprint(\"\\n=== 基于机器学习的特征重要性 ===\")\n\n# 数据标准化\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# 使用随机森林评估特征重要性\nrf = RandomForestClassifier(n_estimators=100, random_state=42)\nrf.fit(X_scaled, y)\n\n# 提取特征重要性\nrf_importances = rf.feature_importances_\nrf_feature_importance = dict(zip(feature_columns, rf_importances))\nsorted_rf_features = sorted(rf_feature_importance.items(), key=lambda x: x[1], reverse=True)\n\nprint(\"\\n基于随机森林的前15个重要特征:\")\nfor feature, importance in sorted_rf_features[:15]:\n    print(f\"{feature}: {importance:.6f}\")\n\n# 可视化特征重要性\nplt.figure(figsize=(12, 8))\ntop_rf_features = [f[0] for f in sorted_rf_features[:15]]\ntop_rf_importances = [f[1] for f in sorted_rf_features[:15]]\n\nplt.barh(range(len(top_rf_features)), top_rf_importances, align='center')\nplt.yticks(range(len(top_rf_features)), top_rf_features)\nplt.xlabel('随机森林特征重要性')\nplt.title('特征重要性排名 (随机森林)')\nplt.tight_layout()\nplt.show()\n\n# 3. 异常检测特征提取\nprint(\"\\n=== 异常检测特征提取 ===\")\n\n# 仅使用非欺诈交易数据训练异常检测模型\nnormal_data = df[df['Class'] == 0]\nX_normal = normal_data[feature_columns]\nX_normal_scaled = scaler.transform(X_normal)\n\n# 使用Isolation Forest检测异常\nisolation_forest = IsolationForest(n_estimators=100, contamination=0.01, random_state=42)\nisolation_forest.fit(X_normal_scaled)\n\n# 对所有数据进行预测\ndf['IF_score'] = isolation_forest.decision_function(X_scaled) * -1  # 越高越异常\ndf['IF_anomaly'] = isolation_forest.predict(X_scaled) == -1  # True表示异常\n\n# 使用Local Outlier Factor (LOF) 检测异常\nlof = LocalOutlierFactor(n_neighbors=20, contamination=0.01)\ndf['LOF_score'] = lof.fit_predict(X_scaled) == -1  # -1表示异常\ndf['LOF_score'] = df['LOF_score'].astype(int)  # 转为0/1\n\n# 使用DBSCAN进行聚类，检测异常点\ndbscan = DBSCAN(eps=0.5, min_samples=5)\ndf['DBSCAN_cluster'] = dbscan.fit_predict(X_scaled)\ndf['DBSCAN_anomaly'] = df['DBSCAN_cluster'] == -1  # -1表示异常\ndf['DBSCAN_anomaly'] = df['DBSCAN_anomaly'].astype(int)  # 转为0/1\n\n# 分析异常检测结果与欺诈的相关性\nprint(\"\\n异常检测与欺诈的关联性:\")\nfor method in ['IF_anomaly', 'LOF_score', 'DBSCAN_anomaly']:\n    corr = df[method].corr(df['Class'])\n    accuracy = (df[method] == df['Class']).mean()\n    print(f\"{method} 与欺诈的相关系数: {corr:.4f}, 一致率: {accuracy:.4f}\")\n\n# 可视化异常得分与欺诈的关系\nplt.figure(figsize=(12, 6))\n\nplt.subplot(1, 2, 1)\nsns.boxplot(x='Class', y='IF_score', data=df)\nplt.title('Isolation Forest 异常得分 vs 欺诈类别')\nplt.xlabel('欺诈类别 (0=正常, 1=欺诈)')\nplt.ylabel('异常得分 (越高越异常)')\n\nplt.subplot(1, 2, 2)\nsns.countplot(x='IF_anomaly', hue='Class', data=df, palette=['#2ecc71', '#e74c3c'])\nplt.title('Isolation Forest 预测异常 vs 欺诈类别')\nplt.xlabel('是否异常 (0=正常, 1=异常)')\nplt.ylabel('计数')\nplt.legend(['正常交易', '欺诈交易'])\n\nplt.tight_layout()\nplt.show()\n\n# 4. 特征工程 - 创建新特征\nprint(\"\\n=== 特征工程 - 创建新特征 ===\")\n\n# 创建一些可能与欺诈行为相关的新特征\n\n# 1. 时间相关特征（假设Time是以秒为单位的时间）\nif 'Time' in df.columns:\n    # 将Time转换为更有意义的特征\n    df['Hour'] = (df['Time'] / 3600) % 24\n    \n    # 将一天分为不同时段\n    df['TimeSegment'] = pd.cut(df['Hour'], \n                          bins=[0, 6, 12, 18, 24], \n                          labels=['深夜', '上午', '下午', '晚上'],\n                          include_lowest=True)\n    \n    # 计算每个时间段的欺诈率\n    time_segment_fraud_rate = df.groupby('TimeSegment')['Class'].mean()\n    print(\"\\n不同时间段的欺诈率:\")\n    print(time_segment_fraud_rate)\n    \n    # 基于时间段的欺诈风险评分\n    time_risk_mapping = time_segment_fraud_rate.to_dict()\n    df['TimeRiskScore'] = df['TimeSegment'].map(time_risk_mapping)\n\n# 2. 金额相关特征\n# 计算金额的各种变换以捕捉不同范围的模式\ndf['LogAmount'] = np.log1p(df['Amount'])  # 对数变换\ndf['AmountSqrt'] = np.sqrt(df['Amount'])  # 平方根变换\ndf['AmountBin'] = pd.qcut(df['Amount'], q=10, labels=False, duplicates='drop')  # 十分位数\n\n# 计算每个金额分箱的欺诈率\namount_bin_fraud_rate = df.groupby('AmountBin')['Class'].mean()\nprint(\"\\n不同金额区间的欺诈率:\")\nprint(amount_bin_fraud_rate)\n\n# 映射欺诈风险评分\namount_risk_mapping = amount_bin_fraud_rate.to_dict()\ndf['AmountRiskScore'] = df['AmountBin'].map(amount_risk_mapping)\n\n# 3. 异常检测特征组合\n# 组合多个异常检测算法的结果\ndf['AnomalyScore'] = df['IF_score'] + df['LOF_score'] + df['DBSCAN_anomaly']\n\n# 4. 聚合特征 - PCA分量\n# 使用PCA创建新的特征表示\npca = PCA(n_components=5)\npca_features = pca.fit_transform(X_scaled)\n\n# 将PCA特征添加到数据集\nfor i in range(pca_features.shape[1]):\n    df[f'PCA_{i+1}'] = pca_features[:, i]\n\n# 分析PCA特征与欺诈的关系\npca_corr = df[[f'PCA_{i+1}' for i in range(5)] + ['Class']].corr()['Class'].drop('Class')\nprint(\"\\nPCA特征与欺诈的相关性:\")\nprint(pca_corr)\n\n# 5. 特征选择 - 选择最终特征集\nprint(\"\\n=== 最终特征选择 ===\")\n\n# 基于前面的分析选择特征\nimportance_threshold = 0.01  # 随机森林重要性阈值\nselected_rf_features = [f for f, imp in sorted_rf_features if imp > importance_threshold]\n\n# 新创建的特征\nengineered_features = ['IF_score', 'LOF_score', 'DBSCAN_anomaly', 'AnomalyScore']\nif 'TimeRiskScore' in df.columns:\n    engineered_features.append('TimeRiskScore')\nengineered_features.extend(['AmountRiskScore', 'LogAmount'])\nengineered_features.extend([f'PCA_{i+1}' for i in range(3)])  # 前3个PCA分量\n\n# 合并选择的特征\nfinal_features = selected_rf_features + engineered_features\n\n# 去除重复\nfinal_features = list(dict.fromkeys(final_features))\n\nprint(f\"\\n最终选择的特征集 ({len(final_features)}个特征):\")\nfor i, feature in enumerate(final_features):\n    print(f\"{i+1}. {feature}\")\n\n# 准备最终数据集\nX_final = df[final_features]\ny_final = df['Class']\n\nprint(f\"\\n最终数据集形状: {X_final.shape}\")\n\n# 6. 特征重要性可视化\nprint(\"\\n=== 最终特征重要性可视化 ===\")\n\n# 使用最终特征集训练随机森林\nrf_final = RandomForestClassifier(n_estimators=100, random_state=42)\nrf_final.fit(X_final, y_final)\n\n# 提取特征重要性\nfinal_importances = rf_final.feature_importances_\nfinal_importance_dict = dict(zip(final_features, final_importances))\nsorted_final_features = sorted(final_importance_dict.items(), key=lambda x: x[1], reverse=True)\n\n# 可视化最终特征重要性\nplt.figure(figsize=(14, 10))\ntop_n = min(20, len(final_features))  # 最多显示20个特征\ntop_final_features = [f[0] for f in sorted_final_features[:top_n]]\ntop_final_importances = [f[1] for f in sorted_final_features[:top_n]]\n\nplt.barh(range(len(top_final_features)), top_final_importances, align='center')\nplt.yticks(range(len(top_final_features)), top_final_features)\nplt.xlabel('最终特征重要性')\nplt.title('最终选择特征的重要性排名')\nplt.tight_layout()\nplt.show()\n\n# 7. 特征相关性分析\nprint(\"\\n=== 特征相关性分析 ===\")\n\n# 分析最终特征之间的相关性\ncorr_matrix = X_final.corr()\n\n# 绘制相关性热图\nplt.figure(figsize=(16, 14))\nsns.heatmap(corr_matrix, annot=False, cmap='coolwarm', center=0, \n            vmin=-1, vmax=1, square=True, linewidths=.5, cbar_kws={\"shrink\": .8})\nplt.title('最终特征之间的相关性热图')\nplt.tight_layout()\nplt.show()\n\n# 8. 总结\nprint(\"\\n=== 异常特征提取总结 ===\")\nprint(\"完成了以下特征工程和选择步骤:\")\nprint(\"1. 使用ANOVA F-test评估原始特征的统计学重要性\")\nprint(\"2. 使用随机森林评估特征重要性\")\nprint(\"3. 应用多种异常检测算法创建异常分数特征\")\nprint(\"4. 创建时间和金额相关的风险评分特征\")\nprint(\"5. 使用PCA创建降维特征\")\nprint(\"6. 整合多种技术选择最终特征集\")\n\nprint(\"\\n这些特征将用于后续的欺诈检测模型构建。特征工程是欺诈检测中至关重要的一步，\")\nprint(\"因为它能捕捉欺诈行为的微妙模式，提高模型性能。\")"
        },
        {
          "id": 4,
          "title": "欺诈检测建模",
          "description": "构建欺诈检测模型，解决不平衡数据问题",
          "example_code": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, StratifiedKFold\nfrom sklearn.metrics import classification_report, confusion_matrix, precision_recall_curve, auc\nfrom sklearn.metrics import roc_curve, roc_auc_score, precision_score, recall_score, f1_score, accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neural_network import MLPClassifier\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.pipeline import Pipeline as ImbPipeline\nfrom sklearn.pipeline import Pipeline\nfrom app.services.docker_matplotlib_fix import configure_matplotlib_fonts\n\n# 配置字体以支持中文显示\nconfigure_matplotlib_fonts()\nfrom app.services.bank_analysis import bank_service\n\n# 加载已处理好的欺诈数据（假设前面步骤已完成特征工程）\ndf = bank_service.load_credit_fraud_data()\nprint(f\"加载数据集，共{df.shape[0]}行，{df.shape[1]}列\")\n\n# 1. 数据准备\nprint(\"\\n=== 数据准备 ===\")\n\n# 检查类别分布\nclass_counts = df['Class'].value_counts()\nprint(\"类别分布:\")\nfor label, count in class_counts.items():\n    percentage = count / len(df) * 100\n    fraud_label = \"欺诈交易\" if label == 1 else \"正常交易\"\n    print(f\"{fraud_label}: {count} ({percentage:.4f}%)\")\n\n# 基于前一步骤选择特征\n# 这里演示使用，假设我们选择了原始V特征和一些工程特征\nbase_features = [col for col in df.columns if col.startswith('V')]\nengineered_features = ['Amount', 'LogAmount']\n\n# 如果前一步创建了这些特征，则使用它们\nif 'Hour' in df.columns:\n    engineered_features.append('Hour')\nif 'AnomalyScore' in df.columns:\n    engineered_features.append('AnomalyScore')\nif 'TimeRiskScore' in df.columns:\n    engineered_features.append('TimeRiskScore')\n\n# 组合所有特征\nfeatures = base_features + engineered_features\nprint(f\"\\n使用{len(features)}个特征训练模型\")\n\n# 分离特征和目标变量\nX = df[features]\ny = df['Class']\n\n# 拆分训练集和测试集\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n\nprint(f\"训练集: {X_train.shape[0]}行, 测试集: {X_test.shape[0]}行\")\nprint(f\"训练集欺诈比例: {y_train.mean()*100:.4f}%\")\nprint(f\"测试集欺诈比例: {y_test.mean()*100:.4f}%\")\n\n# 2. 处理不平衡数据问题\nprint(\"\\n=== 处理不平衡数据 ===\")\n\n# 2.1 类别加权\nweight_for_0 = 1.0\nweight_for_1 = class_counts[0] / class_counts[1]\nclass_weights = {0: weight_for_0, 1: weight_for_1}\nprint(f\"类别权重 - 正常交易: {weight_for_0}, 欺诈交易: {weight_for_1:.4f}\")\n\n# 2.2 SMOTE过采样\nprint(\"\\n应用SMOTE过采样技术...\")\nsmote = SMOTE(random_state=42)\nX_resampled, y_resampled = smote.fit_resample(X_train, y_train)\nprint(f\"SMOTE后 - 训练集: {X_resampled.shape[0]}行\")\nprint(f\"SMOTE后 - 欺诈比例: {y_resampled.mean()*100:.4f}%\")\n\n# 2.3 随机欠采样\nprint(\"\\n应用随机欠采样技术...\")\nunder_sampler = RandomUnderSampler(random_state=42)\nX_undersampled, y_undersampled = under_sampler.fit_resample(X_train, y_train)\nprint(f\"欠采样后 - 训练集: {X_undersampled.shape[0]}行\")\nprint(f\"欠采样后 - 欺诈比例: {y_undersampled.mean()*100:.4f}%\")\n\n# 3. 模型评估函数\nprint(\"\\n=== 定义模型评估函数 ===\")\n\ndef evaluate_model(model, X_test, y_test, model_name=\"模型\", threshold=0.5):\n    \"\"\"评估模型性能\"\"\"\n    # 预测概率\n    if hasattr(model, \"predict_proba\"):\n        y_proba = model.predict_proba(X_test)[:, 1]\n    else:  # 如果模型不支持predict_proba，使用决策函数\n        y_proba = model.decision_function(X_test)\n        y_proba = (y_proba - y_proba.min()) / (y_proba.max() - y_proba.min())\n    \n    # 基于概率阈值的预测\n    y_pred = (y_proba >= threshold).astype(int)\n    \n    # 计算评估指标\n    accuracy = accuracy_score(y_test, y_pred)\n    precision = precision_score(y_test, y_pred)\n    recall = recall_score(y_test, y_pred)\n    f1 = f1_score(y_test, y_pred)\n    auc_score = roc_auc_score(y_test, y_proba)\n    \n    # 混淆矩阵\n    cm = confusion_matrix(y_test, y_pred)\n    tn, fp, fn, tp = cm.ravel()\n    \n    # 输出结果\n    print(f\"\\n{model_name}评估结果:\")\n    print(f\"准确率: {accuracy:.4f}\")\n    print(f\"精确率: {precision:.4f}\")\n    print(f\"召回率: {recall:.4f}\")\n    print(f\"F1分数: {f1:.4f}\")\n    print(f\"AUC: {auc_score:.4f}\")\n    \n    print(\"\\n混淆矩阵:\")\n    print(f\"真负例(TN): {tn}  |  假正例(FP): {fp}\")\n    print(f\"假负例(FN): {fn}  |  真正例(TP): {tp}\")\n    \n    # 成本计算（假设欺诈交易未检出的成本远高于误报的成本）\n    cost_fn = 10  # 假设漏检一起欺诈交易的成本是误报10倍\n    cost_fp = 1\n    total_cost = fn * cost_fn + fp * cost_fp\n    print(f\"总成本: {total_cost} (FN成本: {fn*cost_fn}, FP成本: {fp*cost_fp})\")\n    \n    return {\n        \"model\": model,\n        \"name\": model_name,\n        \"accuracy\": accuracy,\n        \"precision\": precision,\n        \"recall\": recall,\n        \"f1\": f1,\n        \"auc\": auc_score,\n        \"confusion_matrix\": cm,\n        \"y_proba\": y_proba,\n        \"y_pred\": y_pred,\n        \"threshold\": threshold,\n        \"cost\": total_cost\n    }\n\n# 4. 训练多种基础模型\nprint(\"\\n=== 训练基础模型 ===\")\n\n# 4.1 标准化数据\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\nX_resampled_scaled = scaler.transform(X_resampled)\nX_undersampled_scaled = scaler.transform(X_undersampled)\n\n# 4.2 创建不同的模型 - 使用原始不平衡数据 + 类别权重\nprint(\"\\n使用原始不平衡数据 + 类别权重训练模型\")\n\n# 逻辑回归\nlogistic = LogisticRegression(class_weight=class_weights, max_iter=1000, random_state=42)\nlogistic.fit(X_train_scaled, y_train)\nresults_logistic = evaluate_model(logistic, X_test_scaled, y_test, \"逻辑回归 (类别权重)\")\n\n# 随机森林\nrf = RandomForestClassifier(class_weight=class_weights, n_estimators=100, random_state=42)\nrf.fit(X_train_scaled, y_train)\nresults_rf = evaluate_model(rf, X_test_scaled, y_test, \"随机森林 (类别权重)\")\n\n# 4.3 使用SMOTE过采样数据训练模型\nprint(\"\\n使用SMOTE过采样数据训练模型\")\n\n# 支持向量机\nsvm = SVC(gamma='auto', probability=True, random_state=42)\nsvm.fit(X_resampled_scaled, y_resampled)\nresults_svm = evaluate_model(svm, X_test_scaled, y_test, \"SVM (SMOTE)\")\n\n# 梯度提升树\ngbc = GradientBoostingClassifier(n_estimators=100, random_state=42)\ngbc.fit(X_resampled_scaled, y_resampled)\nresults_gbc = evaluate_model(gbc, X_test_scaled, y_test, \"梯度提升树 (SMOTE)\")\n\n# 4.4 使用欠采样数据训练模型\nprint(\"\\n使用欠采样数据训练模型\")\n\n# 多层感知机\nmlp = MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=300, random_state=42)\nmlp.fit(X_undersampled_scaled, y_undersampled)\nresults_mlp = evaluate_model(mlp, X_test_scaled, y_test, \"多层感知机 (欠采样)\")\n\n# 5. 集成多个模型\nprint(\"\\n=== 集成多个模型 ===\")\n\n# 创建投票分类器\nvoting_clf = VotingClassifier(\n    estimators=[\n        ('logistic', logistic),\n        ('rf', rf),\n        ('svm', svm),\n        ('gbc', gbc)\n    ],\n    voting='soft'  # 使用概率而不是硬投票\n)\n\n# 在原始训练数据上拟合\nvoting_clf.fit(X_train_scaled, y_train)\nresults_voting = evaluate_model(voting_clf, X_test_scaled, y_test, \"投票分类器 (集成)\")\n\n# 6. 特征重要性分析\nprint(\"\\n=== 特征重要性分析 ===\")\n\n# 获取随机森林的特征重要性\nfeature_importances = rf.feature_importances_\n\n# 创建特征重要性数据框\nimportance_df = pd.DataFrame({\n    '特征': features,\n    '重要性': feature_importances\n})\nimportance_df = importance_df.sort_values('重要性', ascending=False)\n\n# 显示前15个最重要的特征\nprint(\"\\n随机森林特征重要性 (前15个):\")\nprint(importance_df.head(15))\n\n# 可视化特征重要性\nplt.figure(figsize=(12, 8))\nsns.barplot(x='重要性', y='特征', data=importance_df.head(15))\nplt.title('欺诈检测模型 - 特征重要性')\nplt.tight_layout()\nplt.show()\n\n# 7. 交叉验证评估最佳模型\nprint(\"\\n=== 交叉验证评估最佳模型 ===\")\n\n# 假设随机森林是最佳模型（根据前面的评估）\nbest_model = rf\n\n# 定义交叉验证策略 - 使用分层K折交叉验证\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n# 对精确率、召回率和F1分数进行交叉验证\ncv_precision = cross_val_score(best_model, X_train_scaled, y_train, cv=skf, scoring='precision')\ncv_recall = cross_val_score(best_model, X_train_scaled, y_train, cv=skf, scoring='recall')\ncv_f1 = cross_val_score(best_model, X_train_scaled, y_train, cv=skf, scoring='f1')\ncv_auc = cross_val_score(best_model, X_train_scaled, y_train, cv=skf, scoring='roc_auc')\n\nprint(\"随机森林交叉验证结果:\")\nprint(f\"精确率: {cv_precision.mean():.4f} ± {cv_precision.std():.4f}\")\nprint(f\"召回率: {cv_recall.mean():.4f} ± {cv_recall.std():.4f}\")\nprint(f\"F1分数: {cv_f1.mean():.4f} ± {cv_f1.std():.4f}\")\nprint(f\"AUC: {cv_auc.mean():.4f} ± {cv_auc.std():.4f}\")\n\n# 8. 成本敏感学习\nprint(\"\\n=== 成本敏感学习 ===\")\n\n# 创建一个自定义的成本敏感管道\n# 先使用SMOTE过采样，然后应用随机森林分类器\ncost_sensitive_pipeline = ImbPipeline([\n    ('sampling', SMOTE(random_state=42)),\n    ('scaler', StandardScaler()),\n    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))\n])\n\n# 拟合管道\ncost_sensitive_pipeline.fit(X_train, y_train)\n\n# 评估成本敏感模型\nresults_cost = evaluate_model(\n    cost_sensitive_pipeline, X_test, y_test, \"成本敏感随机森林 (SMOTE)\"\n)\n\n# 9. 可视化模型比较\nprint(\"\\n=== 可视化模型比较 ===\")\n\n# 收集所有模型结果\nmodels = [results_logistic, results_rf, results_svm, results_gbc, \n          results_mlp, results_voting, results_cost]\nmodel_names = [model['name'] for model in models]\n\n# 准备比较数据\nmetrics = ['precision', 'recall', 'f1', 'auc']\nmetric_names = ['精确率', '召回率', 'F1分数', 'AUC']\ncomparison_data = []\n\nfor model in models:\n    for metric, metric_name in zip(metrics, metric_names):\n        comparison_data.append({\n            '模型': model['name'],\n            '指标': metric_name,\n            '值': model[metric]\n        })\n\ncomparison_df = pd.DataFrame(comparison_data)\n\n# 可视化比较\nplt.figure(figsize=(14, 8))\nsns.barplot(x='模型', y='值', hue='指标', data=comparison_df)\nplt.title('欺诈检测模型性能比较')\nplt.xticks(rotation=45)\nplt.ylim(0, 1)\nplt.tight_layout()\nplt.show()\n\n# 比较各模型的成本\ncosts = [model['cost'] for model in models]\n\nplt.figure(figsize=(12, 6))\nsns.barplot(x=model_names, y=costs)\nplt.title('欺诈检测模型成本比较 (越低越好)')\nplt.ylabel('成本 (FN * 10 + FP)')\nplt.xlabel('模型')\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n\n# 10. ROC曲线比较\nplt.figure(figsize=(12, 8))\n\n# 为每个模型绘制ROC曲线\nfor model in models:\n    fpr, tpr, _ = roc_curve(y_test, model['y_proba'])\n    plt.plot(fpr, tpr, label=f\"{model['name']} (AUC = {model['auc']:.4f})\")\n\n# 添加随机猜测的基准线\nplt.plot([0, 1], [0, 1], 'k--', label='随机猜测')\nplt.xlabel('假正例率 (FPR)')\nplt.ylabel('真正例率 (TPR)')\nplt.title('ROC曲线比较')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n\n# 11. 精确率-召回率曲线比较\nplt.figure(figsize=(12, 8))\n\n# 为每个模型绘制PR曲线\nfor model in models:\n    precision_curve, recall_curve, _ = precision_recall_curve(y_test, model['y_proba'])\n    plt.plot(recall_curve, precision_curve, label=f\"{model['name']} (F1 = {model['f1']:.4f})\")\n\n# 添加基准线（数据集中正例的比例）\nbaseline = y_test.mean()\nplt.axhline(y=baseline, color='k', linestyle='--', label=f'基准 (正例比例 = {baseline:.4f})')\n\nplt.xlabel('召回率')\nplt.ylabel('精确率')\nplt.title('精确率-召回率曲线比较')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n\n# 12. 总结最佳模型\nprint(\"\\n=== 欺诈检测模型总结 ===\")\n\n# 确定最佳模型（基于F1分数）\nbest_model_idx = np.argmax([model['f1'] for model in models])\nbest_model_result = models[best_model_idx]\n\nprint(f\"最佳模型: {best_model_result['name']}\")\nprint(f\"F1分数: {best_model_result['f1']:.4f}\")\nprint(f\"精确率: {best_model_result['precision']:.4f}\")\nprint(f\"召回率: {best_model_result['recall']:.4f}\")\nprint(f\"AUC: {best_model_result['auc']:.4f}\")\nprint(f\"总成本: {best_model_result['cost']}\")\n\nprint(\"\\n欺诈检测模型构建的关键点:\")\nprint(\"1. 数据极度不平衡，需要特殊处理技术\")\nprint(\"2. 成本敏感学习对欺诈检测至关重要\")\nprint(\"3. 模型的选择应基于业务需求（如成本最小化或高召回率）\")\nprint(\"4. 集成多个模型可以提高整体性能\")\nprint(\"5. 特征工程和选择对模型性能有显著影响\")\n\nprint(\"\\n下一步工作:\")\nprint(\"1. 进一步调优最佳模型的超参数\")\nprint(\"2. 探索不同阈值对模型性能的影响\")\nprint(\"3. 分析误分类的案例，找出改进方向\")\nprint(\"4. 实施在线学习策略，适应欺诈模式的变化\")"
        },
        {
          "id": 5,
          "title": "模型调优与评估",
          "description": "使用精确率-召回率和AUC评估模型性能",
          "example_code": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, cross_val_score, StratifiedKFold\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\nfrom sklearn.metrics import confusion_matrix, classification_report, roc_curve, precision_recall_curve\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.svm import SVC\nfrom imblearn.over_sampling import SMOTE\nimport matplotlib as mpl\nfrom app.services.docker_matplotlib_fix import configure_matplotlib_fonts\n\n# 配置字体以支持中文显示\nconfigure_matplotlib_fonts()\nfrom app.services.bank_analysis import bank_service\n\n# 加载欺诈数据\ndf = bank_service.load_credit_fraud_data()\nprint(f\"加载数据集，共{df.shape[0]}行，{df.shape[1]}列\")\n\n# 1. 数据准备\nprint(\"\\n=== 数据准备 ===\")\n\n# 选择特征\nfeature_columns = [col for col in df.columns if col.startswith('V')] + ['Amount']\nX = df[feature_columns]\ny = df['Class']\n\n# 标准化特征\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# 划分训练集和测试集\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42, stratify=y)\n\n# 处理类别不平衡\nsmote = SMOTE(random_state=42)\nX_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n\n# 2. 模型优化 - 逻辑回归\nprint(\"\\n=== 逻辑回归模型优化 ===\")\n\n# 定义参数网格\nparam_grid_lr = {\n    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n    'penalty': ['l1', 'l2'],\n    'solver': ['liblinear', 'saga'],\n    'class_weight': [None, 'balanced']\n}\n\n# 使用网格搜索找到最佳参数\ngrid_lr = GridSearchCV(\n    LogisticRegression(max_iter=1000, random_state=42),\n    param_grid_lr,\n    cv=5,\n    scoring='f1',\n    n_jobs=-1\n)\ngrid_lr.fit(X_train_resampled, y_train_resampled)\n\n# 输出最佳参数\nprint(f\"最佳参数: {grid_lr.best_params_}\")\nprint(f\"最佳F1分数: {grid_lr.best_score_:.4f}\")\n\n# 使用最佳参数的模型\nbest_lr = grid_lr.best_estimator_\n\n# 3. 模型优化 - 随机森林\nprint(\"\\n=== 随机森林模型优化 ===\")\n\n# 使用随机搜索而不是网格搜索来加速\nparam_dist_rf = {\n    'n_estimators': [50, 100, 200],\n    'max_depth': [None, 10, 20, 30],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 4],\n    'max_features': ['auto', 'sqrt'],\n    'class_weight': [None, 'balanced', 'balanced_subsample']\n}\n\nrandom_search_rf = RandomizedSearchCV(\n    RandomForestClassifier(random_state=42),\n    param_distributions=param_dist_rf,\n    n_iter=20,\n    cv=5,\n    scoring='f1',\n    n_jobs=-1,\n    random_state=42\n)\nrandom_search_rf.fit(X_train_resampled, y_train_resampled)\n\n# 输出最佳参数\nprint(f\"最佳参数: {random_search_rf.best_params_}\")\nprint(f\"最佳F1分数: {random_search_rf.best_score_:.4f}\")\n\n# 使用最佳参数的模型\nbest_rf = random_search_rf.best_estimator_\n\n# 4. 交叉验证评估\nprint(\"\\n=== 交叉验证评估 ===\")\n\n# 定义要评估的模型\nmodels = {\n    '逻辑回归': best_lr,\n    '随机森林': best_rf,\n    '梯度提升树': GradientBoostingClassifier(random_state=42)\n}\n\n# 定义交叉验证策略\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n# 评估每个模型\ncv_results = {}\nfor name, model in models.items():\n    cv_scores = cross_val_score(model, X_train_resampled, y_train_resampled, cv=skf, scoring='f1')\n    cv_results[name] = {\n        'mean': cv_scores.mean(),\n        'std': cv_scores.std(),\n        'scores': cv_scores\n    }\n    print(f\"{name}: F1 = {cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")\n\n# 5. 最终模型评估\nprint(\"\\n=== 最终模型评估 ===\")\n\n# 在测试集上评估每个模型\ntest_results = {}\nfor name, model in models.items():\n    # 在重采样的训练集上训练模型\n    model.fit(X_train_resampled, y_train_resampled)\n    \n    # 在测试集上预测\n    y_pred = model.predict(X_test)\n    y_prob = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else None\n    \n    # 计算评估指标\n    accuracy = accuracy_score(y_test, y_pred)\n    precision = precision_score(y_test, y_pred)\n    recall = recall_score(y_test, y_pred)\n    f1 = f1_score(y_test, y_pred)\n    auc = roc_auc_score(y_test, y_prob) if y_prob is not None else None\n    \n    # 保存结果\n    test_results[name] = {\n        'accuracy': accuracy,\n        'precision': precision,\n        'recall': recall,\n        'f1': f1,\n        'auc': auc,\n        'y_pred': y_pred,\n        'y_prob': y_prob\n    }\n    \n    # 输出评估结果\n    print(f\"\\n{name}评估结果:\")\n    print(f\"准确率: {accuracy:.4f}\")\n    print(f\"精确率: {precision:.4f}\")\n    print(f\"召回率: {recall:.4f}\")\n    print(f\"F1分数: {f1:.4f}\")\n    if auc:\n        print(f\"ROC AUC: {auc:.4f}\")\n    \n    # 混淆矩阵\n    cm = confusion_matrix(y_test, y_pred)\n    tn, fp, fn, tp = cm.ravel()\n    print(f\"\\n混淆矩阵:\")\n    print(f\"真负例(TN): {tn}\")\n    print(f\"假正例(FP): {fp}\")\n    print(f\"假负例(FN): {fn}\")\n    print(f\"真正例(TP): {tp}\")\n\n# 6. 可视化比较\nprint(\"\\n=== 模型性能比较 ===\")\n\n# 创建比较表格\ncomparison_df = pd.DataFrame({\n    '模型': list(models.keys()),\n    '准确率': [test_results[name]['accuracy'] for name in models],\n    '精确率': [test_results[name]['precision'] for name in models],\n    '召回率': [test_results[name]['recall'] for name in models],\n    'F1分数': [test_results[name]['f1'] for name in models],\n    'ROC AUC': [test_results[name]['auc'] for name in models if test_results[name]['auc'] is not None]\n})\nprint(comparison_df)\n\n# 绘制ROC曲线\nplt.figure(figsize=(12, 6))\n\n# ROC曲线\nplt.subplot(1, 2, 1)\nfor name in models.keys():\n    if test_results[name]['y_prob'] is not None:\n        fpr, tpr, _ = roc_curve(y_test, test_results[name]['y_prob'])\n        auc = test_results[name]['auc']\n        plt.plot(fpr, tpr, label=f'{name} (AUC = {auc:.3f})')\n\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlabel('假正例率 (FPR)')\nplt.ylabel('真正例率 (TPR)')\nplt.title('ROC曲线比较')\nplt.legend()\n\n# 精确率-召回率曲线\nplt.subplot(1, 2, 2)\nfor name in models.keys():\n    if test_results[name]['y_prob'] is not None:\n        precision_values, recall_values, _ = precision_recall_curve(y_test, test_results[name]['y_prob'])\n        plt.plot(recall_values, precision_values, label=f'{name}')\n\nplt.xlabel('召回率')\nplt.ylabel('精确率')\nplt.title('精确率-召回率曲线比较')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n\n# 7. 成本敏感评估\nprint(\"\\n=== 成本敏感评估 ===\")\n\n# 假设错误分类的成本\nfn_cost = 100  # 漏检欺诈的成本\nfp_cost = 10   # 误报的成本\n\n# 计算每个模型的总成本\nfor name in models.keys():\n    cm = confusion_matrix(y_test, test_results[name]['y_pred'])\n    tn, fp, fn, tp = cm.ravel()\n    total_cost = (fn * fn_cost) + (fp * fp_cost)\n    avg_cost_per_transaction = total_cost / len(y_test)\n    print(f\"{name}的总成本: {total_cost}，平均每笔交易成本: {avg_cost_per_transaction:.2f}\")\n\n# 8. 不同阈值下的模型性能\n# 选择最佳模型进行阈值分析\nbest_model_name = max(test_results, key=lambda x: test_results[x]['f1'])\nbest_model = models[best_model_name]\nbest_y_prob = test_results[best_model_name]['y_prob']\n\nif best_y_prob is not None:\n    thresholds = np.arange(0.1, 1.0, 0.1)\n    threshold_results = []\n    \n    for threshold in thresholds:\n        y_pred_threshold = (best_y_prob >= threshold).astype(int)\n        precision = precision_score(y_test, y_pred_threshold)\n        recall = recall_score(y_test, y_pred_threshold)\n        f1 = f1_score(y_test, y_pred_threshold)\n        cm = confusion_matrix(y_test, y_pred_threshold)\n        tn, fp, fn, tp = cm.ravel()\n        cost = (fn * fn_cost) + (fp * fp_cost)\n        \n        threshold_results.append({\n            '阈值': threshold,\n            '精确率': precision,\n            '召回率': recall,\n            'F1分数': f1,\n            '成本': cost\n        })\n    \n    threshold_df = pd.DataFrame(threshold_results)\n    print(f\"\\n{best_model_name}在不同阈值下的性能:\")\n    print(threshold_df)\n    \n    # 找出F1分数最高的阈值\n    best_f1_threshold = threshold_df.loc[threshold_df['F1分数'].idxmax(), '阈值']\n    # 找出成本最低的阈值\n    best_cost_threshold = threshold_df.loc[threshold_df['成本'].idxmin(), '阈值']\n    \n    print(f\"\\nF1分数最高的阈值: {best_f1_threshold}\")\n    print(f\"成本最低的阈值: {best_cost_threshold}\")\n\n# 9. 模型调优总结\nprint(\"\\n=== 模型调优总结 ===\")\n\n# 性能最好的模型\nbest_model_name = max(test_results, key=lambda x: test_results[x]['f1'])\nbest_f1 = test_results[best_model_name]['f1']\nbest_recall = test_results[best_model_name]['recall']\nbest_precision = test_results[best_model_name]['precision']\n\nprint(f\"性能最佳的模型是: {best_model_name}，F1分数: {best_f1:.4f}\")\nprint(f\"其精确率: {best_precision:.4f}，召回率: {best_recall:.4f}\")\n\n# 调优建议\nprint(\"\\n模型调优建议:\")\nprint(\"1. 特征工程是提升欺诈检测模型性能的关键\")\nprint(\"2. 类别不平衡处理对于提高模型的召回率至关重要\")\nprint(\"3. 阈值选择应基于业务目标，权衡精确率和召回率\")\nprint(\"4. 定期重新训练模型以适应欺诈模式的变化\")\nprint(\"5. 考虑使用集成方法，综合多个模型的优势\");"
        },
        {
          "id": 6,
          "title": "欺诈预警系统",
          "description": "设计实时欺诈交易监控与预警系统架构",
          "example_code": "# 欺诈预警系统架构设计\nimport pandas as pd\nimport numpy as np\nfrom sklearn.ensemble import RandomForestClassifier\nfrom flask import Flask, request, jsonify\nimport redis\nimport json\nfrom datetime import datetime\n\n# 欺诈检测模型类\nclass FraudDetectionModel:\n    def __init__(self):\n        self.model = RandomForestClassifier(n_estimators=100)\n        self.threshold = 0.7  # 欺诈预警阈值\n        \n    def predict_proba(self, transaction):\n        # 在实际系统中，这里会进行特征提取和预测\n        return 0.85  # 模拟返回欺诈概率\n\n# 交易处理组件\nclass TransactionProcessor:\n    def __init__(self, model):\n        self.model = model\n        \n    def process_transaction(self, transaction):\n        # 获取欺诈概率\n        fraud_probability = self.model.predict_proba(transaction)\n        \n        # 返回预警结果\n        is_fraud = fraud_probability >= self.model.threshold\n        return {\n            'transaction_id': transaction.get('id'),\n            'fraud_probability': fraud_probability,\n            'is_fraud': is_fraud,\n            'timestamp': datetime.now().isoformat()\n        }\n\n# 预警管理组件\nclass AlertManager:\n    def __init__(self, redis_client=None):\n        self.redis_client = redis_client\n        self.alert_levels = {\n            'low': {'min': 0.7, 'max': 0.8},\n            'medium': {'min': 0.8, 'max': 0.9},\n            'high': {'min': 0.9, 'max': 1.0}\n        }\n        \n    def generate_alert(self, result):\n        if not result['is_fraud']:\n            return None\n            \n        # 确定预警级别\n        score = result['fraud_probability']\n        level = 'high'\n        for name, config in self.alert_levels.items():\n            if config['min'] <= score < config['max']:\n                level = name\n                break\n        \n        # 创建预警对象\n        alert = {\n            'alert_id': f\"alert_{result['transaction_id']}\",\n            'level': level,\n            'fraud_probability': score,\n            'timestamp': result['timestamp']\n        }\n        \n        # 存储预警 (实际项目中会存入Redis或数据库)\n        return alert\n\n# 系统初始化和API设计\ndef create_fraud_api():\n    model = FraudDetectionModel()\n    processor = TransactionProcessor(model)\n    alert_manager = AlertManager()\n    \n    app = Flask('fraud_warning_system')\n    \n    @app.route('/predict', methods=['POST'])\n    def predict():\n        transaction = request.get_json()\n        result = processor.process_transaction(transaction)\n        \n        if result['is_fraud']:\n            alert = alert_manager.generate_alert(result)\n            if alert:\n                result['alert'] = alert\n        \n        return jsonify(result)\n    \n    return app\n\n# 运行系统\nif __name__ == \"__main__\":\n    app = create_fraud_api()\n    app.run(host='0.0.0.0', port=5000)"
        }
      ]
    },
    {
      "id": 4,
      "title": "股票技术指标分析",
      "description": "计算和分析股票技术指标，包括移动平均线、MACD、RSI等",
      "category": "security",
      "difficulty_level": 2,
      "estimated_duration": 90,
      "prerequisites": "基本的金融和股票知识",
      "data_source": "股票历史价格数据",
      "steps": [
        {
          "id": 1,
          "title": "数据获取",
          "description": "获取股票历史价格数据",
          "example_code": "# 股票数据获取\nimport pandas as pd\nimport yfinance as yf\nimport matplotlib.pyplot as plt\nfrom datetime import datetime, timedelta\n\n# 设置时间范围\nend_date = datetime.now()\nstart_date = end_date - timedelta(days=365)  # 获取一年的数据\n\n# 获取股票数据\ndef get_stock_data(ticker, start, end):\n    \"\"\"\n    获取股票历史价格数据\n    \n    参数:\n        ticker (str): 股票代码\n        start (datetime): 开始日期\n        end (datetime): 结束日期\n    \n    返回:\n        pandas.DataFrame: 包含股票历史价格数据的DataFrame\n    \"\"\"\n    try:\n        # 使用yfinance下载数据\n        data = yf.download(ticker, start=start, end=end)\n        \n        # 检查数据是否为空\n        if data.empty:\n            print(f\"无法获取 {ticker} 的数据\")\n            return None\n            \n        print(f\"成功获取 {ticker} 的数据，共 {len(data)} 条记录\")\n        return data\n    except Exception as e:\n        print(f\"获取数据时出错: {str(e)}\")\n        return None\n\n# 设置要分析的股票列表\ntickers = ['AAPL', 'MSFT', 'GOOG', 'AMZN']\n\n# 下载并存储数据\nstock_data = {}\nfor ticker in tickers:\n    print(f\"正在获取 {ticker} 的历史数据...\")\n    data = get_stock_data(ticker, start_date, end_date)\n    if data is not None:\n        stock_data[ticker] = data\n\n# 查看数据结构\nif 'AAPL' in stock_data:\n    print(\"\\n苹果股票数据示例:\")\n    print(stock_data['AAPL'].head())\n    \n    # 显示基本统计信息\n    print(\"\\n统计信息:\")\n    print(stock_data['AAPL'].describe())\n    \n    # 绘制股价走势图\n    plt.figure(figsize=(12, 6))\n    plt.plot(stock_data['AAPL']['Close'])\n    plt.title('AAPL 收盘价走势')\n    plt.xlabel('日期')\n    plt.ylabel('价格 (USD)')\n    plt.grid(True)\n    plt.tight_layout()\n    plt.show()\n\n# 将数据保存为CSV文件\nfor ticker, data in stock_data.items():\n    data.to_csv(f\"{ticker}_historical_data.csv\")\n    print(f\"已将 {ticker} 的数据保存到 {ticker}_historical_data.csv\")"
        },
        {
          "id": 2,
          "title": "趋势指标计算",
          "description": "计算移动平均线、MACD等趋势指标",
          "example_code": "# 趋势指标计算\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport yfinance as yf\nfrom datetime import datetime, timedelta\n\n# 设置时间范围\nend_date = datetime.now()\nstart_date = end_date - timedelta(days=365)  # 获取一年的数据\n\n# 获取示例股票数据\nticker = 'AAPL'\nprint(f\"获取 {ticker} 的历史数据...\")\ndata = yf.download(ticker, start=start_date, end=end_date)\n\nif data.empty:\n    print(f\"无法获取 {ticker} 的数据\")\nelse:\n    print(f\"成功获取 {ticker} 的数据，共 {len(data)} 条记录\")\n    \n    # 计算简单移动平均线 (SMA)\n    def calculate_sma(data, window):\n        # 计算简单移动平均线\n        return data['Close'].rolling(window=window).mean()\n    \n    # 计算指数移动平均线 (EMA)\n    def calculate_ema(data, span):\n        # 计算指数移动平均线\n        return data['Close'].ewm(span=span, adjust=False).mean()\n    \n    # 计算MACD (Moving Average Convergence Divergence)\n    def calculate_macd(data, fast_span=12, slow_span=26, signal_span=9):\n        # 计算MACD指标\n        # 计算快速和慢速EMA\n        fast_ema = calculate_ema(data, fast_span)\n        slow_ema = calculate_ema(data, slow_span)\n        \n        # 计算MACD线\n        macd_line = fast_ema - slow_ema\n        \n        # 计算信号线 (MACD的EMA)\n        signal_line = macd_line.ewm(span=signal_span, adjust=False).mean()\n        \n        # 计算柱状图 (Histogram)\n        histogram = macd_line - signal_line\n        \n        return macd_line, signal_line, histogram\n    \n    # 计算布林带 (Bollinger Bands)\n    def calculate_bollinger_bands(data, window=20, num_std=2):\n        # 计算布林带\n        sma = calculate_sma(data, window)\n        std = data['Close'].rolling(window=window).std()\n        upper_band = sma + (std * num_std)\n        lower_band = sma - (std * num_std)\n        return upper_band, sma, lower_band\n    \n    # 计算不同周期的移动平均线\n    data['SMA5'] = calculate_sma(data, 5)\n    data['SMA20'] = calculate_sma(data, 20)\n    data['SMA60'] = calculate_sma(data, 60)\n    data['EMA12'] = calculate_ema(data, 12)\n    data['EMA26'] = calculate_ema(data, 26)\n    \n    # 计算MACD\n    data['MACD'], data['MACD_Signal'], data['MACD_Histogram'] = calculate_macd(data)\n    \n    # 计算布林带\n    data['BB_Upper'], data['BB_Middle'], data['BB_Lower'] = calculate_bollinger_bands(data)\n    \n    # 显示计算后的数据\n    print(\"\n计算后的数据示例:\")\n    print(data.tail())\n    \n    # 绘制移动平均线\n    plt.figure(figsize=(12, 6))\n    plt.plot(data.index, data['Close'], label='收盘价', alpha=0.5)\n    plt.plot(data.index, data['SMA5'], label='SMA5', linewidth=1)\n    plt.plot(data.index, data['SMA20'], label='SMA20', linewidth=1)\n    plt.plot(data.index, data['SMA60'], label='SMA60', linewidth=1.5)\n    plt.title(f'{ticker} 股价和移动平均线')\n    plt.xlabel('日期')\n    plt.ylabel('价格 (USD)')\n    plt.legend()\n    plt.grid(True)\n    plt.tight_layout()\n    plt.show()\n    \n    # 绘制MACD\n    plt.figure(figsize=(12, 6))\n    \n    # 上半部分绘制股价\n    plt.subplot(2, 1, 1)\n    plt.plot(data.index, data['Close'], label='收盘价')\n    plt.title(f'{ticker} 股价')\n    plt.ylabel('价格 (USD)')\n    plt.grid(True)\n    plt.legend()\n    \n    # 下半部分绘制MACD\n    plt.subplot(2, 1, 2)\n    plt.plot(data.index, data['MACD'], label='MACD')\n    plt.plot(data.index, data['MACD_Signal'], label='信号线')\n    plt.bar(data.index, data['MACD_Histogram'], label='柱状图', alpha=0.5)\n    plt.title('MACD指标')\n    plt.xlabel('日期')\n    plt.ylabel('MACD值')\n    plt.legend()\n    plt.grid(True)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # 绘制布林带\n    plt.figure(figsize=(12, 6))\n    plt.plot(data.index, data['Close'], label='收盘价', alpha=0.7)\n    plt.plot(data.index, data['BB_Upper'], label='上轨', linestyle='--', linewidth=1)\n    plt.plot(data.index, data['BB_Middle'], label='中轨', linewidth=1)\n    plt.plot(data.index, data['BB_Lower'], label='下轨', linestyle='--', linewidth=1)\n    plt.title(f'{ticker} 布林带')\n    plt.xlabel('日期')\n    plt.ylabel('价格 (USD)')\n    plt.legend()\n    plt.grid(True)\n    plt.tight_layout()\n    plt.show()"
        },
        {
          "id": 3,
          "title": "摆动指标计算",
          "description": "计算RSI、KDJ等摆动指标",
          "example_code": "# 摆动指标计算\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport yfinance as yf\nfrom datetime import datetime, timedelta\n\n# 设置时间范围\nend_date = datetime.now()\nstart_date = end_date - timedelta(days=365)  # 获取一年的数据\n\n# 获取示例股票数据\nticker = 'AAPL'\nprint(f\"获取 {ticker} 的历史数据...\")\ndata = yf.download(ticker, start=start_date, end=end_date)\n\nif data.empty:\n    print(f\"无法获取 {ticker} 的数据\")\nelse:\n    print(f\"成功获取 {ticker} 的数据，共 {len(data)} 条记录\")\n    \n    # 计算相对强弱指数 (RSI)\n    def calculate_rsi(data, window=14):\n        # 计算相对强弱指数 (RSI)\n        # 计算价格变化\n        delta = data['Close'].diff()\n        \n        # 区分上涨和下跌\n        gain = delta.copy()\n        loss = delta.copy()\n        gain[gain < 0] = 0\n        loss[loss > 0] = 0\n        loss = abs(loss)\n        \n        # 计算平均上涨和下跌\n        avg_gain = gain.rolling(window=window).mean()\n        avg_loss = loss.rolling(window=window).mean()\n        \n        # 计算相对强弱值和RSI\n        rs = avg_gain / avg_loss\n        rsi = 100 - (100 / (1 + rs))\n        \n        return rsi\n    \n    # 计算随机振荡器 (Stochastic Oscillator)\n    def calculate_stochastic(data, k_window=14, d_window=3):\n        # 计算随机振荡器指标 (KDJ)\n        # 计算N天内的最高价和最低价\n        low_min = data['Low'].rolling(window=k_window).min()\n        high_max = data['High'].rolling(window=k_window).max()\n        \n        # 计算K值：(收盘价 - 最低价) / (最高价 - 最低价) * 100\n        k = 100 * ((data['Close'] - low_min) / (high_max - low_min))\n        \n        # 计算D值：K值的移动平均\n        d = k.rolling(window=d_window).mean()\n        \n        # 计算J值：3*K - 2*D\n        j = 3 * k - 2 * d\n        \n        return k, d, j\n    \n    # 计算威廉指标 (Williams %R)\n    def calculate_williams_r(data, window=14):\n        # 计算威廉指标 (Williams %R)\n        # 计算N天内的最高价和最低价\n        low_min = data['Low'].rolling(window=window).min()\n        high_max = data['High'].rolling(window=window).max()\n        \n        # 计算威廉指标：(最高价 - 收盘价) / (最高价 - 最低价) * -100\n        williams_r = -100 * ((high_max - data['Close']) / (high_max - low_min))\n        \n        return williams_r\n    \n    # 计算顺势指标 (CCI - Commodity Channel Index)\n    def calculate_cci(data, window=20):\n        # 计算顺势指标 (CCI)\n        # 计算典型价格 (Typical Price)\n        tp = (data['High'] + data['Low'] + data['Close']) / 3\n        \n        # 计算典型价格的简单移动平均\n        tp_sma = tp.rolling(window=window).mean()\n        \n        # 计算平均绝对偏差\n        mad = tp.rolling(window=window).apply(lambda x: np.mean(np.abs(x - np.mean(x))))\n        \n        # 计算CCI: (典型价格 - 典型价格的SMA) / (0.015 * 平均绝对偏差)\n        cci = (tp - tp_sma) / (0.015 * mad)\n        \n        return cci\n    \n    # 计算不同摆动指标\n    data['RSI'] = calculate_rsi(data)\n    data['K'], data['D'], data['J'] = calculate_stochastic(data)\n    data['Williams_R'] = calculate_williams_r(data)\n    data['CCI'] = calculate_cci(data)\n    \n    # 显示计算后的数据\n    print(\"\n计算后的数据示例:\")\n    print(data.tail())\n    \n    # 绘制RSI\n    plt.figure(figsize=(12, 6))\n    \n    # 上半部分绘制股价\n    plt.subplot(2, 1, 1)\n    plt.plot(data.index, data['Close'], label='收盘价')\n    plt.title(f'{ticker} 股价')\n    plt.ylabel('价格 (USD)')\n    plt.grid(True)\n    plt.legend()\n    \n    # 下半部分绘制RSI\n    plt.subplot(2, 1, 2)\n    plt.plot(data.index, data['RSI'], label='RSI')\n    plt.axhline(y=70, color='r', linestyle='--', alpha=0.5)\n    plt.axhline(y=30, color='g', linestyle='--', alpha=0.5)\n    plt.title('相对强弱指数 (RSI)')\n    plt.xlabel('日期')\n    plt.ylabel('RSI值')\n    plt.ylim(0, 100)\n    plt.grid(True)\n    plt.legend()\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # 绘制KDJ\n    plt.figure(figsize=(12, 6))\n    \n    # 上半部分绘制股价\n    plt.subplot(2, 1, 1)\n    plt.plot(data.index, data['Close'], label='收盘价')\n    plt.title(f'{ticker} 股价')\n    plt.ylabel('价格 (USD)')\n    plt.grid(True)\n    plt.legend()\n    \n    # 下半部分绘制KDJ\n    plt.subplot(2, 1, 2)\n    plt.plot(data.index, data['K'], label='K线')\n    plt.plot(data.index, data['D'], label='D线')\n    plt.plot(data.index, data['J'], label='J线', alpha=0.5)\n    plt.axhline(y=80, color='r', linestyle='--', alpha=0.3)\n    plt.axhline(y=20, color='g', linestyle='--', alpha=0.3)\n    plt.title('随机振荡器 (KDJ)')\n    plt.xlabel('日期')\n    plt.ylabel('KDJ值')\n    plt.legend()\n    plt.grid(True)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # 绘制威廉指标\n    plt.figure(figsize=(12, 6))\n    \n    # 上半部分绘制股价\n    plt.subplot(2, 1, 1)\n    plt.plot(data.index, data['Close'], label='收盘价')\n    plt.title(f'{ticker} 股价')\n    plt.ylabel('价格 (USD)')\n    plt.grid(True)\n    plt.legend()\n    \n    # 下半部分绘制威廉指标\n    plt.subplot(2, 1, 2)\n    plt.plot(data.index, data['Williams_R'], label='Williams %R')\n    plt.axhline(y=-20, color='r', linestyle='--', alpha=0.5)\n    plt.axhline(y=-80, color='g', linestyle='--', alpha=0.5)\n    plt.title('威廉指标 (Williams %R)')\n    plt.xlabel('日期')\n    plt.ylabel('Williams %R值')\n    plt.ylim(-100, 0)\n    plt.grid(True)\n    plt.legend()\n    \n    plt.tight_layout()\n    plt.show()"
        },
        {
          "id": 4,
          "title": "指标可视化",
          "description": "绘制技术指标和价格图表",
          "example_code": "# 技术指标可视化\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nimport yfinance as yf\nfrom datetime import datetime, timedelta\nimport mplfinance as mpf\nimport seaborn as sns\n\n# 设置样式\nplt.style.use('seaborn-v0_8-darkgrid')\nsns.set_style(\"darkgrid\")\n\n# 设置时间范围\nend_date = datetime.now()\nstart_date = end_date - timedelta(days=365)  # 获取一年的数据\n\n# 获取示例股票数据\nticker = 'AAPL'\nprint(f\"获取 {ticker} 的历史数据...\")\ndata = yf.download(ticker, start=start_date, end=end_date)\n\nif data.empty:\n    print(f\"无法获取 {ticker} 的数据\")\nelse:\n    print(f\"成功获取 {ticker} 的数据，共 {len(data)} 条记录\")\n    \n    # 计算技术指标\n    # 1. 移动平均线\n    data['SMA20'] = data['Close'].rolling(window=20).mean()\n    data['SMA50'] = data['Close'].rolling(window=50).mean()\n    data['EMA20'] = data['Close'].ewm(span=20, adjust=False).mean()\n    \n    # 2. 布林带\n    data['SMA20'] = data['Close'].rolling(window=20).mean()\n    data['STD20'] = data['Close'].rolling(window=20).std()\n    data['BOL_upper'] = data['SMA20'] + (data['STD20'] * 2)\n    data['BOL_lower'] = data['SMA20'] - (data['STD20'] * 2)\n    \n    # 3. MACD\n    data['EMA12'] = data['Close'].ewm(span=12, adjust=False).mean()\n    data['EMA26'] = data['Close'].ewm(span=26, adjust=False).mean()\n    data['MACD'] = data['EMA12'] - data['EMA26']\n    data['MACD_signal'] = data['MACD'].ewm(span=9, adjust=False).mean()\n    data['MACD_hist'] = data['MACD'] - data['MACD_signal']\n    \n    # 4. RSI\n    delta = data['Close'].diff()\n    gain = delta.copy()\n    loss = delta.copy()\n    gain[gain < 0] = 0\n    loss[loss > 0] = 0\n    loss = abs(loss)\n    avg_gain = gain.rolling(window=14).mean()\n    avg_loss = loss.rolling(window=14).mean()\n    rs = avg_gain / avg_loss\n    data['RSI'] = 100 - (100 / (1 + rs))\n    \n    # 5. 随机振荡器\n    # K线\n    low_14 = data['Low'].rolling(window=14).min()\n    high_14 = data['High'].rolling(window=14).max()\n    data['K'] = 100 * ((data['Close'] - low_14) / (high_14 - low_14))\n    # D线\n    data['D'] = data['K'].rolling(window=3).mean()\n    \n    # 移除空值\n    data.dropna(inplace=True)\n    \n    # 1. 创建包含K线图和多个指标的组合图表\n    def plot_stock_with_indicators(data, ticker):\n        # 绘制股票K线图和多个技术指标\n        # 创建一个有6行的图表布局\n        fig = plt.figure(figsize=(12, 16))\n        gs = gridspec.GridSpec(6, 1, height_ratios=[3, 1, 1, 1, 1, 1])\n        \n        # 主图：K线图\n        ax1 = plt.subplot(gs[0])\n        ax1.plot(data.index, data['Close'], label='收盘价')\n        ax1.plot(data.index, data['SMA20'], label='SMA20')\n        ax1.plot(data.index, data['SMA50'], label='SMA50')\n        ax1.plot(data.index, data['BOL_upper'], 'r--', label='布林上轨')\n        ax1.plot(data.index, data['BOL_lower'], 'g--', label='布林下轨')\n        ax1.fill_between(data.index, data['BOL_lower'], data['BOL_upper'], color='gray', alpha=0.1)\n        ax1.set_title(f'{ticker} 股价和技术指标', fontsize=15)\n        ax1.set_ylabel('价格 (USD)')\n        ax1.legend()\n        ax1.grid(True)\n        \n        # 成交量\n        ax2 = plt.subplot(gs[1], sharex=ax1)\n        ax2.bar(data.index, data['Volume'], color='blue', alpha=0.5, label='成交量')\n        ax2.set_ylabel('成交量')\n        ax2.grid(True)\n        ax2.legend()\n        \n        # MACD\n        ax3 = plt.subplot(gs[2], sharex=ax1)\n        ax3.plot(data.index, data['MACD'], label='MACD')\n        ax3.plot(data.index, data['MACD_signal'], label='信号线')\n        ax3.bar(data.index, data['MACD_hist'], label='柱状图', alpha=0.5, color='green')\n        ax3.axhline(y=0, color='r', linestyle='--', alpha=0.3)\n        ax3.set_ylabel('MACD')\n        ax3.legend()\n        ax3.grid(True)\n        \n        # RSI\n        ax4 = plt.subplot(gs[3], sharex=ax1)\n        ax4.plot(data.index, data['RSI'], label='RSI')\n        ax4.axhline(y=70, color='r', linestyle='--', alpha=0.3)\n        ax4.axhline(y=30, color='g', linestyle='--', alpha=0.3)\n        ax4.fill_between(data.index, data['RSI'], 70, where=(data['RSI'] >= 70),\n                       color='red', alpha=0.3)\n        ax4.fill_between(data.index, data['RSI'], 30, where=(data['RSI'] <= 30),\n                       color='green', alpha=0.3)\n        ax4.set_ylabel('RSI')\n        ax4.set_ylim(0, 100)\n        ax4.legend()\n        ax4.grid(True)\n        \n        # KD\n        ax5 = plt.subplot(gs[4], sharex=ax1)\n        ax5.plot(data.index, data['K'], label='K线')\n        ax5.plot(data.index, data['D'], label='D线')\n        ax5.axhline(y=80, color='r', linestyle='--', alpha=0.3)\n        ax5.axhline(y=20, color='g', linestyle='--', alpha=0.3)\n        ax5.fill_between(data.index, data['K'], 80, where=(data['K'] >= 80),\n                       color='red', alpha=0.3)\n        ax5.fill_between(data.index, data['K'], 20, where=(data['K'] <= 20),\n                       color='green', alpha=0.3)\n        ax5.set_ylabel('KD')\n        ax5.set_ylim(0, 100)\n        ax5.legend()\n        ax5.grid(True)\n        \n        # 添加相关性热力图\n        ax6 = plt.subplot(gs[5])\n        # 计算指标之间的相关性\n        correlation_data = data[['Close', 'SMA20', 'EMA20', 'MACD', 'RSI', 'K', 'D']].corr()\n        # 绘制热力图\n        sns.heatmap(correlation_data, annot=True, cmap='coolwarm', ax=ax6, \n                  linewidths=0.5, fmt='.2f', cbar=True)\n        ax6.set_title('指标相关性')\n        \n        # 调整布局\n        plt.tight_layout()\n        plt.show()\n    \n    # 2. 使用mplfinance创建专业蜡烛图\n    def plot_candlestick_chart(data):\n        # 使用mplfinance创建专业蜡烛图\n        # 添加移动平均线\n        apds = [mpf.make_addplot(data['SMA20'], color='blue'),\n              mpf.make_addplot(data['SMA50'], color='red'),\n              mpf.make_addplot(data['BOL_upper'], color='gray', linestyle='--'),\n              mpf.make_addplot(data['BOL_lower'], color='gray', linestyle='--')]\n        \n        # 创建蜡烛图\n        mpf.plot(data, type='candle', style='yahoo', volume=True, \n               title=f'{ticker} 蜡烛图和技术指标', \n               addplot=apds,\n               figsize=(12, 8))\n    \n    # 3. 创建交互式图表（实际环境中可使用plotly或bokeh）\n    def create_interactive_chart():\n        # 说明：在实际环境下，可使用plotly或bokeh创建交互式图表。\n        # 在这个例子中，我们只展示代码结构。\n        # \n        # 示例代码（使用plotly）：\n        # import plotly.graph_objects as go\n        # from plotly.subplots import make_subplots\n        # \n        # fig = make_subplots(rows=2, cols=1, shared_xaxes=True, \n        #                    vertical_spacing=0.1, row_heights=[0.7, 0.3])\n        #                    \n        # fig.add_trace(go.Candlestick(\n        #     x=data.index,\n        #     open=data['Open'],\n        #     high=data['High'],\n        #     low=data['Low'],\n        #     close=data['Close'],\n        #     name='K线'\n        # ), row=1, col=1)\n        # \n        # fig.add_trace(go.Scatter(\n        #     x=data.index,\n        #     y=data['SMA20'],\n        #     name='SMA20'\n        # ), row=1, col=1)\n        # \n        # fig.add_trace(go.Bar(\n        #     x=data.index,\n        #     y=data['Volume'],\n        #     name='成交量'\n        # ), row=2, col=1)\n        # \n        # fig.update_layout(title=f'{ticker} 交互式图表',\n        #                  xaxis_title='日期',\n        #                  yaxis_title='价格',\n        #                  xaxis_rangeslider_visible=False)\n        #                  \n        # fig.show()\n        print(\"在实际环境中，可使用plotly或bokeh创建交互式图表。\")\n    \n    # 展示不同类型的可视化\n    print(\"\n1. 综合技术指标图表\")\n    plot_stock_with_indicators(data[-120:], ticker)  # 展示最近120个交易日\n    \n    print(\"\n2. 蜡烛图和技术指标\")\n    plot_candlestick_chart(data[-60:])  # 展示最近60个交易日\n    \n    print(\"\n3. 交互式图表说明\")\n    create_interactive_chart()\n    \n    print(\"\n技术指标可视化完成，可以进一步分析指标产生的交易信号。\")\"\n"
        },
        {
          "id": 5,
          "title": "交易信号分析",
          "description": "分析指标产生的交易信号",
          "example_code": "# 交易信号分析\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport yfinance as yf\nfrom datetime import datetime, timedelta\nimport matplotlib.gridspec as gridspec\n\n# 设置时间范围\nend_date = datetime.now()\nstart_date = end_date - timedelta(days=365*2)  # 获取两年的数据\n\n# 获取示例股票数据\nticker = 'AAPL'\nprint(f\"获取 {ticker} 的历史数据...\")\ndata = yf.download(ticker, start=start_date, end=end_date)\n\nif data.empty:\n    print(f\"无法获取 {ticker} 的数据\")\nelse:\n    print(f\"成功获取 {ticker} 的数据，共 {len(data)} 条记录\")\n    \n    # 计算技术指标\n    # 1. 移动平均线\n    data['SMA20'] = data['Close'].rolling(window=20).mean()\n    data['SMA50'] = data['Close'].rolling(window=50).mean()\n    \n    # 2. MACD\n    data['EMA12'] = data['Close'].ewm(span=12, adjust=False).mean()\n    data['EMA26'] = data['Close'].ewm(span=26, adjust=False).mean()\n    data['MACD'] = data['EMA12'] - data['EMA26']\n    data['MACD_signal'] = data['MACD'].ewm(span=9, adjust=False).mean()\n    data['MACD_hist'] = data['MACD'] - data['MACD_signal']\n    \n    # 3. RSI\n    delta = data['Close'].diff()\n    gain = delta.copy()\n    loss = delta.copy()\n    gain[gain < 0] = 0\n    loss[loss > 0] = 0\n    loss = abs(loss)\n    avg_gain = gain.rolling(window=14).mean()\n    avg_loss = loss.rolling(window=14).mean()\n    rs = avg_gain / avg_loss\n    data['RSI'] = 100 - (100 / (1 + rs))\n    \n    # 4. 布林带\n    data['BOL_middle'] = data['Close'].rolling(window=20).mean()\n    data['BOL_std'] = data['Close'].rolling(window=20).std()\n    data['BOL_upper'] = data['BOL_middle'] + 2 * data['BOL_std']\n    data['BOL_lower'] = data['BOL_middle'] - 2 * data['BOL_std']\n    \n    # 5. KD指标\n    low_14 = data['Low'].rolling(window=14).min()\n    high_14 = data['High'].rolling(window=14).max()\n    data['K'] = 100 * ((data['Close'] - low_14) / (high_14 - low_14))\n    data['D'] = data['K'].rolling(window=3).mean()\n    \n    # 生成交易信号\n    # 1. 移动平均线交叉信号\n    data['SMA_Signal'] = 0\n    data.loc[data['SMA20'] > data['SMA50'], 'SMA_Signal'] = 1  # 买入信号\n    data.loc[data['SMA20'] < data['SMA50'], 'SMA_Signal'] = -1  # 卖出信号\n    \n    # 2. MACD交叉信号\n    data['MACD_Signal'] = 0\n    data.loc[data['MACD'] > data['MACD_signal'], 'MACD_Signal'] = 1  # 买入信号\n    data.loc[data['MACD'] < data['MACD_signal'], 'MACD_Signal'] = -1  # 卖出信号\n    \n    # 3. RSI超买超卖信号\n    data['RSI_Signal'] = 0\n    data.loc[data['RSI'] < 30, 'RSI_Signal'] = 1  # 超卖，买入信号\n    data.loc[data['RSI'] > 70, 'RSI_Signal'] = -1  # 超买，卖出信号\n    \n    # 4. 布林带信号\n    data['BOL_Signal'] = 0\n    data.loc[data['Close'] < data['BOL_lower'], 'BOL_Signal'] = 1  # 价格低于下轨，买入信号\n    data.loc[data['Close'] > data['BOL_upper'], 'BOL_Signal'] = -1  # 价格高于上轨，卖出信号\n    \n    # 5. KD指标信号\n    data['KD_Signal'] = 0\n    data.loc[(data['K'] < 20) & (data['K'] > data['D']), 'KD_Signal'] = 1  # K值在超卖区并上穿D，买入信号\n    data.loc[(data['K'] > 80) & (data['K'] < data['D']), 'KD_Signal'] = -1  # K值在超买区并下穿D，卖出信号\n    \n    # 组合信号（简单多数投票）\n    signal_columns = ['SMA_Signal', 'MACD_Signal', 'RSI_Signal', 'BOL_Signal', 'KD_Signal']\n    data['Combined_Signal'] = data[signal_columns].sum(axis=1)\n    data['Vote_Signal'] = 0\n    data.loc[data['Combined_Signal'] >= 2, 'Vote_Signal'] = 1  # 至少两个买入信号\n    data.loc[data['Combined_Signal'] <= -2, 'Vote_Signal'] = -1  # 至少两个卖出信号\n    \n    # 识别交易点\n    data['Trade'] = 0\n    data.loc[data['Vote_Signal'].diff() == 1, 'Trade'] = 1  # 新买入信号\n    data.loc[data['Vote_Signal'].diff() == -2, 'Trade'] = -1  # 从买入变为卖出\n    \n    # 移除NaN值\n    data = data.dropna()\n    \n    # 可视化交易信号\n    def plot_trading_signals(data, ticker):\n        # 绘制股票价格和交易信号\n        fig = plt.figure(figsize=(14, 12))\n        gs = gridspec.GridSpec(4, 1, height_ratios=[3, 1, 1, 1])\n        \n        # 主图：股价和移动平均线\n        ax1 = plt.subplot(gs[0])\n        ax1.plot(data.index, data['Close'], label='收盘价', alpha=0.7)\n        ax1.plot(data.index, data['SMA20'], label='SMA20', alpha=0.7)\n        ax1.plot(data.index, data['SMA50'], label='SMA50', alpha=0.7)\n        # 绘制布林带\n        ax1.plot(data.index, data['BOL_upper'], 'r--', label='布林上轨', alpha=0.3)\n        ax1.plot(data.index, data['BOL_middle'], 'g--', label='布林中轨', alpha=0.3)\n        ax1.plot(data.index, data['BOL_lower'], 'r--', label='布林下轨', alpha=0.3)\n        ax1.fill_between(data.index, data['BOL_lower'], data['BOL_upper'], color='gray', alpha=0.1)\n        \n        # 标记买入和卖出点\n        buy_signals = data[data['Trade'] == 1]\n        sell_signals = data[data['Trade'] == -1]\n        \n        ax1.scatter(buy_signals.index, buy_signals['Close'], marker='^', color='green', s=100, label='买入信号')\n        ax1.scatter(sell_signals.index, sell_signals['Close'], marker='v', color='red', s=100, label='卖出信号')\n        \n        ax1.set_title(f'{ticker} 股价和交易信号', fontsize=15)\n        ax1.set_ylabel('价格 (USD)')\n        ax1.legend()\n        ax1.grid(True)\n        \n        # MACD\n        ax2 = plt.subplot(gs[1], sharex=ax1)\n        ax2.plot(data.index, data['MACD'], label='MACD')\n        ax2.plot(data.index, data['MACD_signal'], label='信号线')\n        ax2.bar(data.index, data['MACD_hist'], label='柱状图', color='green', alpha=0.5)\n        ax2.axhline(y=0, color='r', linestyle='--', alpha=0.3)\n        \n        # 标记MACD交叉点\n        macd_buy = data[(data['MACD_Signal'] == 1) & (data['MACD_Signal'].shift(1) != 1)]\n        macd_sell = data[(data['MACD_Signal'] == -1) & (data['MACD_Signal'].shift(1) != -1)]\n        \n        ax2.scatter(macd_buy.index, macd_buy['MACD'], marker='^', color='green', s=50, label='MACD买入')\n        ax2.scatter(macd_sell.index, macd_sell['MACD'], marker='v', color='red', s=50, label='MACD卖出')\n        \n        ax2.set_ylabel('MACD')\n        ax2.legend()\n        ax2.grid(True)\n        \n        # RSI\n        ax3 = plt.subplot(gs[2], sharex=ax1)\n        ax3.plot(data.index, data['RSI'], label='RSI')\n        ax3.axhline(y=70, color='r', linestyle='--', alpha=0.3)\n        ax3.axhline(y=30, color='g', linestyle='--', alpha=0.3)\n        ax3.fill_between(data.index, data['RSI'], 70, where=(data['RSI'] >= 70), color='red', alpha=0.3)\n        ax3.fill_between(data.index, data['RSI'], 30, where=(data['RSI'] <= 30), color='green', alpha=0.3)\n        \n        # 标记RSI超买超卖点\n        rsi_buy = data[(data['RSI_Signal'] == 1) & (data['RSI_Signal'].shift(1) != 1)]\n        rsi_sell = data[(data['RSI_Signal'] == -1) & (data['RSI_Signal'].shift(1) != -1)]\n        \n        ax3.scatter(rsi_buy.index, rsi_buy['RSI'], marker='^', color='green', s=50, label='RSI买入')\n        ax3.scatter(rsi_sell.index, rsi_sell['RSI'], marker='v', color='red', s=50, label='RSI卖出')\n        \n        ax3.set_ylabel('RSI')\n        ax3.set_ylim(0, 100)\n        ax3.legend()\n        ax3.grid(True)\n        \n        # 组合信号\n        ax4 = plt.subplot(gs[3], sharex=ax1)\n        ax4.plot(data.index, data['Combined_Signal'], label='综合信号')\n        ax4.axhline(y=2, color='g', linestyle='--', alpha=0.3)\n        ax4.axhline(y=-2, color='r', linestyle='--', alpha=0.3)\n        ax4.axhline(y=0, color='k', linestyle='-', alpha=0.2)\n        \n        # 标记组合信号点\n        vote_buy = data[(data['Vote_Signal'] == 1) & (data['Vote_Signal'].shift(1) != 1)]\n        vote_sell = data[(data['Vote_Signal'] == -1) & (data['Vote_Signal'].shift(1) != -1)]\n        \n        ax4.scatter(vote_buy.index, vote_buy['Combined_Signal'], marker='^', color='green', s=50, label='组合买入')\n        ax4.scatter(vote_sell.index, vote_sell['Combined_Signal'], marker='v', color='red', s=50, label='组合卖出')\n        \n        ax4.set_ylabel('组合信号')\n        ax4.set_xlabel('日期')\n        ax4.legend()\n        ax4.grid(True)\n        \n        plt.tight_layout()\n        plt.show()\n    \n    # 信号频率分析\n    def analyze_signal_frequency(data):\n        # 分析各指标的信号频率\n        signal_columns = ['SMA_Signal', 'MACD_Signal', 'RSI_Signal', 'BOL_Signal', 'KD_Signal', 'Vote_Signal']\n        \n        signal_stats = pd.DataFrame(index=signal_columns)\n        \n        # 计算买入信号次数\n        signal_stats['买入信号(1)'] = [sum(data[col] == 1) for col in signal_columns]\n        \n        # 计算卖出信号次数\n        signal_stats['卖出信号(-1)'] = [sum(data[col] == -1) for col in signal_columns]\n        \n        # 计算中性信号次数\n        signal_stats['中性信号(0)'] = [sum(data[col] == 0) for col in signal_columns]\n        \n        # 计算信号变化次数\n        signal_stats['信号变化次数'] = [sum(data[col].diff() != 0) for col in signal_columns]\n        \n        # 计算总记录数\n        signal_stats['总记录数'] = len(data)\n        \n        # 计算买入信号百分比\n        signal_stats['买入信号比例'] = signal_stats['买入信号(1)'] / signal_stats['总记录数'] * 100\n        \n        # 计算卖出信号百分比\n        signal_stats['卖出信号比例'] = signal_stats['卖出信号(-1)'] / signal_stats['总记录数'] * 100\n        \n        return signal_stats\n    \n    # 信号一致性分析\n    def analyze_signal_consistency(data):\n        # 分析不同指标信号的一致性\n        signal_columns = ['SMA_Signal', 'MACD_Signal', 'RSI_Signal', 'BOL_Signal', 'KD_Signal']\n        \n        # 创建一致性矩阵\n        consistency_matrix = pd.DataFrame(index=signal_columns, columns=signal_columns)\n        \n        # 计算每对指标之间的信号一致性\n        for i, col1 in enumerate(signal_columns):\n            for j, col2 in enumerate(signal_columns):\n                if i == j:\n                    consistency_matrix.loc[col1, col2] = 100  # 自身一致性为100%\n                else:\n                    # 计算信号相同的百分比\n                    same_signals = sum(data[col1] == data[col2])\n                    consistency_matrix.loc[col1, col2] = same_signals / len(data) * 100\n        \n        return consistency_matrix\n    \n    # 执行信号分析\n    print(\"\n绘制交易信号...\")\n    plot_trading_signals(data, ticker)\n    \n    print(\"\n信号频率分析:\")\n    signal_freq = analyze_signal_frequency(data)\n    print(signal_freq)\n    \n    print(\"\n信号一致性分析 (%):\")\n    signal_consistency = analyze_signal_consistency(data)\n    print(signal_consistency)\n    \n    # 绘制信号频率柱状图\n    plt.figure(figsize=(12, 6))\n    signal_freq[['买入信号比例', '卖出信号比例']].plot(kind='bar')\n    plt.title('各指标买入卖出信号频率')\n    plt.ylabel('信号比例 (%)')\n    plt.grid(True, axis='y')\n    plt.tight_layout()\n    plt.show()\n    \n    # 绘制信号一致性热力图\n    plt.figure(figsize=(10, 8))\n    plt.imshow(signal_consistency, cmap='YlGnBu')\n    plt.colorbar(label='一致性百分比 (%)')\n    plt.xticks(range(len(signal_columns)), signal_columns, rotation=45)\n    plt.yticks(range(len(signal_columns)), signal_columns)\n    \n    # 添加数值标签\n    for i in range(len(signal_columns)):\n        for j in range(len(signal_columns)):\n            plt.text(j, i, f\"{signal_consistency.iloc[i, j]:.1f}%\", \n                   ha=\"center\", va=\"center\", \n                   color=\"black\" if signal_consistency.iloc[i, j] > 50 else \"white\")\n            \n    plt.title('技术指标信号一致性矩阵')\n    plt.tight_layout()\n    plt.show()"
        },
        {
          "id": 6,
          "title": "回测策略",
          "description": "回测基于技术指标的交易策略",
          "example_code": "# 基于技术指标的交易策略回测\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport yfinance as yf\nfrom datetime import datetime, timedelta\nimport matplotlib.gridspec as gridspec\n\n# 设置时间范围\nend_date = datetime.now()\nstart_date = end_date - timedelta(days=365*3)  # 获取三年的数据\n\n# 获取示例股票数据\nticker = 'AAPL'\nprint(f\"获取 {ticker} 的历史数据...\")\ndata = yf.download(ticker, start=start_date, end=end_date)\n\nif data.empty:\n    print(f\"无法获取 {ticker} 的数据\")\nelse:\n    print(f\"成功获取 {ticker} 的数据，共 {len(data)} 条记录\")\n    \n    # 1. 定义交易策略类\n    class TradingStrategy:\n        # 交易策略基类\n        \n        def __init__(self, data):\n            self.data = data.copy()\n            self.signals = pd.DataFrame(index=data.index)\n            self.signals['signal'] = 0  # 0: 无信号, 1: 买入, -1: 卖出\n            self.signals['position'] = 0  # 0: 空仓, 1: 持仓\n            \n        def generate_signals(self):\n            # 生成交易信号，子类中实现具体策略\n            pass\n            \n        def calculate_positions(self):\n            # 根据信号计算持仓\n            # 初始化持仓列\n            self.signals['position'] = 0\n            \n            # 根据信号计算持仓\n            # 1表示持有，0表示空仓\n            self.signals['position'] = self.signals['signal'].cumsum()\n            self.signals['position'] = self.signals['position'].clip(lower=0, upper=1)\n            \n        def backtest(self, initial_capital=10000.0, transaction_cost=0.001):\n            # 回测策略\n            # 确保已生成信号和持仓\n            if 'position' not in self.signals.columns:\n                self.calculate_positions()\n                \n            # 创建一个包含回测结果的DataFrame\n            backtest = pd.DataFrame(index=self.signals.index)\n            backtest['price'] = self.data['Close']\n            backtest['signal'] = self.signals['signal']\n            backtest['position'] = self.signals['position']\n            \n            # 计算每日收益\n            backtest['returns'] = self.data['Close'].pct_change()\n            \n            # 计算策略收益\n            backtest['strategy_returns'] = backtest['position'].shift(1) * backtest['returns']\n            \n            # 考虑交易成本\n            backtest['trade'] = backtest['position'].diff().abs()\n            backtest['cost'] = backtest['trade'] * transaction_cost\n            backtest['strategy_returns'] = backtest['strategy_returns'] - backtest['cost']\n            \n            # 计算累计收益\n            backtest['cum_returns'] = (1 + backtest['returns']).cumprod()\n            backtest['cum_strategy_returns'] = (1 + backtest['strategy_returns']).cumprod()\n            \n            # 计算资金曲线\n            backtest['equity'] = initial_capital * backtest['cum_strategy_returns']\n            \n            # 计算最大回撤\n            backtest['peak'] = backtest['equity'].cummax()\n            backtest['drawdown'] = (backtest['equity'] - backtest['peak']) / backtest['peak']\n            \n            return backtest\n        \n        def evaluate_performance(self, backtest_results):\n            # 评估策略表现\n            # 计算总收益率\n            total_return = backtest_results['cum_strategy_returns'].iloc[-1] - 1\n            \n            # 计算年化收益率\n            days = (backtest_results.index[-1] - backtest_results.index[0]).days\n            annual_return = ((1 + total_return) ** (365 / days)) - 1\n            \n            # 计算夏普比率\n            risk_free_rate = 0.02  # 假设无风险利率为2%\n            sharpe_ratio = ((backtest_results['strategy_returns'].mean() * 252) - risk_free_rate) /                           (backtest_results['strategy_returns'].std() * np.sqrt(252))\n            \n            # 计算最大回撤\n            max_drawdown = backtest_results['drawdown'].min()\n            \n            # 计算胜率\n            winning_trades = sum(backtest_results['strategy_returns'] > 0)\n            total_trades = sum(backtest_results['trade'] > 0)\n            win_rate = winning_trades / total_trades if total_trades > 0 else 0\n            \n            # 计算收益风险比\n            risk_reward_ratio = abs(total_return / max_drawdown) if max_drawdown != 0 else float('inf')\n            \n            # 汇总结果\n            performance = {\n                'Total Return': f\"{total_return * 100:.2f}%\",\n                'Annual Return': f\"{annual_return * 100:.2f}%\",\n                'Sharpe Ratio': f\"{sharpe_ratio:.2f}\",\n                'Max Drawdown': f\"{max_drawdown * 100:.2f}%\",\n                'Win Rate': f\"{win_rate * 100:.2f}%\",\n                'Risk Reward Ratio': f\"{risk_reward_ratio:.2f}\",\n                'Total Trades': total_trades\n            }\n            \n            return performance\n        \n        def plot_results(self, backtest_results, ticker):\n            # 绘制回测结果\n            fig = plt.figure(figsize=(14, 12))\n            gs = gridspec.GridSpec(3, 1, height_ratios=[2, 1, 1])\n            \n            # 股价和信号\n            ax1 = plt.subplot(gs[0])\n            ax1.plot(backtest_results.index, self.data['Close'], label='股价')\n            \n            # 标记买入和卖出点\n            buy_signals = backtest_results[backtest_results['signal'] == 1]\n            sell_signals = backtest_results[backtest_results['signal'] == -1]\n            \n            ax1.scatter(buy_signals.index, self.data.loc[buy_signals.index, 'Close'], \n                       marker='^', color='green', s=100, label='买入信号')\n            ax1.scatter(sell_signals.index, self.data.loc[sell_signals.index, 'Close'], \n                       marker='v', color='red', s=100, label='卖出信号')\n            \n            ax1.set_title(f'{ticker} - 股价和交易信号', fontsize=15)\n            ax1.set_ylabel('价格')\n            ax1.legend()\n            ax1.grid(True)\n            \n            # 资金曲线\n            ax2 = plt.subplot(gs[1], sharex=ax1)\n            ax2.plot(backtest_results.index, backtest_results['equity'], label='策略资金曲线')\n            ax2.plot(backtest_results.index, \n                    backtest_results['price'] / backtest_results['price'].iloc[0] * backtest_results['equity'].iloc[0], \n                    'r--', label='Buy & Hold')\n            ax2.set_ylabel('资金')\n            ax2.legend()\n            ax2.grid(True)\n            \n            # 回撤\n            ax3 = plt.subplot(gs[2], sharex=ax1)\n            ax3.fill_between(backtest_results.index, backtest_results['drawdown'] * 100, 0, \n                           color='red', alpha=0.3)\n            ax3.set_ylabel('回撤 (%)')\n            ax3.set_xlabel('日期')\n            ax3.set_ylim(backtest_results['drawdown'].min() * 100 * 1.1, 5)\n            ax3.grid(True)\n            \n            plt.tight_layout()\n            plt.show()\n    \n    # 2. 定义具体策略实现\n    class MovingAverageCrossStrategy(TradingStrategy):\n        # 移动平均线交叉策略\n        \n        def __init__(self, data, short_window=20, long_window=50):\n            super().__init__(data)\n            self.short_window = short_window\n            self.long_window = long_window\n            self.generate_signals()\n            \n        def generate_signals(self):\n            # 基于短期和长期移动平均线的交叉生成信号\n            # 计算短期和长期移动平均线\n            self.data['short_ma'] = self.data['Close'].rolling(window=self.short_window).mean()\n            self.data['long_ma'] = self.data['Close'].rolling(window=self.long_window).mean()\n            \n            # 初始化信号列\n            self.signals['signal'] = 0\n            \n            # 当短期均线上穿长期均线时，生成买入信号\n            self.signals.loc[(self.data['short_ma'] > self.data['long_ma']) & \n                           (self.data['short_ma'].shift(1) <= self.data['long_ma'].shift(1)), \n                           'signal'] = 1\n            \n            # 当短期均线下穿长期均线时，生成卖出信号\n            self.signals.loc[(self.data['short_ma'] < self.data['long_ma']) & \n                           (self.data['short_ma'].shift(1) >= self.data['long_ma'].shift(1)), \n                           'signal'] = -1\n            \n            # 计算持仓\n            self.calculate_positions()\n    \n    class MACDStrategy(TradingStrategy):\n        # MACD策略\n        \n        def __init__(self, data, fast_span=12, slow_span=26, signal_span=9):\n            super().__init__(data)\n            self.fast_span = fast_span\n            self.slow_span = slow_span\n            self.signal_span = signal_span\n            self.generate_signals()\n            \n        def generate_signals(self):\n            # 基于MACD和信号线的交叉生成信号\n            # 计算MACD\n            self.data['ema_fast'] = self.data['Close'].ewm(span=self.fast_span, adjust=False).mean()\n            self.data['ema_slow'] = self.data['Close'].ewm(span=self.slow_span, adjust=False).mean()\n            self.data['macd'] = self.data['ema_fast'] - self.data['ema_slow']\n            self.data['macd_signal'] = self.data['macd'].ewm(span=self.signal_span, adjust=False).mean()\n            self.data['macd_hist'] = self.data['macd'] - self.data['macd_signal']\n            \n            # 初始化信号列\n            self.signals['signal'] = 0\n            \n            # 当MACD上穿信号线时，生成买入信号\n            self.signals.loc[(self.data['macd'] > self.data['macd_signal']) & \n                           (self.data['macd'].shift(1) <= self.data['macd_signal'].shift(1)), \n                           'signal'] = 1\n            \n            # 当MACD下穿信号线时，生成卖出信号\n            self.signals.loc[(self.data['macd'] < self.data['macd_signal']) & \n                           (self.data['macd'].shift(1) >= self.data['macd_signal'].shift(1)), \n                           'signal'] = -1\n            \n            # 计算持仓\n            self.calculate_positions()\n    \n    class RSIStrategy(TradingStrategy):\n        # RSI超买超卖策略\n        \n        def __init__(self, data, window=14, overbought=70, oversold=30):\n            super().__init__(data)\n            self.window = window\n            self.overbought = overbought\n            self.oversold = oversold\n            self.generate_signals()\n            \n        def generate_signals(self):\n            # 基于RSI超买超卖生成信号\n            # 计算RSI\n            delta = self.data['Close'].diff()\n            gain = delta.copy()\n            loss = delta.copy()\n            gain[gain < 0] = 0\n            loss[loss > 0] = 0\n            loss = abs(loss)\n            avg_gain = gain.rolling(window=self.window).mean()\n            avg_loss = loss.rolling(window=self.window).mean()\n            rs = avg_gain / avg_loss\n            self.data['rsi'] = 100 - (100 / (1 + rs))\n            \n            # 初始化信号列\n            self.signals['signal'] = 0\n            \n            # 当RSI从超卖区域上升时，生成买入信号\n            self.signals.loc[(self.data['rsi'] > self.oversold) & \n                           (self.data['rsi'].shift(1) <= self.oversold), \n                           'signal'] = 1\n            \n            # 当RSI从超买区域下降时，生成卖出信号\n            self.signals.loc[(self.data['rsi'] < self.overbought) & \n                           (self.data['rsi'].shift(1) >= self.overbought), \n                           'signal'] = -1\n            \n            # 计算持仓\n            self.calculate_positions()\n    \n    # 3. 策略组合\n    class CombinedStrategy(TradingStrategy):\n        # 组合策略\n        \n        def __init__(self, data):\n            super().__init__(data)\n            \n            # 创建各个子策略\n            self.ma_strategy = MovingAverageCrossStrategy(data)\n            self.macd_strategy = MACDStrategy(data)\n            self.rsi_strategy = RSIStrategy(data)\n            \n            self.generate_signals()\n            \n        def generate_signals(self):\n            # 基于多策略投票生成信号\n            # 获取各策略的信号\n            ma_signals = self.ma_strategy.signals['signal']\n            macd_signals = self.macd_strategy.signals['signal']\n            rsi_signals = self.rsi_strategy.signals['signal']\n            \n            # 计算综合信号分数 (-3 到 3)\n            self.signals['score'] = ma_signals + macd_signals + rsi_signals\n            \n            # 初始化信号列\n            self.signals['signal'] = 0\n            \n            # 当至少两个策略给出买入信号时，生成买入信号\n            self.signals.loc[self.signals['score'] >= 2, 'signal'] = 1\n            \n            # 当至少两个策略给出卖出信号时，生成卖出信号\n            self.signals.loc[self.signals['score'] <= -2, 'signal'] = -1\n            \n            # 转换为实际交易信号（从持有变为不持有才卖出）\n            actual_signals = self.signals['signal'].copy()\n            position = 0\n            \n            for i in range(len(actual_signals)):\n                if actual_signals.iloc[i] == 1:  # 买入信号\n                    if position == 0:  # 当前无持仓\n                        position = 1\n                    else:  # 已有持仓，不重复买入\n                        actual_signals.iloc[i] = 0\n                elif actual_signals.iloc[i] == -1:  # 卖出信号\n                    if position == 1:  # 当前有持仓\n                        position = 0\n                    else:  # 已无持仓，不重复卖出\n                        actual_signals.iloc[i] = 0\n                else:  # 无信号\n                    actual_signals.iloc[i] = 0\n            \n            self.signals['signal'] = actual_signals\n            \n            # 计算持仓\n            self.calculate_positions()\n    \n    # 4. 执行回测\n    # 移除缺失值\n    data = data.dropna()\n    \n    # 创建策略实例\n    print(\"\n实例化交易策略...\")\n    ma_strategy = MovingAverageCrossStrategy(data)\n    macd_strategy = MACDStrategy(data)\n    rsi_strategy = RSIStrategy(data)\n    combined_strategy = CombinedStrategy(data)\n    \n    # 运行回测\n    print(\"\n开始回测...\")\n    ma_backtest = ma_strategy.backtest()\n    macd_backtest = macd_strategy.backtest()\n    rsi_backtest = rsi_strategy.backtest()\n    combined_backtest = combined_strategy.backtest()\n    \n    # 评估策略表现\n    print(\"\n策略表现评估:\")\n    ma_performance = ma_strategy.evaluate_performance(ma_backtest)\n    macd_performance = macd_strategy.evaluate_performance(macd_backtest)\n    rsi_performance = rsi_strategy.evaluate_performance(rsi_backtest)\n    combined_performance = combined_strategy.evaluate_performance(combined_backtest)\n    \n    # 输出结果\n    performance_table = pd.DataFrame({\n        '移动平均线策略': pd.Series(ma_performance),\n        'MACD策略': pd.Series(macd_performance),\n        'RSI策略': pd.Series(rsi_performance),\n        '组合策略': pd.Series(combined_performance)\n    })\n    \n    print(performance_table)\n    \n    # 可视化回测结果\n    print(\"\n绘制移动平均线策略回测结果...\")\n    ma_strategy.plot_results(ma_backtest, ticker)\n    \n    print(\"\n绘制MACD策略回测结果...\")\n    macd_strategy.plot_results(macd_backtest, ticker)\n    \n    print(\"\n绘制RSI策略回测结果...\")\n    rsi_strategy.plot_results(rsi_backtest, ticker)\n    \n    print(\"\n绘制组合策略回测结果...\")\n    combined_strategy.plot_results(combined_backtest, ticker)\n    \n    # 绘制各策略收益曲线对比\n    plt.figure(figsize=(12, 6))\n    plt.plot(ma_backtest.index, ma_backtest['cum_strategy_returns'], label='移动平均线策略')\n    plt.plot(macd_backtest.index, macd_backtest['cum_strategy_returns'], label='MACD策略')\n    plt.plot(rsi_backtest.index, rsi_backtest['cum_strategy_returns'], label='RSI策略')\n    plt.plot(combined_backtest.index, combined_backtest['cum_strategy_returns'], label='组合策略')\n    plt.plot(data.index, (1 + data['Close'].pct_change()).cumprod(), 'k--', label='Buy & Hold')\n    \n    plt.title(f'{ticker} - 策略收益对比')\n    plt.xlabel('日期')\n    plt.ylabel('累计收益')\n    plt.legend()\n    plt.grid(True)\n    plt.tight_layout()\n    plt.show()"
        }
      ]
    },
    {
      "id": 5,
      "title": "投资组合优化",
      "description": "使用马科维茨模型和夏普比率优化股票投资组合",
      "category": "security",
      "difficulty_level": 4,
      "estimated_duration": 150,
      "prerequisites": "投资理论和基础统计知识",
      "data_source": "多只股票历史收益率数据",
      "steps": [
        {
          "id": 1,
          "title": "数据准备",
          "description": "获取多只股票的历史价格和收益率",
          "example_code": "# 投资组合优化 - 数据准备\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport yfinance as yf\nfrom datetime import datetime, timedelta\n\n# 设置时间范围 - 获取5年数据\nend_date = datetime.now()\nstart_date = end_date - timedelta(days=365*5)\n\n# 定义要分析的股票列表 - 选择不同行业的代表性股票\ntickers = ['AAPL', 'MSFT', 'AMZN', 'GOOGL', 'BRK-B', 'JPM', 'JNJ', 'PG', 'XOM', 'HD']\nticker_names = {\n    'AAPL': '苹果', 'MSFT': '微软', 'AMZN': '亚马逊', 'GOOGL': '谷歌',\n    'BRK-B': '伯克希尔', 'JPM': '摩根大通', 'JNJ': '强生', 'PG': '宝洁',\n    'XOM': '埃克森美孚', 'HD': '家得宝'\n}\n\nprint(f\"获取 {len(tickers)} 只股票的历史数据...\")\n\n# 获取股票数据\ndef get_stock_data(tickers, start, end):\n    # 获取多只股票的历史价格数据\n    #\n    # 参数:\n    #    tickers (list): 股票代码列表\n    #    start (datetime): 开始日期\n    #    end (datetime): 结束日期\n    #\n    # 返回:\n    #    pandas.DataFrame: 包含多只股票收盘价的DataFrame\n    try:\n        data = yf.download(tickers, start=start, end=end)['Adj Close']\n        \n        # 检查数据是否完整\n        if data.isnull().sum().sum() > 0:\n            print(\"警告: 数据中存在缺失值，将使用前向填充法处理\")\n            data = data.fillna(method='ffill')\n            \n        print(f\"成功获取数据，时间范围: {data.index[0]} 至 {data.index[-1]}\")\n        \n        return data\n    except Exception as e:\n        print(f\"获取数据时出错: {str(e)}\")\n        return None\n\n# 获取股票价格数据\nprices = get_stock_data(tickers, start_date, end_date)\n\nif prices is not None:\n    # 计算日收益率\n    returns = prices.pct_change().dropna()\n    \n    # 计算月收益率 (将日期重采样为月度数据)\n    monthly_returns = prices.resample('M').last().pct_change().dropna()\n    \n    # 计算年收益率 (将日期重采样为年度数据)\n    annual_returns = prices.resample('Y').last().pct_change().dropna()\n    \n    # 显示价格数据样例\n    print(\"\n价格数据样例:\")\n    print(prices.head())\n    \n    # 显示日收益率样例\n    print(\"\n日收益率样例:\")\n    print(returns.head())\n    \n    # 计算基础统计信息\n    stats = returns.describe().T\n    stats['annualized_return'] = returns.mean() * 252  # 年化收益率\n    stats['annualized_volatility'] = returns.std() * np.sqrt(252)  # 年化波动率\n    \n    print(\"\n基本统计信息:\")\n    print(stats[['mean', 'std', 'annualized_return', 'annualized_volatility']])\n    \n    # 可视化股票价格走势\n    plt.figure(figsize=(12, 6))\n    # 将价格标准化为初始值=100，以便于比较\n    normalized_prices = prices / prices.iloc[0] * 100\n    normalized_prices.plot()\n    plt.title('股票价格走势 (初始值=100)')\n    plt.xlabel('日期')\n    plt.ylabel('标准化价格')\n    plt.legend(ticker_names.values(), loc='upper left')\n    plt.grid(True)\n    plt.tight_layout()\n    plt.show()\n    \n    # 可视化收益率分布\n    plt.figure(figsize=(12, 6))\n    for ticker in tickers:\n        plt.hist(returns[ticker], bins=50, alpha=0.3, label=ticker_names.get(ticker, ticker))\n    plt.title('日收益率分布')\n    plt.xlabel('日收益率')\n    plt.ylabel('频率')\n    plt.legend()\n    plt.grid(True)\n    plt.tight_layout()\n    plt.show()\n    \n    # 保存数据至CSV文件\n    prices.to_csv('portfolio_prices.csv')\n    returns.to_csv('portfolio_returns.csv')\n    monthly_returns.to_csv('portfolio_monthly_returns.csv')\n    \n    print(\"\n数据已保存至CSV文件，可用于后续分析\")\nelse:\n    print(\"无法获取股票数据，请检查网络连接和股票代码\")"
        },
        {
          "id": 2,
          "title": "风险和收益计算",
          "description": "计算各股票的预期收益和风险",
          "example_code": "# 投资组合优化 - 风险和收益计算\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport yfinance as yf\nfrom datetime import datetime, timedelta\nimport seaborn as sns\n\n# 加载之前保存的数据，或者重新获取\n# 此处假设已有之前步骤的数据\n# 如需重新获取数据，可执行以下代码：\n\n# 设置时间范围 - 获取5年数据\nend_date = datetime.now()\nstart_date = end_date - timedelta(days=365*5)\n\n# 定义要分析的股票列表 - 选择不同行业的代表性股票\ntickers = ['AAPL', 'MSFT', 'AMZN', 'GOOGL', 'BRK-B', 'JPM', 'JNJ', 'PG', 'XOM', 'HD']\nticker_names = {\n    'AAPL': '苹果', 'MSFT': '微软', 'AMZN': '亚马逊', 'GOOGL': '谷歌',\n    'BRK-B': '伯克希尔', 'JPM': '摩根大通', 'JNJ': '强生', 'PG': '宝洁',\n    'XOM': '埃克森美孚', 'HD': '家得宝'\n}\n\nprint(\"获取股票数据...\")\n# 获取股票数据\nprices = yf.download(tickers, start=start_date, end=end_date)['Adj Close']\n# 处理缺失值\nprices = prices.fillna(method='ffill')\n# 计算日收益率\nreturns = prices.pct_change().dropna()\n\nprint(\"计算风险和收益指标...\")\n\n# 1. 计算预期收益率和风险\n# 计算日均收益率\nmean_daily_returns = returns.mean()\n# 计算日收益率的协方差矩阵\ncov_matrix = returns.cov()\n\n# 年化收益率和风险\ntrading_days = 252  # 年交易日数量\nannual_returns = mean_daily_returns * trading_days\nannual_covariance = cov_matrix * trading_days\nannual_std = np.sqrt(np.diag(annual_covariance))  # 年化标准差\n\n# 创建风险收益汇总表\nrisk_return_summary = pd.DataFrame({\n    '年化收益率': annual_returns,\n    '年化风险(标准差)': annual_std,\n    '夏普比率': annual_returns / annual_std,  # 简化版夏普比率，假设无风险利率为0\n    '变异系数': annual_std / annual_returns\n})\n\n# 按夏普比率排序\nrisk_return_summary = risk_return_summary.sort_values('夏普比率', ascending=False)\n\nprint(\"\n股票风险收益汇总：\")\nprint(risk_return_summary)\n\n# 2. 计算更多风险指标\n# 计算最大回撤\ndef calculate_drawdown(price_series):\n    # 计算累计收益\n    wealth_index = (1 + price_series.pct_change()).cumprod()\n    # 计算之前的最大值\n    previous_peaks = wealth_index.cummax()\n    # 计算回撤\n    drawdowns = (wealth_index - previous_peaks) / previous_peaks\n    return drawdowns.min()  # 返回最大回撤\n\n# 计算下行风险（低于目标收益的标准差）\ndef downside_risk(returns, target=0):\n    # 计算低于目标收益的收益部分\n    downside_returns = returns.copy()\n    downside_returns[returns > target] = 0\n    return np.sqrt((downside_returns**2).mean()) * np.sqrt(trading_days)\n\n# 计算VaR (Value at Risk)\ndef var_historic(returns, level=5):\n    # 历史VaR，95%置信度\n    return np.percentile(returns, level) * np.sqrt(trading_days)\n\n# 计算CVaR (Conditional Value at Risk，也称Expected Shortfall)\ndef cvar_historic(returns, level=5):\n    # 计算历史CVaR\n    var = np.percentile(returns, level)\n    return returns[returns <= var].mean() * np.sqrt(trading_days)\n\n# 计算各股票的风险指标\nadvanced_risk = pd.DataFrame(index=tickers)\nfor ticker in tickers:\n    advanced_risk.loc[ticker, '最大回撤'] = calculate_drawdown(prices[ticker])\n    advanced_risk.loc[ticker, '下行风险'] = downside_risk(returns[ticker])\n    advanced_risk.loc[ticker, 'VaR(95%)'] = var_historic(returns[ticker])\n    advanced_risk.loc[ticker, 'CVaR(95%)'] = cvar_historic(returns[ticker])\n\n# 合并风险指标\nrisk_summary = pd.concat([risk_return_summary, advanced_risk], axis=1)\n\nprint(\"\n扩展风险指标：\")\nprint(risk_summary)\n\n# 可视化风险和收益\n# 1. 风险-收益散点图\nplt.figure(figsize=(12, 8))\nplt.scatter(risk_summary['年化风险(标准差)'], risk_summary['年化收益率'], \n           s=300, alpha=0.6)\n\n# 添加股票名称标签\nfor i, ticker in enumerate(risk_summary.index):\n    plt.annotate(ticker_names.get(ticker, ticker), \n                xy=(risk_summary['年化风险(标准差)'][i], risk_summary['年化收益率'][i]),\n                xytext=(5, 5), textcoords='offset points',\n                fontsize=12)\n\nplt.title('股票风险-收益图', fontsize=15)\nplt.xlabel('年化风险 (标准差)', fontsize=12)\nplt.ylabel('年化收益率', fontsize=12)\nplt.axhline(y=0, color='r', linestyle='--', alpha=0.3)\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n# 2. 夏普比率条形图\nplt.figure(figsize=(12, 6))\nrisk_summary['夏普比率'].sort_values().plot(kind='barh', color='skyblue')\nplt.title('各股票夏普比率', fontsize=15)\nplt.xlabel('夏普比率', fontsize=12)\nplt.axvline(x=0, color='r', linestyle='--', alpha=0.3)\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n# 3. 风险指标热图\nplt.figure(figsize=(14, 10))\nrisk_metrics = risk_summary[['年化风险(标准差)', '最大回撤', '下行风险', 'VaR(95%)', 'CVaR(95%)']]\n# 对数据进行标准化，使不同指标可比\nrisk_metrics_normalized = (risk_metrics - risk_metrics.min()) / (risk_metrics.max() - risk_metrics.min())\nsns.heatmap(risk_metrics_normalized.T, annot=risk_metrics.T, fmt='.2f', \n           cmap='YlOrRd', linewidths=0.5, cbar_kws={'label': '归一化风险分数'})\nplt.title('股票风险指标比较', fontsize=15)\nplt.tight_layout()\nplt.show()\n\n# 保存数据以便后续分析\nrisk_summary.to_csv('risk_return_analysis.csv')\ncov_matrix.to_csv('covariance_matrix.csv')\n\nprint(\"\n风险和收益分析完成，数据已保存至CSV文件\")"
        },
        {
          "id": 3,
          "title": "相关性分析",
          "description": "分析股票间的相关性",
          "example_code": "# 投资组合优化 - 相关性分析\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport yfinance as yf\nimport seaborn as sns\nfrom datetime import datetime, timedelta\nfrom scipy.cluster import hierarchy\nfrom scipy.spatial import distance\n\n# 加载之前保存的数据，或者重新获取\n# 此处假设已有之前步骤的数据\n# 如需重新获取数据，可执行以下代码：\n\n# 设置时间范围 - 获取5年数据\nend_date = datetime.now()\nstart_date = end_date - timedelta(days=365*5)\n\n# 定义要分析的股票列表 - 选择不同行业的代表性股票\ntickers = ['AAPL', 'MSFT', 'AMZN', 'GOOGL', 'BRK-B', 'JPM', 'JNJ', 'PG', 'XOM', 'HD']\nticker_names = {\n    'AAPL': '苹果', 'MSFT': '微软', 'AMZN': '亚马逊', 'GOOGL': '谷歌',\n    'BRK-B': '伯克希尔', 'JPM': '摩根大通', 'JNJ': '强生', 'PG': '宝洁',\n    'XOM': '埃克森美孚', 'HD': '家得宝'\n}\n\nprint(\"获取股票数据...\")\n# 获取股票数据\nprices = yf.download(tickers, start=start_date, end=end_date)['Adj Close']\n# 处理缺失值\nprices = prices.fillna(method='ffill')\n# 计算日收益率\nreturns = prices.pct_change().dropna()\n\nprint(\"开始相关性分析...\")\n\n# 1. 计算相关系数矩阵\ncorrelation_matrix = returns.corr()\n\nprint(\"\n收益率相关系数矩阵：\")\nprint(correlation_matrix)\n\n# 2. 计算协方差矩阵\ncovariance_matrix = returns.cov()\n\n# 3. 可视化相关性矩阵\nplt.figure(figsize=(12, 10))\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5, fmt='.2f', \n           cbar_kws={'label': '相关系数'})\nplt.title('股票收益率相关性热图', fontsize=15)\nplt.tight_layout()\nplt.show()\n\n# 4. 计算滚动相关性\ndef calculate_rolling_correlation(returns, ticker1, ticker2, window=60):\n    # 计算两只股票的滚动相关性\n    return returns[ticker1].rolling(window=window).corr(returns[ticker2])\n\n# 选择几对有代表性的股票进行滚动相关性分析\npairs = [\n    ('AAPL', 'MSFT'),  # 同行业：科技\n    ('AAPL', 'JNJ'),   # 跨行业：科技 vs 医疗\n    ('JPM', 'XOM'),    # 跨行业：金融 vs 能源\n    ('PG', 'JNJ')      # 相关行业：消费品 vs 医疗\n]\n\nplt.figure(figsize=(14, 10))\n\nfor i, (ticker1, ticker2) in enumerate(pairs):\n    plt.subplot(2, 2, i+1)\n    \n    # 计算滚动相关性 (60个交易日，约3个月)\n    rolling_corr = calculate_rolling_correlation(returns, ticker1, ticker2, window=60)\n    \n    # 绘制滚动相关性\n    rolling_corr.plot()\n    plt.title(f'{ticker_names.get(ticker1, ticker1)} vs {ticker_names.get(ticker2, ticker2)} 滚动相关性', \n             fontsize=12)\n    plt.axhline(y=correlation_matrix.loc[ticker1, ticker2], color='r', \n               linestyle='--', alpha=0.3, \n               label=f'全期相关性: {correlation_matrix.loc[ticker1, ticker2]:.2f}')\n    plt.ylim(-1, 1)\n    plt.legend()\n    plt.grid(True)\n\nplt.tight_layout()\nplt.show()\n\n# 5. 分析相关性稳定性\n# 将数据拆分为多个时间段，计算各时间段的相关性矩阵\nperiods = {\n    '完整期间': (returns.index[0], returns.index[-1]),\n    '前半期': (returns.index[0], returns.index[len(returns)//2]),\n    '后半期': (returns.index[len(returns)//2], returns.index[-1]),\n    '近一年': (returns.index[-252], returns.index[-1])\n}\n\n# 计算各期间的相关性矩阵\nperiod_corr = {}\nfor period_name, (start_date, end_date) in periods.items():\n    period_data = returns[(returns.index >= start_date) & (returns.index <= end_date)]\n    period_corr[period_name] = period_data.corr()\n\n# 计算相关性稳定性（不同时间段相关系数的标准差）\ncorr_stability = pd.DataFrame(index=tickers, columns=tickers)\nfor i in tickers:\n    for j in tickers:\n        # 获取不同时间段的相关系数\n        corr_values = [matrix.loc[i, j] for matrix in period_corr.values()]\n        # 计算相关系数的标准差\n        corr_stability.loc[i, j] = np.std(corr_values)\n\nprint(\"\n相关性稳定性矩阵（标准差）：\")\nprint(corr_stability)\n\n# 可视化相关性稳定性\nplt.figure(figsize=(12, 10))\nsns.heatmap(corr_stability, annot=True, cmap='YlGnBu', linewidths=0.5, fmt='.3f', \n           cbar_kws={'label': '相关系数标准差'})\nplt.title('相关性稳定性分析 - 不同时期相关系数的标准差', fontsize=15)\nplt.tight_layout()\nplt.show()\n\n# 6. 层次聚类分析 (Hierarchical Clustering)\n# 基于相关性距离进行股票聚类\n# 将相关性矩阵转换为距离矩阵\ncorr_distance = 1 - correlation_matrix.abs()\n\n# 执行层次聚类\nlinkage = hierarchy.linkage(distance.squareform(corr_distance), method='ward')\n\n# 绘制树状图\nplt.figure(figsize=(14, 8))\nhierarchy.dendrogram(linkage, labels=correlation_matrix.columns, \n                   leaf_rotation=90, leaf_font_size=12)\nplt.title('基于相关性的股票层次聚类', fontsize=15)\nplt.xlabel('股票')\nplt.ylabel('聚类距离')\nplt.axhline(y=0.5, color='r', linestyle='--', alpha=0.3)  # 可能的聚类划分线\nplt.tight_layout()\nplt.show()\n\n# 7. 主成分分析 (PCA)\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n\n# 标准化收益率数据\nscaler = StandardScaler()\nscaled_returns = scaler.fit_transform(returns)\n\n# 执行PCA\npca = PCA()\npca.fit(scaled_returns)\n\n# 计算主成分贡献率\nexplained_variance_ratio = pca.explained_variance_ratio_\ncumulative_variance_ratio = np.cumsum(explained_variance_ratio)\n\n# 绘制碎石图\nplt.figure(figsize=(10, 6))\nplt.bar(range(1, len(explained_variance_ratio) + 1), explained_variance_ratio, alpha=0.6, \n       label='单个方差贡献率')\nplt.step(range(1, len(cumulative_variance_ratio) + 1), cumulative_variance_ratio, where='mid', \n        label='累计方差贡献率')\nplt.axhline(y=0.9, color='r', linestyle='--', alpha=0.5, label='90% 方差贡献率')\nplt.title('主成分分析 - 方差贡献率', fontsize=15)\nplt.xlabel('主成分')\nplt.ylabel('方差贡献率')\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n# 查看第一主成分和第二主成分的载荷\nloadings = pd.DataFrame(pca.components_.T[:, :2], index=tickers, \n                      columns=['第一主成分', '第二主成分'])\nprint(\"\n主成分载荷：\")\nprint(loadings)\n\n# 可视化主成分载荷\nplt.figure(figsize=(10, 8))\nplt.scatter(loadings['第一主成分'], loadings['第二主成分'], s=200, alpha=0.6)\n\n# 添加股票名称标签\nfor i, ticker in enumerate(tickers):\n    plt.annotate(ticker_names.get(ticker, ticker), \n                xy=(loadings['第一主成分'][i], loadings['第二主成分'][i]),\n                xytext=(5, 5), textcoords='offset points',\n                fontsize=12)\n\nplt.axhline(y=0, color='k', linestyle='-', alpha=0.3)\nplt.axvline(x=0, color='k', linestyle='-', alpha=0.3)\nplt.title('主成分分析 - 股票在前两个主成分上的分布', fontsize=15)\nplt.xlabel('第一主成分')\nplt.ylabel('第二主成分')\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n# 保存相关性数据以供后续使用\ncorrelation_matrix.to_csv('stock_correlation.csv')\ncorr_stability.to_csv('correlation_stability.csv')\n\nprint(\"\n相关性分析完成，相关数据已保存至CSV文件\")"
        },
        {
          "id": 4,
          "title": "效率前沿构建",
          "description": "构建投资组合的效率前沿",
          "example_code": "# 投资组合优化 - 效率前沿构建\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport yfinance as yf\nimport seaborn as sns\nfrom datetime import datetime, timedelta\nfrom scipy.optimize import minimize\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# 加载之前保存的数据，或者重新获取\n# 此处假设已有之前步骤的数据\n# 如需重新获取数据，可执行以下代码：\n\n# 设置时间范围 - 获取5年数据\nend_date = datetime.now()\nstart_date = end_date - timedelta(days=365*5)\n\n# 定义要分析的股票列表 - 选择不同行业的代表性股票\ntickers = ['AAPL', 'MSFT', 'AMZN', 'GOOGL', 'BRK-B', 'JPM', 'JNJ', 'PG', 'XOM', 'HD']\nticker_names = {\n    'AAPL': '苹果', 'MSFT': '微软', 'AMZN': '亚马逊', 'GOOGL': '谷歌',\n    'BRK-B': '伯克希尔', 'JPM': '摩根大通', 'JNJ': '强生', 'PG': '宝洁',\n    'XOM': '埃克森美孚', 'HD': '家得宝'\n}\n\nprint(\"获取股票数据...\")\n# 获取股票数据\nprices = yf.download(tickers, start=start_date, end=end_date)['Adj Close']\n# 处理缺失值\nprices = prices.fillna(method='ffill')\n# 计算日收益率\nreturns = prices.pct_change().dropna()\n\n# 计算平均收益率和协方差矩阵\nmean_returns = returns.mean()\ncov_matrix = returns.cov()\n\n# 年化\ntrading_days = 252\nmean_returns_annual = mean_returns * trading_days\ncov_matrix_annual = cov_matrix * trading_days\n\nprint(\"开始构建效率前沿...\")\n\n# 1. 定义投资组合优化函数\n\n# 计算投资组合预期收益\ndef portfolio_return(weights, mean_returns):\n    return np.sum(mean_returns * weights)\n\n# 计算投资组合风险（标准差）\ndef portfolio_volatility(weights, cov_matrix):\n    return np.sqrt(np.dot(weights.T, np.dot(cov_matrix, weights)))\n\n# 计算投资组合夏普比率\ndef portfolio_sharpe_ratio(weights, mean_returns, cov_matrix, risk_free_rate=0.02):\n    return (portfolio_return(weights, mean_returns) - risk_free_rate) / portfolio_volatility(weights, cov_matrix)\n\n# 最小化负夏普比率的目标函数（相当于最大化夏普比率）\ndef neg_sharpe_ratio(weights, mean_returns, cov_matrix, risk_free_rate=0.02):\n    return -portfolio_sharpe_ratio(weights, mean_returns, cov_matrix, risk_free_rate)\n\n# 目标函数：最小化投资组合风险\ndef minimize_volatility(weights, cov_matrix):\n    return portfolio_volatility(weights, cov_matrix)\n\n# 2. 生成随机投资组合\ndef generate_random_portfolios(num_portfolios, mean_returns, cov_matrix, risk_free_rate=0.02):\n    n_assets = len(mean_returns)\n    results = np.zeros((3, num_portfolios))\n    weights_record = []\n    \n    for i in range(num_portfolios):\n        # 生成随机权重\n        weights = np.random.random(n_assets)\n        weights = weights / np.sum(weights)  # 归一化\n        weights_record.append(weights)\n        \n        # 计算投资组合收益\n        portfolio_ret = portfolio_return(weights, mean_returns)\n        # 计算投资组合波动性\n        portfolio_vol = portfolio_volatility(weights, cov_matrix)\n        # 计算投资组合夏普比率\n        portfolio_sharpe = (portfolio_ret - risk_free_rate) / portfolio_vol\n        \n        # 记录结果\n        results[0, i] = portfolio_ret\n        results[1, i] = portfolio_vol\n        results[2, i] = portfolio_sharpe\n    \n    # 转换为DataFrame\n    columns = ['收益率', '波动率', '夏普比率']\n    portfolios = pd.DataFrame(results.T, columns=columns)\n    \n    return portfolios, weights_record\n\n# 3. 获取最优投资组合（最大夏普比率组合和最小方差组合）\ndef optimal_portfolio(mean_returns, cov_matrix, risk_free_rate=0.02):\n    num_assets = len(mean_returns)\n    args = (mean_returns, cov_matrix, risk_free_rate)\n    constraints = ({'type': 'eq', 'fun': lambda x: np.sum(x) - 1})\n    bounds = tuple((0, 1) for asset in range(num_assets))\n    \n    # 最大夏普比率组合\n    max_sharpe_result = minimize(neg_sharpe_ratio, num_assets * [1./num_assets], args=args,\n                               method='SLSQP', bounds=bounds, constraints=constraints)\n    max_sharpe_weights = max_sharpe_result['x']\n    \n    # 最小方差组合\n    min_vol_args = (cov_matrix)\n    min_vol_result = minimize(minimize_volatility, num_assets * [1./num_assets], args=min_vol_args,\n                           method='SLSQP', bounds=bounds, constraints=constraints)\n    min_vol_weights = min_vol_result['x']\n    \n    # 计算最大夏普比率组合的收益和风险\n    max_sharpe_returns = portfolio_return(max_sharpe_weights, mean_returns)\n    max_sharpe_volatility = portfolio_volatility(max_sharpe_weights, cov_matrix)\n    max_sharpe_ratio = (max_sharpe_returns - risk_free_rate) / max_sharpe_volatility\n    \n    # 计算最小方差组合的收益和风险\n    min_vol_returns = portfolio_return(min_vol_weights, mean_returns)\n    min_vol_volatility = portfolio_volatility(min_vol_weights, cov_matrix)\n    min_vol_ratio = (min_vol_returns - risk_free_rate) / min_vol_volatility\n    \n    # 构建结果字典\n    max_sharpe_portfolio = {\n        'Return': max_sharpe_returns,\n        'Volatility': max_sharpe_volatility,\n        'Sharpe Ratio': max_sharpe_ratio,\n        'Weights': max_sharpe_weights\n    }\n    \n    min_vol_portfolio = {\n        'Return': min_vol_returns,\n        'Volatility': min_vol_volatility,\n        'Sharpe Ratio': min_vol_ratio,\n        'Weights': min_vol_weights\n    }\n    \n    return max_sharpe_portfolio, min_vol_portfolio\n\n# 4. 计算给定目标收益率的最小方差组合\ndef efficient_return(mean_returns, cov_matrix, target_return):\n    num_assets = len(mean_returns)\n    args = (cov_matrix)\n    \n    def portfolio_return_constraint(weights):\n        return portfolio_return(weights, mean_returns) - target_return\n    \n    constraints = (\n        {'type': 'eq', 'fun': lambda x: np.sum(x) - 1},\n        {'type': 'eq', 'fun': portfolio_return_constraint}\n    )\n    bounds = tuple((0, 1) for asset in range(num_assets))\n    \n    result = minimize(minimize_volatility, num_assets * [1./num_assets], args=args,\n                   method='SLSQP', bounds=bounds, constraints=constraints)\n    return result['x']\n\n# 5. 生成效率前沿\ndef efficient_frontier(mean_returns, cov_matrix, returns_range):\n    efficients = []\n    for ret in returns_range:\n        weights = efficient_return(mean_returns, cov_matrix, ret)\n        efficients.append(weights)\n    return efficients\n\n# 生成随机投资组合\nnum_portfolios = 10000\nprint(f\"生成 {num_portfolios} 个随机投资组合...\")\nportfolios, weights_record = generate_random_portfolios(num_portfolios, mean_returns_annual, cov_matrix_annual)\n\n# 计算最优投资组合\nprint(\"计算最优投资组合...\")\nmax_sharpe_portfolio, min_vol_portfolio = optimal_portfolio(mean_returns_annual, cov_matrix_annual)\n\n# 计算效率前沿\nprint(\"生成效率前沿...\")\n# 设定收益率范围\ntarget_returns = np.linspace(min_vol_portfolio['Return'], max(mean_returns_annual) * 0.8, 50)\nefficient_weights = efficient_frontier(mean_returns_annual, cov_matrix_annual, target_returns)\n\n# 计算每个效率前沿投资组合的波动率\nefficient_volatilities = []\nfor weights in efficient_weights:\n    efficient_volatilities.append(portfolio_volatility(weights, cov_matrix_annual))\n\n# 可视化结果\n# 1. 随机投资组合和效率前沿\nplt.figure(figsize=(14, 8))\n\n# 绘制随机投资组合\nplt.scatter(portfolios['波动率'], portfolios['收益率'], \n           c=portfolios['夏普比率'], cmap='viridis', \n           alpha=0.5, s=30, marker='o', label='随机投资组合')\nplt.colorbar(label='夏普比率')\n\n# 绘制效率前沿\nplt.plot(efficient_volatilities, target_returns, 'r-', linewidth=3, label='效率前沿')\n\n# 绘制最大夏普比率组合\nplt.scatter(max_sharpe_portfolio['Volatility'], max_sharpe_portfolio['Return'], \n           marker='*', color='red', s=300, label='最大夏普比率组合')\n\n# 绘制最小方差组合\nplt.scatter(min_vol_portfolio['Volatility'], min_vol_portfolio['Return'], \n           marker='X', color='green', s=300, label='最小方差组合')\n\n# 绘制各个单一资产\nfor i, ticker in enumerate(mean_returns.index):\n    plt.scatter(np.sqrt(cov_matrix_annual.iloc[i, i]), mean_returns_annual[i], \n               marker='o', s=150, color='black', alpha=0.8)\n    plt.annotate(ticker_names.get(ticker, ticker), \n                xy=(np.sqrt(cov_matrix_annual.iloc[i, i]), mean_returns_annual[i]),\n                xytext=(5, 5), textcoords='offset points',\n                fontsize=12)\n\nplt.title('投资组合优化 - 效率前沿', fontsize=16)\nplt.xlabel('年化波动率 (风险)', fontsize=14)\nplt.ylabel('年化收益率', fontsize=14)\nplt.legend(fontsize=12)\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n# 2. 查看最优投资组合权重分配\nprint(\"\n最大夏普比率投资组合:\")\nmax_sharpe_df = pd.DataFrame({\n    '股票': [ticker_names.get(ticker, ticker) for ticker in tickers],\n    '分配比例': max_sharpe_portfolio['Weights'] * 100\n})\nmax_sharpe_df = max_sharpe_df.sort_values('分配比例', ascending=False)\nprint(max_sharpe_df)\nprint(f\"预期年化收益率: {max_sharpe_portfolio['Return']*100:.2f}%\")\nprint(f\"预期年化波动率: {max_sharpe_portfolio['Volatility']*100:.2f}%\")\nprint(f\"夏普比率: {max_sharpe_portfolio['Sharpe Ratio']:.4f}\")\n\nprint(\"\n最小方差投资组合:\")\nmin_vol_df = pd.DataFrame({\n    '股票': [ticker_names.get(ticker, ticker) for ticker in tickers],\n    '分配比例': min_vol_portfolio['Weights'] * 100\n})\nmin_vol_df = min_vol_df.sort_values('分配比例', ascending=False)\nprint(min_vol_df)\nprint(f\"预期年化收益率: {min_vol_portfolio['Return']*100:.2f}%\")\nprint(f\"预期年化波动率: {min_vol_portfolio['Volatility']*100:.2f}%\")\nprint(f\"夏普比率: {min_vol_portfolio['Sharpe Ratio']:.4f}\")\n\n# 3. 可视化最优投资组合权重\nplt.figure(figsize=(18, 8))\n\n# 最大夏普比率组合权重\nplt.subplot(1, 2, 1)\nplt.pie(max_sharpe_df['分配比例'], labels=max_sharpe_df['股票'], autopct='%1.1f%%', startangle=90, \n       wedgeprops={'edgecolor': 'white', 'linewidth': 1.5}, textprops={'fontsize': 12})\nplt.title('最大夏普比率投资组合权重分配', fontsize=16)\n\n# 最小方差组合权重\nplt.subplot(1, 2, 2)\nplt.pie(min_vol_df['分配比例'], labels=min_vol_df['股票'], autopct='%1.1f%%', startangle=90, \n       wedgeprops={'edgecolor': 'white', 'linewidth': 1.5}, textprops={'fontsize': 12})\nplt.title('最小方差投资组合权重分配', fontsize=16)\n\nplt.tight_layout()\nplt.show()\n\n# 保存结果\n# 保存最优组合权重\nmax_sharpe_df.to_csv('max_sharpe_portfolio.csv', index=False)\nmin_vol_df.to_csv('min_variance_portfolio.csv', index=False)\n\n# 保存效率前沿数据\nef_data = pd.DataFrame({\n    '年化波动率': efficient_volatilities,\n    '年化收益率': target_returns\n})\nef_data.to_csv('efficient_frontier.csv', index=False)\n\nprint(\"\n效率前沿构建完成，结果已保存至CSV文件\")"
        },
        {
          "id": 5,
          "title": "最优组合确定",
          "description": "确定最优夏普比率的投资组合",
          "example_code": "# 投资组合优化 - 最优组合确定\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport yfinance as yf\nimport seaborn as sns\nfrom datetime import datetime, timedelta\nfrom scipy.optimize import minimize\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# 加载之前保存的数据，或者重新获取\n# 此处假设已有之前步骤的数据\n# 如需重新获取数据，可执行以下代码：\n\n# 设置时间范围 - 获取5年数据\nend_date = datetime.now()\nstart_date = end_date - timedelta(days=365*5)\n\n# 定义要分析的股票列表 - 选择不同行业的代表性股票\ntickers = ['AAPL', 'MSFT', 'AMZN', 'GOOGL', 'BRK-B', 'JPM', 'JNJ', 'PG', 'XOM', 'HD']\nticker_names = {\n    'AAPL': '苹果', 'MSFT': '微软', 'AMZN': '亚马逊', 'GOOGL': '谷歌',\n    'BRK-B': '伯克希尔', 'JPM': '摩根大通', 'JNJ': '强生', 'PG': '宝洁',\n    'XOM': '埃克森美孚', 'HD': '家得宝'\n}\n\nprint(\"获取股票数据...\")\n# 获取股票数据\nprices = yf.download(tickers, start=start_date, end=end_date)['Adj Close']\n# 处理缺失值\nprices = prices.fillna(method='ffill')\n# 计算日收益率\nreturns = prices.pct_change().dropna()\n\n# 计算平均收益率和协方差矩阵\nmean_returns = returns.mean()\ncov_matrix = returns.cov()\n\n# 年化\ntrading_days = 252\nmean_returns_annual = mean_returns * trading_days\ncov_matrix_annual = cov_matrix * trading_days\n\nprint(\"开始确定最优投资组合...\")\n\n# 1. 投资组合优化工具函数\n# 计算投资组合预期收益\ndef portfolio_return(weights, mean_returns):\n    return np.sum(mean_returns * weights)\n\n# 计算投资组合风险（标准差）\ndef portfolio_volatility(weights, cov_matrix):\n    return np.sqrt(np.dot(weights.T, np.dot(cov_matrix, weights)))\n\n# 计算投资组合夏普比率\ndef portfolio_sharpe_ratio(weights, mean_returns, cov_matrix, risk_free_rate=0.02):\n    return (portfolio_return(weights, mean_returns) - risk_free_rate) / portfolio_volatility(weights, cov_matrix)\n\n# 最小化负夏普比率的目标函数（相当于最大化夏普比率）\ndef neg_sharpe_ratio(weights, mean_returns, cov_matrix, risk_free_rate=0.02):\n    return -portfolio_sharpe_ratio(weights, mean_returns, cov_matrix, risk_free_rate)\n\n# 目标函数：最小化投资组合风险\ndef minimize_volatility(weights, cov_matrix):\n    return portfolio_volatility(weights, cov_matrix)\n\n# 2. 最优化求解器 - 基础版本\ndef optimize_portfolio(objective_function, args, n_assets, constraints=None, bounds=None, initial_weights=None):\n    if initial_weights is None:\n        initial_weights = np.array([1.0 / n_assets] * n_assets)\n    \n    if constraints is None:\n        constraints = ({'type': 'eq', 'fun': lambda x: np.sum(x) - 1})\n    \n    if bounds is None:\n        bounds = tuple((0, 1) for _ in range(n_assets))\n    \n    result = minimize(objective_function, initial_weights, args=args,\n                    method='SLSQP', bounds=bounds, constraints=constraints)\n    \n    return result\n\n# 3. 优化不同目标的投资组合\n# 3.1 最大夏普比率投资组合\ndef max_sharpe_ratio_portfolio(mean_returns, cov_matrix, risk_free_rate=0.02):\n    n_assets = len(mean_returns)\n    args = (mean_returns, cov_matrix, risk_free_rate)\n    constraints = ({'type': 'eq', 'fun': lambda x: np.sum(x) - 1})\n    bounds = tuple((0, 1) for _ in range(n_assets))\n    \n    result = optimize_portfolio(neg_sharpe_ratio, args, n_assets, constraints, bounds)\n    \n    weights = result['x']\n    ret = portfolio_return(weights, mean_returns)\n    vol = portfolio_volatility(weights, cov_matrix)\n    sharpe = portfolio_sharpe_ratio(weights, mean_returns, cov_matrix, risk_free_rate)\n    \n    return {\n        'weights': weights,\n        'return': ret,\n        'volatility': vol,\n        'sharpe_ratio': sharpe\n    }\n\n# 3.2 最小方差投资组合\ndef min_variance_portfolio(mean_returns, cov_matrix):\n    n_assets = len(mean_returns)\n    args = (cov_matrix,)\n    constraints = ({'type': 'eq', 'fun': lambda x: np.sum(x) - 1})\n    bounds = tuple((0, 1) for _ in range(n_assets))\n    \n    result = optimize_portfolio(minimize_volatility, args, n_assets, constraints, bounds)\n    \n    weights = result['x']\n    ret = portfolio_return(weights, mean_returns)\n    vol = portfolio_volatility(weights, cov_matrix)\n    sharpe = (ret - 0.02) / vol  # 使用0.02作为无风险收益率\n    \n    return {\n        'weights': weights,\n        'return': ret,\n        'volatility': vol,\n        'sharpe_ratio': sharpe\n    }\n\n# 3.3 指定目标收益的最小方差投资组合\ndef min_variance_for_target_return(mean_returns, cov_matrix, target_return):\n    n_assets = len(mean_returns)\n    args = (cov_matrix,)\n    \n    # 增加收益率约束\n    constraints = (\n        {'type': 'eq', 'fun': lambda x: np.sum(x) - 1},\n        {'type': 'eq', 'fun': lambda x: portfolio_return(x, mean_returns) - target_return}\n    )\n    \n    bounds = tuple((0, 1) for _ in range(n_assets))\n    \n    result = optimize_portfolio(minimize_volatility, args, n_assets, constraints, bounds)\n    \n    weights = result['x']\n    ret = portfolio_return(weights, mean_returns)\n    vol = portfolio_volatility(weights, cov_matrix)\n    sharpe = (ret - 0.02) / vol  # 使用0.02作为无风险收益率\n    \n    return {\n        'weights': weights,\n        'return': ret,\n        'volatility': vol,\n        'sharpe_ratio': sharpe\n    }\n\n# 4. 考虑投资约束的高级优化\n# 4.1 带风险预算的最优投资组合\ndef risk_budget_portfolio(cov_matrix, risk_budget):\n    n_assets = len(cov_matrix)\n    init_weights = np.array([1.0 / n_assets] * n_assets)\n    \n    # 风险贡献目标函数\n    def risk_contribution_objective(weights, cov_matrix, risk_budget):\n        # 计算总风险\n        port_vol = portfolio_volatility(weights, cov_matrix)\n        # 计算每个资产的风险贡献\n        marginal_contrib = np.dot(cov_matrix, weights)\n        risk_contrib = np.multiply(marginal_contrib, weights) / port_vol\n        # 计算风险贡献与目标风险预算之间的差异\n        diff = risk_contrib - risk_budget\n        return np.sum(np.square(diff))\n    \n    args = (cov_matrix, risk_budget)\n    constraints = ({'type': 'eq', 'fun': lambda x: np.sum(x) - 1})\n    bounds = tuple((0.01, 1) for _ in range(n_assets))  # 确保每个资产至少有一些权重\n    \n    result = optimize_portfolio(risk_contribution_objective, args, n_assets, constraints, bounds, init_weights)\n    \n    weights = result['x']\n    ret = portfolio_return(weights, mean_returns_annual)\n    vol = portfolio_volatility(weights, cov_matrix_annual)\n    sharpe = (ret - 0.02) / vol  # 使用0.02作为无风险收益率\n    \n    return {\n        'weights': weights,\n        'return': ret,\n        'volatility': vol,\n        'sharpe_ratio': sharpe\n    }\n\n# 4.2 带行业约束的最优投资组合\n# 假设我们有行业分类信息\nsectors = {\n    'AAPL': 'Technology',\n    'MSFT': 'Technology',\n    'AMZN': 'Consumer Cyclical',\n    'GOOGL': 'Communication Services',\n    'BRK-B': 'Financial Services',\n    'JPM': 'Financial Services',\n    'JNJ': 'Healthcare',\n    'PG': 'Consumer Defensive',\n    'XOM': 'Energy',\n    'HD': 'Consumer Cyclical'\n}\n\ndef sector_constrained_portfolio(mean_returns, cov_matrix, sectors, max_sector_weight=0.4):\n    n_assets = len(mean_returns)\n    args = (mean_returns, cov_matrix, 0.02)  # 最后一个参数是无风险收益率\n    \n    # 基本约束条件：权重之和为1\n    constraints = [{'type': 'eq', 'fun': lambda x: np.sum(x) - 1}]\n    \n    # 添加行业约束\n    unique_sectors = list(set(sectors.values()))\n    for sector in unique_sectors:\n        sector_indices = [i for i, ticker in enumerate(sectors.keys()) if sectors[ticker] == sector]\n        \n        # 创建行业权重限制函数\n        def sector_weight_constraint(weights, indices=sector_indices, max_weight=max_sector_weight):\n            return max_weight - sum(weights[i] for i in indices)\n        \n        constraints.append({'type': 'ineq', 'fun': sector_weight_constraint})\n    \n    bounds = tuple((0, 1) for _ in range(n_assets))\n    \n    result = optimize_portfolio(neg_sharpe_ratio, args, n_assets, constraints, bounds)\n    \n    weights = result['x']\n    ret = portfolio_return(weights, mean_returns)\n    vol = portfolio_volatility(weights, cov_matrix)\n    sharpe = portfolio_sharpe_ratio(weights, mean_returns, cov_matrix, 0.02)\n    \n    return {\n        'weights': weights,\n        'return': ret,\n        'volatility': vol,\n        'sharpe_ratio': sharpe\n    }\n\n# 优化不同约束条件下的投资组合\nprint(\"计算多种优化约束下的最优组合...\")\n\n# 最大夏普比率组合\nmax_sharpe = max_sharpe_ratio_portfolio(mean_returns_annual, cov_matrix_annual)\nprint(\"\n最大夏普比率投资组合:\")\nsharpe_weights_df = pd.DataFrame({\n    '股票': [ticker_names.get(ticker, ticker) for ticker in tickers],\n    '分配比例(%)': max_sharpe['weights'] * 100\n})\nsharpe_weights_df = sharpe_weights_df.sort_values('分配比例(%)', ascending=False)\nprint(sharpe_weights_df)\nprint(f\"预期年化收益率: {max_sharpe['return']*100:.2f}%\")\nprint(f\"预期年化波动率: {max_sharpe['volatility']*100:.2f}%\")\nprint(f\"夏普比率: {max_sharpe['sharpe_ratio']:.4f}\")\n\n# 最小方差组合\nmin_var = min_variance_portfolio(mean_returns_annual, cov_matrix_annual)\nprint(\"\n最小方差投资组合:\")\nmin_var_weights_df = pd.DataFrame({\n    '股票': [ticker_names.get(ticker, ticker) for ticker in tickers],\n    '分配比例(%)': min_var['weights'] * 100\n})\nmin_var_weights_df = min_var_weights_df.sort_values('分配比例(%)', ascending=False)\nprint(min_var_weights_df)\nprint(f\"预期年化收益率: {min_var['return']*100:.2f}%\")\nprint(f\"预期年化波动率: {min_var['volatility']*100:.2f}%\")\nprint(f\"夏普比率: {min_var['sharpe_ratio']:.4f}\")\n\n# 指定目标收益的最小方差组合\ntarget_return = 0.25  # 25%的目标收益率\ntarget_ret_min_var = min_variance_for_target_return(mean_returns_annual, cov_matrix_annual, target_return)\nprint(f\"\n目标收益率{target_return*100}%的最小方差投资组合:\")\ntarget_ret_weights_df = pd.DataFrame({\n    '股票': [ticker_names.get(ticker, ticker) for ticker in tickers],\n    '分配比例(%)': target_ret_min_var['weights'] * 100\n})\ntarget_ret_weights_df = target_ret_weights_df.sort_values('分配比例(%)', ascending=False)\nprint(target_ret_weights_df)\nprint(f\"预期年化收益率: {target_ret_min_var['return']*100:.2f}%\")\nprint(f\"预期年化波动率: {target_ret_min_var['volatility']*100:.2f}%\")\nprint(f\"夏普比率: {target_ret_min_var['sharpe_ratio']:.4f}\")\n\n# 风险平价组合（假设风险预算相等）\nrisk_budget = np.array([1.0 / len(tickers)] * len(tickers))\nrisk_parity = risk_budget_portfolio(cov_matrix_annual, risk_budget)\nprint(\"\n风险平价投资组合:\")\nrisk_parity_weights_df = pd.DataFrame({\n    '股票': [ticker_names.get(ticker, ticker) for ticker in tickers],\n    '分配比例(%)': risk_parity['weights'] * 100\n})\nrisk_parity_weights_df = risk_parity_weights_df.sort_values('分配比例(%)', ascending=False)\nprint(risk_parity_weights_df)\nprint(f\"预期年化收益率: {risk_parity['return']*100:.2f}%\")\nprint(f\"预期年化波动率: {risk_parity['volatility']*100:.2f}%\")\nprint(f\"夏普比率: {risk_parity['sharpe_ratio']:.4f}\")\n\n# 行业约束组合\nsector_constrained = sector_constrained_portfolio(mean_returns_annual, cov_matrix_annual, sectors, max_sector_weight=0.4)\nprint(\"\n行业约束投资组合 (每个行业最大权重40%):\")\nsector_constrained_weights_df = pd.DataFrame({\n    '股票': [ticker_names.get(ticker, ticker) for ticker in tickers],\n    '分配比例(%)': sector_constrained['weights'] * 100,\n    '行业': [sectors[ticker] for ticker in tickers]\n})\nsector_constrained_weights_df = sector_constrained_weights_df.sort_values('分配比例(%)', ascending=False)\nprint(sector_constrained_weights_df)\nprint(f\"预期年化收益率: {sector_constrained['return']*100:.2f}%\")\nprint(f\"预期年化波动率: {sector_constrained['volatility']*100:.2f}%\")\nprint(f\"夏普比率: {sector_constrained['sharpe_ratio']:.4f}\")\n\n# 可视化比较不同优化策略\nstrategies = ['最大夏普比率', '最小方差', f'目标收益{target_return*100}%', '风险平价', '行业约束']\nreturns = [max_sharpe['return'], min_var['return'], target_ret_min_var['return'], \n          risk_parity['return'], sector_constrained['return']]\nvolatilities = [max_sharpe['volatility'], min_var['volatility'], target_ret_min_var['volatility'], \n               risk_parity['volatility'], sector_constrained['volatility']]\nsharpes = [max_sharpe['sharpe_ratio'], min_var['sharpe_ratio'], target_ret_min_var['sharpe_ratio'], \n          risk_parity['sharpe_ratio'], sector_constrained['sharpe_ratio']]\n\n# 创建比较表格\ncomparison_df = pd.DataFrame({\n    '优化策略': strategies,\n    '年化收益率(%)': [r*100 for r in returns],\n    '年化波动率(%)': [v*100 for v in volatilities],\n    '夏普比率': sharpes\n})\n\nprint(\"\n不同优化策略比较:\")\nprint(comparison_df)\n\n# 可视化结果\n# 1. 风险-收益散点图\nplt.figure(figsize=(12, 8))\nplt.scatter(comparison_df['年化波动率(%)'], comparison_df['年化收益率(%)'], \n           s=300, c=comparison_df['夏普比率'], cmap='viridis', alpha=0.7)\n\n# 添加标签\nfor i, strategy in enumerate(strategies):\n    plt.annotate(strategy, \n                xy=(comparison_df['年化波动率(%)'][i], comparison_df['年化收益率(%)'][i]),\n                xytext=(5, 5), textcoords='offset points',\n                fontsize=12)\n\nplt.colorbar(label='夏普比率')\nplt.title('不同优化策略的风险-收益特征', fontsize=16)\nplt.xlabel('年化波动率 (%)', fontsize=14)\nplt.ylabel('年化收益率 (%)', fontsize=14)\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n# 2. 投资组合权重比较\n# 准备数据\nweights_data = pd.concat([\n    pd.Series(max_sharpe['weights'], index=tickers, name='最大夏普比率'),\n    pd.Series(min_var['weights'], index=tickers, name='最小方差'),\n    pd.Series(target_ret_min_var['weights'], index=tickers, name=f'目标收益{target_return*100}%'),\n    pd.Series(risk_parity['weights'], index=tickers, name='风险平价'),\n    pd.Series(sector_constrained['weights'], index=tickers, name='行业约束')\n], axis=1)\n\n# 用中文股票名称替换代码\nweights_data.index = [ticker_names.get(ticker, ticker) for ticker in tickers]\n\n# 可视化权重分配\nplt.figure(figsize=(15, 10))\nweights_data.plot(kind='bar', stacked=False)\nplt.title('不同优化策略的投资组合权重分配', fontsize=16)\nplt.xlabel('股票', fontsize=14)\nplt.ylabel('权重', fontsize=14)\nplt.xticks(rotation=45)\nplt.legend(title='优化策略')\nplt.tight_layout()\nplt.show()\n\n# 3. 各策略风险贡献饼图\ndef calculate_risk_contribution(weights, cov_matrix):\n    portfolio_vol = portfolio_volatility(weights, cov_matrix)\n    # 计算边际贡献\n    marginal_contrib = np.dot(cov_matrix, weights)\n    # 计算风险贡献\n    risk_contrib = np.multiply(marginal_contrib, weights) / portfolio_vol\n    return risk_contrib\n\nfig, axes = plt.subplots(2, 3, figsize=(20, 12))\naxes = axes.flatten()\n\n# 从中文名称映射回股票代码\nticker_codes = {v: k for k, v in ticker_names.items()}\nchinese_names = [ticker_names.get(ticker, ticker) for ticker in tickers]\n\nfor i, (strategy, weights) in enumerate(zip(strategies, \n                                          [max_sharpe['weights'], min_var['weights'], \n                                           target_ret_min_var['weights'], risk_parity['weights'], \n                                           sector_constrained['weights']])):\n    # 计算风险贡献\n    risk_contrib = calculate_risk_contribution(weights, cov_matrix_annual)\n    risk_contrib_pct = risk_contrib / np.sum(risk_contrib) * 100\n    \n    # 创建饼图\n    axes[i].pie(risk_contrib_pct, labels=chinese_names, autopct='%1.1f%%', startangle=90, \n              wedgeprops={'edgecolor': 'white', 'linewidth': 1.5})\n    axes[i].set_title(f'{strategy} - 风险贡献分布', fontsize=14)\n\n# 关闭多余的子图\nif len(strategies) < len(axes):\n    for j in range(len(strategies), len(axes)):\n        fig.delaxes(axes[j])\n\nplt.tight_layout()\nplt.show()\n\n# 保存最优组合结果\n# 将比较结果保存为CSV\ncomparison_df.to_csv('portfolio_optimization_comparison.csv', index=False)\n\n# 保存最大夏普比率组合的详细信息\nsharpe_weights_df.to_csv('max_sharpe_portfolio_weights.csv', index=False)\n\nprint(\"\n最优投资组合确定完成，结果已保存至CSV文件\")"
        },
        {
          "id": 6,
          "title": "投资组合评估",
          "description": "评估最优投资组合的表现",
          "example_code": "# 投资组合优化 - 投资组合评估\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport yfinance as yf\nimport seaborn as sns\nfrom datetime import datetime, timedelta\nimport pyfolio as pf\nfrom scipy.stats import norm\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# 加载之前保存的数据，或者重新获取\n# 此处假设已有之前步骤的数据\n# 如需重新获取数据，可执行以下代码：\n\n# 设置时间范围 - 获取5年数据\nend_date = datetime.now()\ntraining_end_date = end_date - timedelta(days=365)  # 使用前4年数据建模，最后1年进行评估\nstart_date = end_date - timedelta(days=365*5)\n\n# 定义要分析的股票列表 - 选择不同行业的代表性股票\ntickers = ['AAPL', 'MSFT', 'AMZN', 'GOOGL', 'BRK-B', 'JPM', 'JNJ', 'PG', 'XOM', 'HD']\nticker_names = {\n    'AAPL': '苹果', 'MSFT': '微软', 'AMZN': '亚马逊', 'GOOGL': '谷歌',\n    'BRK-B': '伯克希尔', 'JPM': '摩根大通', 'JNJ': '强生', 'PG': '宝洁',\n    'XOM': '埃克森美孚', 'HD': '家得宝'\n}\n\nprint(\"获取股票数据...\")\n# 获取股票数据\nprices = yf.download(tickers, start=start_date, end=end_date)['Adj Close']\n# 处理缺失值\nprices = prices.fillna(method='ffill')\n\n# 划分训练集和测试集\ntrain_prices = prices[prices.index <= training_end_date]\ntest_prices = prices[prices.index > training_end_date]\n\n# 计算收益率\ntrain_returns = train_prices.pct_change().dropna()\ntest_returns = test_prices.pct_change().dropna()\n\nprint(\"开始评估投资组合表现...\")\n\n# 1. 计算投资组合权重\n# 使用训练数据计算最优组合\ndef calculate_optimal_portfolio(returns):\n    # 计算最优投资组合权重\n    # 计算平均收益率和协方差矩阵\n    mean_returns = returns.mean()\n    cov_matrix = returns.cov()\n    \n    # 年化\n    trading_days = 252\n    mean_returns_annual = mean_returns * trading_days\n    cov_matrix_annual = cov_matrix * trading_days\n    \n    # 计算投资组合预期收益\n    def portfolio_return(weights, mean_returns):\n        return np.sum(mean_returns * weights)\n    \n    # 计算投资组合风险（标准差）\n    def portfolio_volatility(weights, cov_matrix):\n        return np.sqrt(np.dot(weights.T, np.dot(cov_matrix, weights)))\n    \n    # 计算投资组合夏普比率\n    def portfolio_sharpe_ratio(weights, mean_returns, cov_matrix, risk_free_rate=0.02):\n        return (portfolio_return(weights, mean_returns) - risk_free_rate) / portfolio_volatility(weights, cov_matrix)\n    \n    # 最小化负夏普比率的目标函数\n    def neg_sharpe_ratio(weights, mean_returns, cov_matrix, risk_free_rate=0.02):\n        return -portfolio_sharpe_ratio(weights, mean_returns, cov_matrix, risk_free_rate)\n    \n    # 最大夏普比率优化\n    from scipy.optimize import minimize\n    \n    n_assets = len(mean_returns)\n    args = (mean_returns_annual, cov_matrix_annual, 0.02)\n    constraints = ({'type': 'eq', 'fun': lambda x: np.sum(x) - 1})\n    bounds = tuple((0, 1) for _ in range(n_assets))\n    \n    # 最大夏普比率组合\n    result = minimize(neg_sharpe_ratio, n_assets * [1./n_assets], args=args,\n                    method='SLSQP', bounds=bounds, constraints=constraints)\n    \n    max_sharpe_weights = result['x']\n    return max_sharpe_weights\n\n# 计算最优投资组合权重\nopt_weights = calculate_optimal_portfolio(train_returns)\n\n# 创建权重DataFrame\nweights_df = pd.DataFrame({\n    '股票': tickers,\n    '中文名': [ticker_names.get(ticker, ticker) for ticker in tickers],\n    '权重': opt_weights\n})\nweights_df = weights_df.sort_values('权重', ascending=False)\n\nprint(\"\n最优投资组合权重:\")\nprint(weights_df)\n\n# 2. 使用测试数据评估投资组合表现\ndef calculate_portfolio_returns(returns, weights):\n    # 计算投资组合收益率\n    # 确保权重顺序与收益率列顺序一致\n    ordered_weights = pd.Series(weights, index=returns.columns)\n    portfolio_rets = (returns * ordered_weights).sum(axis=1)\n    return portfolio_rets\n\n# 计算投资组合收益率\nportfolio_returns = calculate_portfolio_returns(test_returns, opt_weights)\n\n# 计算市场基准收益率（使用等权重投资组合作为基准）\nbenchmark_weights = np.array([1/len(tickers)] * len(tickers))\nbenchmark_returns = calculate_portfolio_returns(test_returns, benchmark_weights)\n\n# 3. 投资组合表现评估指标\ndef calculate_performance_metrics(returns, benchmark_returns=None, risk_free_rate=0.02/252):\n    # 计算投资组合表现指标\n    # 年化因子\n    annualization_factor = np.sqrt(252)\n    \n    # 累积收益率\n    cum_returns = (1 + returns).cumprod() - 1\n    total_return = cum_returns.iloc[-1]\n    \n    # 年化收益率\n    annual_return = (1 + total_return) ** (252 / len(returns)) - 1\n    \n    # 年化波动率\n    annual_volatility = returns.std() * annualization_factor\n    \n    # 夏普比率\n    excess_returns = returns - risk_free_rate\n    sharpe_ratio = (excess_returns.mean() / returns.std()) * annualization_factor\n    \n    # 最大回撤\n    wealth_index = (1 + returns).cumprod()\n    previous_peaks = wealth_index.cummax()\n    drawdowns = (wealth_index - previous_peaks) / previous_peaks\n    max_drawdown = drawdowns.min()\n    \n    # 计算α和β（如果有基准）\n    alpha, beta = np.nan, np.nan\n    if benchmark_returns is not None:\n        # 计算β (beta)\n        covariance = np.cov(returns, benchmark_returns)[0, 1]\n        variance = np.var(benchmark_returns)\n        beta = covariance / variance\n        \n        # 计算α (alpha)\n        alpha = (annual_return - risk_free_rate * 252) - beta * (benchmark_returns.mean() * 252 - risk_free_rate * 252)\n        \n        # 计算信息比率\n        tracking_error = (returns - benchmark_returns).std() * annualization_factor\n        information_ratio = (returns.mean() - benchmark_returns.mean()) * 252 / tracking_error\n    else:\n        information_ratio = np.nan\n    \n    # 计算索提诺比率\n    downside_deviation = returns[returns < 0].std() * annualization_factor\n    sortino_ratio = (returns.mean() * 252 - risk_free_rate * 252) / downside_deviation if downside_deviation != 0 else np.nan\n    \n    # 计算卡玛比率（Calmar Ratio）\n    calmar_ratio = annual_return / abs(max_drawdown) if max_drawdown != 0 else np.nan\n    \n    # 计算欧米茄比率（Omega Ratio）\n    threshold = risk_free_rate\n    omega_ratio = len(returns[returns >= threshold]) / len(returns[returns < threshold]) if len(returns[returns < threshold]) > 0 else np.nan\n    \n    # 计算胜率\n    win_rate = len(returns[returns > 0]) / len(returns)\n    \n    # 计算盈亏比\n    avg_win = returns[returns > 0].mean()\n    avg_loss = returns[returns < 0].mean()\n    profit_loss_ratio = abs(avg_win / avg_loss) if avg_loss != 0 else np.nan\n    \n    # VaR和CVaR计算\n    var_95 = np.percentile(returns, 5)  # 95% VaR\n    cvar_95 = returns[returns <= var_95].mean()  # 95% CVaR\n    \n    # 创建结果字典\n    metrics = {\n        '总收益率': total_return,\n        '年化收益率': annual_return,\n        '年化波动率': annual_volatility,\n        '夏普比率': sharpe_ratio,\n        '索提诺比率': sortino_ratio,\n        '最大回撤': max_drawdown,\n        '卡玛比率': calmar_ratio,\n        '欧米茄比率': omega_ratio,\n        'Alpha': alpha,\n        'Beta': beta,\n        '信息比率': information_ratio,\n        '胜率': win_rate,\n        '盈亏比': profit_loss_ratio,\n        'VaR(95%)': var_95,\n        'CVaR(95%)': cvar_95\n    }\n    \n    return metrics, cum_returns\n\n# 计算投资组合和基准的表现指标\nportfolio_metrics, portfolio_cum_returns = calculate_performance_metrics(portfolio_returns, benchmark_returns)\nbenchmark_metrics, benchmark_cum_returns = calculate_performance_metrics(benchmark_returns)\n\n# 创建表现指标DataFrame\nmetrics_df = pd.DataFrame({\n    '指标': list(portfolio_metrics.keys()),\n    '投资组合': list(portfolio_metrics.values()),\n    '基准 (等权重)': list(benchmark_metrics.values())\n})\n\n# 格式化数字\nformat_dict = {\n    '总收益率': '{:.2%}',\n    '年化收益率': '{:.2%}',\n    '年化波动率': '{:.2%}',\n    '夏普比率': '{:.2f}',\n    '索提诺比率': '{:.2f}',\n    '最大回撤': '{:.2%}',\n    '卡玛比率': '{:.2f}',\n    '欧米茄比率': '{:.2f}',\n    'Alpha': '{:.4f}',\n    'Beta': '{:.2f}',\n    '信息比率': '{:.2f}',\n    '胜率': '{:.2%}',\n    '盈亏比': '{:.2f}',\n    'VaR(95%)': '{:.2%}',\n    'CVaR(95%)': '{:.2%}'\n}\n\nfor i, metric in enumerate(metrics_df['指标']):\n    if metric in format_dict:\n        portfolio_value = metrics_df.loc[i, '投资组合']\n        benchmark_value = metrics_df.loc[i, '基准 (等权重)']\n        \n        # 检查是否为数值类型，并格式化\n        if isinstance(portfolio_value, (int, float)) and not np.isnan(portfolio_value):\n            metrics_df.loc[i, '投资组合'] = format_dict[metric].format(portfolio_value)\n        \n        if isinstance(benchmark_value, (int, float)) and not np.isnan(benchmark_value):\n            metrics_df.loc[i, '基准 (等权重)'] = format_dict[metric].format(benchmark_value)\n\nprint(\"\n投资组合表现指标:\")\nprint(metrics_df)\n\n# 4. 可视化投资组合表现\n# 4.1 累积收益率对比\nplt.figure(figsize=(14, 7))\nportfolio_cum_returns.plot(label='最优投资组合', color='blue')\nbenchmark_cum_returns.plot(label='基准 (等权重)', color='red', linestyle='--')\nplt.title('投资组合与基准的累积收益率对比', fontsize=16)\nplt.xlabel('日期', fontsize=14)\nplt.ylabel('累积收益率', fontsize=14)\nplt.grid(True)\nplt.legend(fontsize=12)\nplt.tight_layout()\nplt.show()\n\n# 4.2 波动率聚类分析\nrolling_vol = test_returns.rolling(window=30).std() * np.sqrt(252)\ncorr_matrix = rolling_vol.corr()\n\nplt.figure(figsize=(12, 10))\nsns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\nplt.title('股票波动率相关性矩阵', fontsize=16)\nplt.tight_layout()\nplt.show()\n\n# 4.3 回撤分析\ndef plot_drawdown_periods(returns, top=5):\n    # 绘制最大回撤期\n    # 计算累积收益和回撤\n    wealth_index = (1 + returns).cumprod()\n    previous_peaks = wealth_index.cummax()\n    drawdowns = (wealth_index - previous_peaks) / previous_peaks\n    \n    # 识别回撤期\n    is_drawdown = drawdowns < 0\n    \n    # 创建回撤期数组\n    drawdown_periods = []\n    current_period = None\n    \n    for i, (date, value) in enumerate(drawdowns.items()):\n        if value < 0 and (current_period is None):\n            # 开始新的回撤期\n            current_period = {'start': date, 'value': value}\n        elif value == 0 and current_period is not None:\n            # 结束当前回撤期\n            current_period['end'] = date\n            current_period['min_value'] = drawdowns[current_period['start']:date].min()\n            current_period['length'] = (date - current_period['start']).days\n            drawdown_periods.append(current_period)\n            current_period = None\n    \n    # 处理可能的未结束回撤期\n    if current_period is not None:\n        current_period['end'] = drawdowns.index[-1]\n        current_period['min_value'] = drawdowns[current_period['start']:].min()\n        current_period['length'] = (current_period['end'] - current_period['start']).days\n        drawdown_periods.append(current_period)\n    \n    # 按回撤幅度排序\n    drawdown_periods.sort(key=lambda x: x['min_value'])\n    \n    # 绘制前N大回撤\n    plt.figure(figsize=(14, 7))\n    \n    # 绘制累积收益曲线\n    wealth_index.plot(label='累积收益', color='blue')\n    \n    # 为每个主要回撤期添加阴影\n    for i, period in enumerate(drawdown_periods[:top]):\n        plt.fill_between(\n            drawdowns[period['start']:period['end']].index,\n            wealth_index[period['start']:period['end']],\n            previous_peaks[period['start']:period['end']],\n            color=f'C{i+1}',\n            alpha=0.3,\n            label=f\"回撤 #{i+1}: {period['min_value']:.2%}\"\n        )\n    \n    plt.title('投资组合累积收益与主要回撤期', fontsize=16)\n    plt.xlabel('日期', fontsize=14)\n    plt.ylabel('累积收益', fontsize=14)\n    plt.grid(True)\n    plt.legend(fontsize=12)\n    plt.tight_layout()\n    plt.show()\n    \n    # 打印回撤摘要\n    drawdown_summary = pd.DataFrame([\n        {\n            '开始日期': period['start'].strftime('%Y-%m-%d'),\n            '结束日期': period['end'].strftime('%Y-%m-%d'),\n            '持续天数': period['length'],\n            '最大回撤': f\"{period['min_value']:.2%}\"\n        }\n        for period in drawdown_periods[:top]\n    ])\n    \n    return drawdown_summary\n\n# 绘制回撤分析\ndrawdown_summary = plot_drawdown_periods(portfolio_returns)\nprint(\"\n主要回撤期:\")\nprint(drawdown_summary)\n\n# 4.4 月度收益热图\ndef plot_monthly_returns_heatmap(returns):\n    # 绘制月度收益热图\n    # 转换为月度收益\n    monthly_returns = returns.resample('M').apply(lambda x: (1 + x).prod() - 1)\n    \n    # 创建透视表格，行为年份，列为月份\n    monthly_returns.index = monthly_returns.index.to_period('M')\n    monthly_pivot = pd.DataFrame({\n        'Year': monthly_returns.index.year,\n        'Month': monthly_returns.index.month,\n        'Return': monthly_returns.values\n    })\n    \n    # 创建透视表\n    heatmap_data = monthly_pivot.pivot('Year', 'Month', 'Return')\n    \n    # 绘制热图\n    plt.figure(figsize=(14, 8))\n    ax = sns.heatmap(\n        heatmap_data, \n        annot=True, \n        fmt='.1%', \n        cmap='RdYlGn',\n        center=0,\n        linewidths=0.5,\n        cbar_kws={'label': '月度收益率'}\n    )\n    \n    # 设置月份标签\n    month_labels = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', \n                   'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n    ax.set_xticklabels(month_labels)\n    \n    plt.title('投资组合月度收益热图', fontsize=16)\n    plt.tight_layout()\n    plt.show()\n    \n    # 计算按月份统计的平均收益\n    monthly_stats = monthly_pivot.mean(axis=0)\n    monthly_stats.index = month_labels\n    \n    plt.figure(figsize=(12, 6))\n    monthly_stats.plot(kind='bar', color=np.where(monthly_stats >= 0, 'green', 'red'))\n    plt.title('平均月度收益（按月份）', fontsize=16)\n    plt.ylabel('平均收益率', fontsize=14)\n    plt.grid(True, axis='y')\n    plt.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n    plt.tight_layout()\n    plt.show()\n    \n    return monthly_stats\n\n# 绘制月度收益热图\nmonthly_stats = plot_monthly_returns_heatmap(portfolio_returns)\nprint(\"\n平均月度收益（按月份）:\")\nprint(monthly_stats.map(lambda x: f\"{x:.2%}\"))\n\n# 4.5 投资组合风险分析\ndef plot_returns_distribution(returns, benchmark_returns=None):\n    # 绘制收益分布图和风险指标\n    plt.figure(figsize=(14, 10))\n    \n    # 子图布局\n    gs = plt.GridSpec(2, 2)\n    \n    # 1. 收益率分布直方图\n    ax1 = plt.subplot(gs[0, 0])\n    sns.histplot(returns, kde=True, color='blue', alpha=0.6, ax=ax1)\n    \n    # 添加正态分布曲线\n    x = np.linspace(returns.min(), returns.max(), 100)\n    ax1.plot(x, norm.pdf(x, returns.mean(), returns.std()) * len(returns) * (returns.max() - returns.min()) / 50,\n            'r--', linewidth=2)\n    \n    ax1.axvline(x=0, color='black', linestyle='--', alpha=0.5)\n    ax1.set_title('投资组合日收益率分布', fontsize=14)\n    ax1.set_xlabel('日收益率', fontsize=12)\n    ax1.set_ylabel('频率', fontsize=12)\n    \n    # 2. QQ图（正态性检验）\n    ax2 = plt.subplot(gs[0, 1])\n    from scipy import stats\n    stats.probplot(returns, dist=\"norm\", plot=ax2)\n    ax2.set_title('收益率Q-Q图（正态性检验）', fontsize=14)\n    \n    # 3. 滚动波动率\n    ax3 = plt.subplot(gs[1, 0])\n    rolling_std = returns.rolling(window=30).std() * np.sqrt(252)\n    rolling_std.plot(ax=ax3, color='blue', label='投资组合')\n    \n    if benchmark_returns is not None:\n        benchmark_rolling_std = benchmark_returns.rolling(window=30).std() * np.sqrt(252)\n        benchmark_rolling_std.plot(ax=ax3, color='red', linestyle='--', label='基准')\n    \n    ax3.set_title('30日滚动年化波动率', fontsize=14)\n    ax3.set_xlabel('日期', fontsize=12)\n    ax3.set_ylabel('年化波动率', fontsize=12)\n    ax3.legend()\n    ax3.grid(True)\n    \n    # 4. 滚动Beta（如果有基准）\n    ax4 = plt.subplot(gs[1, 1])\n    \n    if benchmark_returns is not None:\n        # 计算滚动Beta\n        rolling_cov = returns.rolling(window=60).cov(benchmark_returns)\n        rolling_var = benchmark_returns.rolling(window=60).var()\n        rolling_beta = rolling_cov / rolling_var\n        \n        rolling_beta.plot(ax=ax4, color='purple')\n        ax4.axhline(y=1, color='black', linestyle='--', alpha=0.5)\n        ax4.set_title('60日滚动Beta系数', fontsize=14)\n        ax4.set_xlabel('日期', fontsize=12)\n        ax4.set_ylabel('Beta', fontsize=12)\n        ax4.grid(True)\n    else:\n        # 如果没有基准，则显示滚动夏普比率\n        risk_free = 0.02/252  # 日无风险收益率\n        rolling_sharpe = (returns.rolling(window=60).mean() - risk_free) / returns.rolling(window=60).std() * np.sqrt(252)\n        rolling_sharpe.plot(ax=ax4, color='green')\n        ax4.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n        ax4.set_title('60日滚动夏普比率', fontsize=14)\n        ax4.set_xlabel('日期', fontsize=12)\n        ax4.set_ylabel('夏普比率', fontsize=12)\n        ax4.grid(True)\n    \n    plt.tight_layout()\n    plt.show()\n\n# 绘制收益分布和风险分析\nplot_returns_distribution(portfolio_returns, benchmark_returns)\n\n# 5. 进行压力测试和情景分析\ndef perform_stress_test(returns, weights, tickers, scenarios=None):\n    # 执行压力测试和情景分析\n    if scenarios is None:\n        # 定义几个基本情景\n        scenarios = {\n            '基本情景': {'annual_return': 0.08, 'annual_volatility': 0.15, 'correlation': 0.3},\n            '牛市情景': {'annual_return': 0.20, 'annual_volatility': 0.12, 'correlation': 0.2},\n            '熊市情景': {'annual_return': -0.25, 'annual_volatility': 0.30, 'correlation': 0.7},\n            '高波动情景': {'annual_return': 0.05, 'annual_volatility': 0.40, 'correlation': 0.5},\n            '低相关情景': {'annual_return': 0.10, 'annual_volatility': 0.20, 'correlation': 0.1}\n        }\n    \n    # 创建结果存储\n    results = pd.DataFrame(index=scenarios.keys(), \n                          columns=['年化收益率', '年化波动率', '夏普比率', '最大回撤', '95% VaR', '95% CVaR'])\n    \n    # 执行每个情景的模拟\n    np.random.seed(42)  # 固定随机种子以获得可重复结果\n    n_days = 252  # 模拟一年的交易日\n    \n    for scenario_name, params in scenarios.items():\n        # 解包参数\n        annual_return = params['annual_return']\n        annual_volatility = params['annual_volatility']\n        correlation = params['correlation']\n        \n        # 创建相关性矩阵\n        n_assets = len(tickers)\n        corr_matrix = np.ones((n_assets, n_assets)) * correlation\n        np.fill_diagonal(corr_matrix, 1.0)\n        \n        # 创建协方差矩阵\n        daily_volatility = annual_volatility / np.sqrt(n_days)\n        vols = np.random.uniform(daily_volatility * 0.7, daily_volatility * 1.3, n_assets)\n        cov_matrix = np.outer(vols, vols) * corr_matrix\n        \n        # 模拟收益率\n        daily_returns = annual_return / n_days\n        means = np.random.uniform(daily_returns * 0.5, daily_returns * 1.5, n_assets)\n        simulated_returns = np.random.multivariate_normal(means, cov_matrix, n_days)\n        \n        # 将模拟收益率转换为DataFrame\n        sim_returns_df = pd.DataFrame(simulated_returns, columns=tickers)\n        \n        # 计算投资组合收益率\n        port_returns = np.dot(simulated_returns, weights)\n        \n        # 计算表现指标\n        cum_returns = (1 + port_returns).cumprod() - 1\n        annual_ret = (1 + cum_returns[-1]) ** (252 / n_days) - 1\n        annual_vol = port_returns.std() * np.sqrt(252)\n        sharpe = (annual_ret - 0.02) / annual_vol if annual_vol != 0 else 0\n        \n        # 计算最大回撤\n        wealth_index = (1 + port_returns).cumprod()\n        previous_peaks = np.maximum.accumulate(wealth_index)\n        drawdowns = (wealth_index - previous_peaks) / previous_peaks\n        max_drawdown = min(drawdowns)\n        \n        # 计算VaR和CVaR\n        var_95 = np.percentile(port_returns, 5)\n        cvar_95 = port_returns[port_returns <= var_95].mean()\n        \n        # 存储结果\n        results.loc[scenario_name] = [\n            f\"{annual_ret:.2%}\",\n            f\"{annual_vol:.2%}\",\n            f\"{sharpe:.2f}\",\n            f\"{max_drawdown:.2%}\",\n            f\"{var_95:.2%}\",\n            f\"{cvar_95:.2%}\"\n        ]\n    \n    return results\n\n# 执行压力测试\nstress_test_results = perform_stress_test(test_returns, opt_weights, tickers)\nprint(\"\n压力测试结果:\")\nprint(stress_test_results)\n\n# 保存评估结果\nmetrics_df.to_csv('portfolio_evaluation_metrics.csv', index=False)\nstress_test_results.to_csv('portfolio_stress_test_results.csv')\n\nprint(\"\n投资组合评估完成，表现指标和测试结果已保存至CSV文件\")"
        }
      ]
    },
    {
      "id": 6,
      "title": "车险索赔率影响因素分析",
      "description": "分析影响车险索赔率的因素，建立预测模型",
      "category": "insurance",
      "difficulty_level": 3,
      "estimated_duration": 120,
      "prerequisites": "基本的统计分析和机器学习知识",
      "data_source": "车险客户和索赔数据",
      "steps": [
        {
          "id": 1,
          "title": "数据探索",
          "description": "探索车险客户和索赔数据",
          "example_code": "# 车险索赔率影响因素分析 - 数据探索\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\n\n# 设置绘图样式\nplt.style.use('seaborn-v0_8-whitegrid')\nsns.set_palette('viridis')\n\n# 加载数据\nprint(\"加载车险客户和索赔数据...\")\n# 实际项目中，这里应该是读取真实数据集\n# 此处使用模拟数据进行示例\nnp.random.seed(42)\n\n# 创建模拟数据\nn_records = 5000\ndata = {\n    # 客户特征\n    'age': np.random.normal(40, 15, n_records).clip(18, 80).astype(int),  # 年龄\n    'gender': np.random.choice(['M', 'F'], n_records),  # 性别\n    'driving_experience': np.random.normal(10, 8, n_records).clip(0, 40).astype(int),  # 驾龄\n    'marital_status': np.random.choice(['single', 'married', 'divorced', 'widowed'], n_records, p=[0.3, 0.5, 0.15, 0.05]),  # 婚姻状况\n    'credit_score': np.random.normal(650, 100, n_records).clip(300, 850).astype(int),  # 信用分数\n    \n    # 车辆特征\n    'vehicle_age': np.random.normal(5, 3, n_records).clip(0, 20).astype(int),  # 车龄\n    'vehicle_type': np.random.choice(['sedan', 'SUV', 'sports', 'truck', 'van'], n_records),  # 车型\n    'vehicle_value': np.random.normal(25000, 10000, n_records).clip(5000, 100000),  # 车辆价值\n    \n    # 保险特征\n    'coverage_type': np.random.choice(['basic', 'extended', 'premium'], n_records),  # 保障类型\n    'policy_duration': np.random.choice([1, 2, 3, 5], n_records),  # 保单期限(年)\n    'premium': np.zeros(n_records),  # 保费(待计算)\n    \n    # 地理特征\n    'region': np.random.choice(['urban', 'suburban', 'rural'], n_records, p=[0.6, 0.3, 0.1]),  # 地区类型\n    'climate_risk': np.random.choice(['low', 'medium', 'high'], n_records),  # 气候风险\n    \n    # 驾驶行为\n    'annual_mileage': np.random.normal(15000, 5000, n_records).clip(1000, 50000),  # 年行驶里程\n    'traffic_violations': np.random.poisson(0.5, n_records),  # 交通违规次数\n    'claim_history': np.random.poisson(0.3, n_records),  # 历史索赔次数\n    \n    # 索赔情况\n    'claim_occurred': np.zeros(n_records, dtype=bool),  # 是否发生索赔\n    'claim_amount': np.zeros(n_records)  # 索赔金额\n}\n\n# 生成一些关联关系\n# 年龄与驾龄关联\ndata['driving_experience'] = np.minimum(data['age'] - 18, data['driving_experience'])\n\n# 违章记录与索赔概率关联\nbase_claim_prob = 0.15\nfor i in range(n_records):\n    # 增加违章对索赔概率的影响\n    violation_factor = 1 + 0.2 * data['traffic_violations'][i]\n    # 年龄因素: 非常年轻和非常老的司机索赔概率更高\n    age_factor = 1 + 0.5 * (abs(data['age'][i] - 40) / 40)\n    # 驾龄因素: 驾龄短的索赔概率更高\n    exp_factor = 1 + max(0, (10 - data['driving_experience'][i]) / 10)\n    # 车型因素\n    vehicle_factor = 1.3 if data['vehicle_type'][i] == 'sports' else 1.0\n    # 年行驶里程因素\n    mileage_factor = data['annual_mileage'][i] / 15000\n    \n    # 综合索赔概率\n    claim_prob = base_claim_prob * violation_factor * age_factor * exp_factor * vehicle_factor * mileage_factor\n    claim_prob = min(claim_prob, 0.9)  # 限制最大概率\n    \n    # 决定是否发生索赔\n    data['claim_occurred'][i] = np.random.random() < claim_prob\n    \n    # 如果发生索赔，生成索赔金额\n    if data['claim_occurred'][i]:\n        # 基础索赔金额\n        base_amount = np.random.gamma(2, 2000)\n        # 车辆价值影响\n        value_factor = data['vehicle_value'][i] / 25000\n        # 受保障类型影响\n        coverage_factor = 1.5 if data['coverage_type'][i] == 'premium' else                          (1.2 if data['coverage_type'][i] == 'extended' else 1.0)\n        \n        # 最终索赔金额\n        data['claim_amount'][i] = base_amount * value_factor * coverage_factor\n\n# 创建DataFrame\ndf = pd.DataFrame(data)\n\n# 计算保费 (示例简化计算)\ndf['premium'] = 500 + 0.02 * df['vehicle_value'] + 100 * df['traffic_violations'] +                 200 * df['claim_history'] - 5 * df['credit_score'].clip(300, 700) / 100\n\n# 创建索赔率列\ndf['claim_rate'] = df['claim_amount'] / df['premium']\ndf.loc[~df['claim_occurred'], 'claim_rate'] = 0\n\nprint(\"数据创建完成，共 {} 条记录\".format(len(df)))\nprint(\"索赔发生率: {:.2%}\".format(df['claim_occurred'].mean()))\nprint(\"平均索赔金额: ${:.2f}\".format(df.loc[df['claim_occurred'], 'claim_amount'].mean()))\nprint(\"平均保费: ${:.2f}\".format(df['premium'].mean()))\n\n# 数据探索分析\nprint(\"\n开始数据探索分析...\")\n\n# 基本统计信息\nprint(\"\n数值型变量的基本统计信息:\")\nnumeric_cols = ['age', 'driving_experience', 'credit_score', 'vehicle_age', \n                'vehicle_value', 'premium', 'annual_mileage', \n                'traffic_violations', 'claim_history', 'claim_amount']\nprint(df[numeric_cols].describe())\n\n# 类别型变量的分布\nprint(\"\n类别型变量的分布:\")\ncategorical_cols = ['gender', 'marital_status', 'vehicle_type', 'coverage_type', \n                    'region', 'climate_risk']\nfor col in categorical_cols:\n    print(f\"\n{col} 分布:\")\n    print(df[col].value_counts(normalize=True))\n\n# 可视化分析\n\n# 1. 索赔发生率分析\nplt.figure(figsize=(14, 10))\nplt.suptitle('不同因素下的索赔发生率', fontsize=16)\n\n# 按年龄组分析\nplt.subplot(2, 3, 1)\ndf['age_group'] = pd.cut(df['age'], bins=[18, 25, 35, 45, 55, 65, 80], \n                        labels=['18-25', '26-35', '36-45', '46-55', '56-65', '66+'])\nage_claim_rate = df.groupby('age_group')['claim_occurred'].mean()\nsns.barplot(x=age_claim_rate.index, y=age_claim_rate.values)\nplt.title('年龄组与索赔率')\nplt.ylabel('索赔发生率')\nplt.xticks(rotation=45)\n\n# 按性别分析\nplt.subplot(2, 3, 2)\ngender_claim_rate = df.groupby('gender')['claim_occurred'].mean()\nsns.barplot(x=gender_claim_rate.index, y=gender_claim_rate.values)\nplt.title('性别与索赔率')\nplt.ylabel('索赔发生率')\n\n# 按车型分析\nplt.subplot(2, 3, 3)\nvehicle_claim_rate = df.groupby('vehicle_type')['claim_occurred'].mean()\nsns.barplot(x=vehicle_claim_rate.index, y=vehicle_claim_rate.values)\nplt.title('车型与索赔率')\nplt.ylabel('索赔发生率')\nplt.xticks(rotation=45)\n\n# 按地区分析\nplt.subplot(2, 3, 4)\nregion_claim_rate = df.groupby('region')['claim_occurred'].mean()\nsns.barplot(x=region_claim_rate.index, y=region_claim_rate.values)\nplt.title('地区与索赔率')\nplt.ylabel('索赔发生率')\n\n# 按驾驶违规次数分析\nplt.subplot(2, 3, 5)\nviolation_groups = df.groupby('traffic_violations')['claim_occurred'].mean()\nsns.barplot(x=violation_groups.index[:6], y=violation_groups.values[:6])\nplt.title('交通违规次数与索赔率')\nplt.xlabel('违规次数')\nplt.ylabel('索赔发生率')\n\n# 按保障类型分析\nplt.subplot(2, 3, 6)\ncoverage_claim_rate = df.groupby('coverage_type')['claim_occurred'].mean()\nsns.barplot(x=coverage_claim_rate.index, y=coverage_claim_rate.values)\nplt.title('保障类型与索赔率')\nplt.ylabel('索赔发生率')\n\nplt.tight_layout()\nplt.subplots_adjust(top=0.9)\nplt.show()\n\n# 2. 索赔金额分析\nplt.figure(figsize=(14, 10))\nplt.suptitle('不同因素下的平均索赔金额', fontsize=16)\n\n# 筛选出发生索赔的数据\nclaim_df = df[df['claim_occurred']]\n\n# 按年龄组分析\nplt.subplot(2, 3, 1)\nage_claim_amount = claim_df.groupby('age_group')['claim_amount'].mean()\nsns.barplot(x=age_claim_amount.index, y=age_claim_amount.values)\nplt.title('年龄组与平均索赔金额')\nplt.ylabel('平均索赔金额 ($)')\nplt.xticks(rotation=45)\n\n# 按性别分析\nplt.subplot(2, 3, 2)\ngender_claim_amount = claim_df.groupby('gender')['claim_amount'].mean()\nsns.barplot(x=gender_claim_amount.index, y=gender_claim_amount.values)\nplt.title('性别与平均索赔金额')\nplt.ylabel('平均索赔金额 ($)')\n\n# 按车型分析\nplt.subplot(2, 3, 3)\nvehicle_claim_amount = claim_df.groupby('vehicle_type')['claim_amount'].mean()\nsns.barplot(x=vehicle_claim_amount.index, y=vehicle_claim_amount.values)\nplt.title('车型与平均索赔金额')\nplt.ylabel('平均索赔金额 ($)')\nplt.xticks(rotation=45)\n\n# 按地区分析\nplt.subplot(2, 3, 4)\nregion_claim_amount = claim_df.groupby('region')['claim_amount'].mean()\nsns.barplot(x=region_claim_amount.index, y=region_claim_amount.values)\nplt.title('地区与平均索赔金额')\nplt.ylabel('平均索赔金额 ($)')\n\n# 按驾驶违规次数分析\nplt.subplot(2, 3, 5)\n# 将违规次数限制在0-5之间以便于可视化\nclaim_df['violation_group'] = claim_df['traffic_violations'].clip(0, 5)\nviolation_amount = claim_df.groupby('violation_group')['claim_amount'].mean()\nsns.barplot(x=violation_amount.index, y=violation_amount.values)\nplt.title('交通违规次数与平均索赔金额')\nplt.xlabel('违规次数')\nplt.ylabel('平均索赔金额 ($)')\n\n# 按保障类型分析\nplt.subplot(2, 3, 6)\ncoverage_claim_amount = claim_df.groupby('coverage_type')['claim_amount'].mean()\nsns.barplot(x=coverage_claim_amount.index, y=coverage_claim_amount.values)\nplt.title('保障类型与平均索赔金额')\nplt.ylabel('平均索赔金额 ($)')\n\nplt.tight_layout()\nplt.subplots_adjust(top=0.9)\nplt.show()\n\n# 3. 相关性分析\nplt.figure(figsize=(12, 10))\n# 选择数值型变量进行相关性分析\ncorr_vars = ['age', 'driving_experience', 'credit_score', 'vehicle_age', \n            'vehicle_value', 'premium', 'annual_mileage', \n            'traffic_violations', 'claim_history', 'claim_amount']\ncorr_matrix = df[corr_vars].corr()\n\n# 创建热图\nsns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\nplt.title('数值型变量相关性矩阵', fontsize=16)\nplt.tight_layout()\nplt.show()\n\n# 4. 索赔金额分布\nplt.figure(figsize=(10, 6))\nplt.hist(claim_df['claim_amount'], bins=50, alpha=0.7, color='skyblue')\nplt.axvline(claim_df['claim_amount'].mean(), color='red', linestyle='--', \n           label=f'平均值: ${claim_df[\"claim_amount\"].mean():.2f}')\nplt.axvline(claim_df['claim_amount'].median(), color='green', linestyle='--', \n           label=f'中位数: ${claim_df[\"claim_amount\"].median():.2f}')\nplt.title('索赔金额分布', fontsize=16)\nplt.xlabel('索赔金额 ($)')\nplt.ylabel('频率')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n# 5. 索赔率与客户特征的箱线图\nplt.figure(figsize=(14, 10))\nplt.suptitle('客户特征与索赔率关系', fontsize=16)\n\n# 按性别绘制箱线图\nplt.subplot(2, 2, 1)\nsns.boxplot(x='gender', y='claim_rate', data=df[df['claim_occurred']])\nplt.title('性别与索赔率')\nplt.ylabel('索赔率')\n\n# 按婚姻状况绘制箱线图\nplt.subplot(2, 2, 2)\nsns.boxplot(x='marital_status', y='claim_rate', data=df[df['claim_occurred']])\nplt.title('婚姻状况与索赔率')\nplt.ylabel('索赔率')\nplt.xticks(rotation=45)\n\n# 按车型绘制箱线图\nplt.subplot(2, 2, 3)\nsns.boxplot(x='vehicle_type', y='claim_rate', data=df[df['claim_occurred']])\nplt.title('车型与索赔率')\nplt.ylabel('索赔率')\nplt.xticks(rotation=45)\n\n# 按地区绘制箱线图\nplt.subplot(2, 2, 4)\nsns.boxplot(x='region', y='claim_rate', data=df[df['claim_occurred']])\nplt.title('地区与索赔率')\nplt.ylabel('索赔率')\n\nplt.tight_layout()\nplt.subplots_adjust(top=0.9)\nplt.show()\n\n# 6. 双变量分析：年龄和驾龄对索赔的影响\nplt.figure(figsize=(10, 8))\n# 创建透视表\nheatmap_data = df.pivot_table(\n    values='claim_occurred', \n    index=pd.cut(df['age'], bins=[18, 30, 40, 50, 60, 80]), \n    columns=pd.cut(df['driving_experience'], bins=[0, 5, 10, 15, 20, 40]),\n    aggfunc='mean'\n)\n\n# 绘制热图\nsns.heatmap(heatmap_data, annot=True, cmap='YlOrRd', fmt='.2f')\nplt.title('年龄和驾龄对索赔发生率的共同影响', fontsize=16)\nplt.xlabel('驾龄')\nplt.ylabel('年龄')\nplt.tight_layout()\nplt.show()\n\n# 保存处理后的数据\ndf.to_csv('car_insurance_data.csv', index=False)\nprint(\"\n数据探索分析完成，处理后的数据已保存至 car_insurance_data.csv\")"
        },
        {
          "id": 2,
          "title": "索赔率计算",
          "description": "计算不同因素下的索赔率",
          "example_code": "# 车险索赔率影响因素分析 - 索赔率计算\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\n# 设置绘图样式\nplt.style.use('seaborn-v0_8-whitegrid')\nsns.set_palette('viridis')\n\n# 加载之前保存的数据\nprint(\"加载车险客户和索赔数据...\")\ntry:\n    # 尝试加载之前步骤保存的数据\n    df = pd.read_csv('car_insurance_data.csv')\n    print(f\"成功加载数据，共 {len(df)} 条记录\")\nexcept FileNotFoundError:\n    print(\"未找到之前步骤保存的数据，请先运行数据探索步骤\")\n    # 这里可以放置数据生成代码（与步骤1相同），此处省略\n    exit(1)\n\nprint(\"\\n开始计算索赔率...\")\n\n# 1. 总体索赔率计算\n# 索赔发生率\noverall_claim_freq = df['claim_occurred'].mean()\n# 索赔金额率（索赔金额除以保费总和）\noverall_claim_severity = df[df['claim_occurred']]['claim_amount'].sum() / df['premium'].sum()\n# 平均索赔金额\navg_claim_amount = df[df['claim_occurred']]['claim_amount'].mean()\n# 平均保费\navg_premium = df['premium'].mean()\n# 总体索赔率（Loss Ratio）\nloss_ratio = df['claim_amount'].sum() / df['premium'].sum()\n\nprint(f\"总体索赔发生率: {overall_claim_freq:.2%}\")\nprint(f\"总体索赔金额率: {overall_claim_severity:.4f}\")\nprint(f\"平均索赔金额: ${avg_claim_amount:.2f}\")\nprint(f\"平均保费: ${avg_premium:.2f}\")\nprint(f\"总体索赔率 (Loss Ratio): {loss_ratio:.2%}\")\n\n# 2. 不同维度的索赔率计算\n\n# 2.1 按客户特征计算索赔率\ndef calculate_claim_metrics_by_category(data, category_col):\n    # 计算不同类别的索赔指标\n    # 按类别分组计算\n    grouped = data.groupby(category_col)\n    \n    # 创建结果DataFrame\n    results = pd.DataFrame({\n        '样本数量': grouped.size(),\n        '索赔发生率': grouped['claim_occurred'].mean(),\n        '平均索赔金额': grouped.apply(lambda x: x[x['claim_occurred']]['claim_amount'].mean() if x['claim_occurred'].any() else 0),\n        '平均保费': grouped['premium'].mean()\n    })\n    \n    # 计算索赔率 (Loss Ratio)\n    results['索赔率'] = grouped.apply(lambda x: x['claim_amount'].sum() / x['premium'].sum())\n    \n    # 计算相对风险比\n    results['相对风险指数'] = results['索赔率'] / loss_ratio\n    \n    return results.sort_values('索赔率', ascending=False)\n\n# 计算不同类别特征的索赔率\nprint(\"\\n按客户特征计算索赔率:\")\n\n# 按性别\ngender_metrics = calculate_claim_metrics_by_category(df, 'gender')\nprint(\"\\n按性别的索赔指标:\")\nprint(gender_metrics)\n\n# 按年龄组\nage_metrics = calculate_claim_metrics_by_category(df, 'age_group')\nprint(\"\\n按年龄组的索赔指标:\")\nprint(age_metrics)\n\n# 按婚姻状况\nmarital_metrics = calculate_claim_metrics_by_category(df, 'marital_status')\nprint(\"\\n按婚姻状况的索赔指标:\")\nprint(marital_metrics)\n\n# 2.2 按车辆特征计算索赔率\nprint(\"\\n按车辆特征计算索赔率:\")\n\n# 按车型\nvehicle_metrics = calculate_claim_metrics_by_category(df, 'vehicle_type')\nprint(\"\\n按车型的索赔指标:\")\nprint(vehicle_metrics)\n\n# 按车龄\ndf['vehicle_age_group'] = pd.cut(df['vehicle_age'], \n                               bins=[0, 3, 6, 10, 20], \n                               labels=['0-3年', '4-6年', '7-10年', '10年以上'])\nvehicle_age_metrics = calculate_claim_metrics_by_category(df, 'vehicle_age_group')\nprint(\"\\n按车龄的索赔指标:\")\nprint(vehicle_age_metrics)\n\n# 2.3 按驾驶行为计算索赔率\nprint(\"\\n按驾驶行为计算索赔率:\")\n\n# 按违规次数\n# 将违规次数分组\ndf['violation_group'] = pd.cut(df['traffic_violations'], \n                             bins=[-1, 0, 1, 2, 10], \n                             labels=['0次', '1次', '2次', '3次及以上'])\nviolation_metrics = calculate_claim_metrics_by_category(df, 'violation_group')\nprint(\"\\n按违规次数的索赔指标:\")\nprint(violation_metrics)\n\n# 按年行驶里程\ndf['mileage_group'] = pd.cut(df['annual_mileage'], \n                           bins=[0, 8000, 15000, 25000, 50000], \n                           labels=['低 (<8k)', '中 (8k-15k)', '高 (15k-25k)', '极高 (>25k)'])\nmileage_metrics = calculate_claim_metrics_by_category(df, 'mileage_group')\nprint(\"\\n按年行驶里程的索赔指标:\")\nprint(mileage_metrics)\n\n# 2.4 按历史索赔次数\nhistory_metrics = calculate_claim_metrics_by_category(df, 'claim_history')\nprint(\"\\n按历史索赔次数的索赔指标:\")\nprint(history_metrics)\n\n# 3. 创建索赔率可视化\n\n# 3.1 不同特征的索赔率比较\ndef plot_claim_rates_by_feature(metrics_df, feature_name, figsize=(12, 6)):\n    # 绘制不同特征的索赔率比较图\n    plt.figure(figsize=figsize)\n    \n    # 排序并绘制柱状图\n    sorted_df = metrics_df.sort_values('索赔率')\n    ax = sorted_df['索赔率'].plot(kind='barh', color='skyblue')\n    \n    # 添加总体索赔率参考线\n    plt.axvline(x=loss_ratio, color='red', linestyle='--', \n               label=f'总体索赔率: {loss_ratio:.2%}')\n    \n    # 添加标签和值\n    for i, v in enumerate(sorted_df['索赔率']):\n        ax.text(v + 0.01, i, f\"{v:.2%}\", va='center')\n    \n    plt.title(f'{feature_name}的索赔率比较', fontsize=15)\n    plt.xlabel('索赔率 (Loss Ratio)', fontsize=12)\n    plt.ylabel(feature_name, fontsize=12)\n    plt.legend()\n    plt.tight_layout()\n    plt.show()\n\n# 绘制不同特征的索赔率比较图\nplot_claim_rates_by_feature(age_metrics, '年龄组')\nplot_claim_rates_by_feature(vehicle_metrics, '车型')\nplot_claim_rates_by_feature(violation_metrics, '违规次数')\nplot_claim_rates_by_feature(mileage_metrics, '年行驶里程')\n\n# 3.2 索赔率热图 - 多维度分析\ndef plot_claim_rate_heatmap(data, row_var, col_var, figsize=(14, 10)):\n    # 绘制双变量索赔率热图\n    # 创建透视表\n    pivot_table = data.pivot_table(\n        values='claim_amount', \n        index=row_var, \n        columns=col_var, \n        aggfunc=lambda x: sum(x) / data.loc[x.index, 'premium'].sum() if len(x) > 0 else 0\n    )\n    \n    # 绘制热图\n    plt.figure(figsize=figsize)\n    sns.heatmap(pivot_table, annot=True, cmap='YlOrRd', fmt='.2%')\n    plt.title(f'{row_var}和{col_var}的索赔率分析', fontsize=16)\n    plt.tight_layout()\n    plt.show()\n    \n    return pivot_table\n\n# 绘制年龄组和违规次数的索赔率热图\nage_violation_heatmap = plot_claim_rate_heatmap(df, 'age_group', 'violation_group')\n\n# 绘制车型和年行驶里程的索赔率热图\nvehicle_mileage_heatmap = plot_claim_rate_heatmap(df, 'vehicle_type', 'mileage_group')\n\n# 3.3 索赔发生频率和索赔金额的关系\nplt.figure(figsize=(12, 8))\n\n# 为各个类别创建散点图\ncategories = {\n    '年龄组': age_metrics,\n    '车型': vehicle_metrics,\n    '婚姻状况': marital_metrics,\n    '违规次数': violation_metrics,\n    '年行驶里程': mileage_metrics\n}\n\nmarkers = ['o', 's', '^', 'd', 'x']\ncolors = ['blue', 'green', 'red', 'purple', 'orange']\n\nfor i, (category, data) in enumerate(categories.items()):\n    plt.scatter(data['索赔发生率'], data['平均索赔金额'], \n               s=100 * data['样本数量'] / data['样本数量'].max(),  # 按样本大小调整点的大小\n               alpha=0.7, \n               marker=markers[i],\n               color=colors[i],\n               label=category)\n    \n    # 为每个点添加标签\n    for j, row in data.iterrows():\n        plt.annotate(j, \n                    xy=(row['索赔发生率'], row['平均索赔金额']),\n                    xytext=(5, 5), \n                    textcoords='offset points',\n                    fontsize=9)\n\nplt.axhline(y=avg_claim_amount, color='black', linestyle='--', \n           label=f'平均索赔金额: ${avg_claim_amount:.2f}')\nplt.axvline(x=overall_claim_freq, color='gray', linestyle='--', \n           label=f'平均索赔频率: {overall_claim_freq:.2%}')\n\nplt.title('索赔频率与索赔金额的关系', fontsize=16)\nplt.xlabel('索赔发生率', fontsize=14)\nplt.ylabel('平均索赔金额 ($)', fontsize=14)\nplt.grid(True, alpha=0.3)\nplt.legend(title='特征类别')\nplt.tight_layout()\nplt.show()\n\n# 4. 索赔率模型分析\n\n# 4.1 频率-金额模型 (Frequency-Severity Model)\nprint(\"\\n构建频率-金额模型分析索赔率...\")\n\n# 频率模型 - 使用逻辑回归预测索赔发生概率\nprint(\"\\n构建索赔频率模型 (逻辑回归):\")\n\n# 准备数据 - 对分类变量进行独热编码\ncat_vars = ['gender', 'marital_status', 'vehicle_type', 'region', 'coverage_type']\nmodel_data = pd.get_dummies(df, columns=cat_vars, drop_first=True)\n\n# 选择自变量和因变量\nX_freq = model_data[[\n    'age', 'driving_experience', 'credit_score', 'vehicle_age', \n    'vehicle_value', 'annual_mileage', 'traffic_violations', \n    'claim_history'\n]]\n# 加入分类变量的哑变量\nfor col in model_data.columns:\n    if any(col.startswith(var + '_') for var in cat_vars):\n        X_freq[col] = model_data[col]\n\n# 添加常数项\nX_freq = sm.add_constant(X_freq)\ny_freq = model_data['claim_occurred']\n\n# 构建逻辑回归模型\nfreq_model = sm.Logit(y_freq, X_freq).fit(disp=0)\nprint(freq_model.summary())\n\n# 提取重要特征的影响\nprint(\"\\n索赔频率模型的重要特征影响 (按影响大小排序):\")\nfreq_odds_ratios = pd.DataFrame({\n    '变量': X_freq.columns,\n    '系数': freq_model.params,\n    '标准误': freq_model.bse,\n    'z值': freq_model.tvalues,\n    'p值': freq_model.pvalues,\n    '95%置信区间下限': freq_model.conf_int()[0],\n    '95%置信区间上限': freq_model.conf_int()[1],\n    '优势比': np.exp(freq_model.params)\n})\nprint(freq_odds_ratios.sort_values('z值', ascending=False).head(10))\n\n# 金额模型 - 使用线性回归预测已发生索赔的金额\nprint(\"\\n构建索赔金额模型 (线性回归):\")\n\n# 只选择发生索赔的样本\nclaim_data = model_data[model_data['claim_occurred']]\n\n# 选择自变量和因变量\nX_sev = claim_data[[\n    'age', 'driving_experience', 'credit_score', 'vehicle_age', \n    'vehicle_value', 'annual_mileage', 'traffic_violations', \n    'claim_history'\n]]\n# 加入分类变量的哑变量\nfor col in claim_data.columns:\n    if any(col.startswith(var + '_') for var in cat_vars):\n        X_sev[col] = claim_data[col]\n\n# 添加常数项\nX_sev = sm.add_constant(X_sev)\ny_sev = claim_data['claim_amount']\n\n# 构建线性回归模型\nsev_model = sm.OLS(y_sev, X_sev).fit()\nprint(sev_model.summary())\n\n# 提取重要特征的影响\nprint(\"\\n索赔金额模型的重要特征影响 (按影响大小排序):\")\nsev_effects = pd.DataFrame({\n    '变量': X_sev.columns,\n    '系数': sev_model.params,\n    '标准误': sev_model.bse,\n    't值': sev_model.tvalues,\n    'p值': sev_model.pvalues,\n    '95%置信区间下限': sev_model.conf_int()[0],\n    '95%置信区间上限': sev_model.conf_int()[1]\n})\nprint(sev_effects.sort_values('t值', ascending=False).head(10))\n\n# 4.2 直接索赔率模型\nprint(\"\\n构建直接索赔率模型:\")\n\n# 创建索赔率特征 (为未索赔客户赋0)\nmodel_data['claim_rate'] = model_data['claim_amount'] / model_data['premium']\nmodel_data.loc[~model_data['claim_occurred'], 'claim_rate'] = 0\n\n# 选择自变量和因变量\nX_rate = model_data[[\n    'age', 'driving_experience', 'credit_score', 'vehicle_age', \n    'vehicle_value', 'annual_mileage', 'traffic_violations', \n    'claim_history'\n]]\n# 加入分类变量的哑变量\nfor col in model_data.columns:\n    if any(col.startswith(var + '_') for var in cat_vars):\n        X_rate[col] = model_data[col]\n\n# 添加常数项\nX_rate = sm.add_constant(X_rate)\ny_rate = model_data['claim_rate']\n\n# 构建线性回归模型\nrate_model = sm.OLS(y_rate, X_rate).fit()\nprint(rate_model.summary())\n\n# 提取重要特征的影响\nprint(\"\\n索赔率模型的重要特征影响 (按影响大小排序):\")\nrate_effects = pd.DataFrame({\n    '变量': X_rate.columns,\n    '系数': rate_model.params,\n    '标准误': rate_model.bse,\n    't值': rate_model.tvalues,\n    'p值': rate_model.pvalues,\n    '95%置信区间下限': rate_model.conf_int()[0],\n    '95%置信区间上限': rate_model.conf_int()[1]\n})\nprint(rate_effects.sort_values('t值', ascending=False).head(10))\n\n# 5. 不同风险因素组合的索赔率分析\nprint(\"\\n分析不同风险因素组合的索赔率...\")\n\n# 创建高风险和低风险组合\ndef create_risk_segments(data):\n    # 基于前面的分析，构建不同风险组合\n    segments = {}\n    \n    # 高风险组合1: 年轻驾驶员 + 跑车 + 高违规次数\n    high_risk1 = data[\n        (data['age'] < 25) & \n        (data['vehicle_type'] == 'sports') & \n        (data['traffic_violations'] >= 2)\n    ]\n    segments['高风险组合1 (年轻+跑车+高违规)'] = high_risk1\n    \n    # 高风险组合2: 高年行驶里程 + 年龄较大 + 车龄较老\n    high_risk2 = data[\n        (data['annual_mileage'] > 25000) & \n        (data['age'] > 65) & \n        (data['vehicle_age'] > 10)\n    ]\n    segments['高风险组合2 (高里程+高龄+老车)'] = high_risk2\n    \n    # 中等风险: 中年驾驶员 + SUV + 中等违规\n    medium_risk = data[\n        (data['age'] >= 35) & (data['age'] <= 55) & \n        (data['vehicle_type'] == 'SUV') & \n        (data['traffic_violations'] == 1)\n    ]\n    segments['中等风险 (中年+SUV+轻微违规)'] = medium_risk\n    \n    # 低风险组合1: 经验丰富 + 轿车 + 无违规\n    low_risk1 = data[\n        (data['driving_experience'] > 15) & \n        (data['vehicle_type'] == 'sedan') & \n        (data['traffic_violations'] == 0)\n    ]\n    segments['低风险组合1 (经验丰富+轿车+无违规)'] = low_risk1\n    \n    # 低风险组合2: 中年 + 良好信用 + 已婚\n    low_risk2 = data[\n        (data['age'] >= 40) & (data['age'] <= 60) & \n        (data['credit_score'] > 750) & \n        (data['marital_status'] == 'married')\n    ]\n    segments['低风险组合2 (中年+高信用+已婚)'] = low_risk2\n    \n    return segments\n\n# 创建风险组合\nrisk_segments = create_risk_segments(df)\n\n# 计算每个组合的索赔指标\nsegments_metrics = {}\nfor name, segment in risk_segments.items():\n    if len(segment) > 0:\n        # 计算该组合的索赔指标\n        claim_freq = segment['claim_occurred'].mean()\n        claim_severity = segment[segment['claim_occurred']]['claim_amount'].mean() if segment['claim_occurred'].any() else 0\n        loss_ratio = segment['claim_amount'].sum() / segment['premium'].sum() if segment['premium'].sum() > 0 else 0\n        \n        segments_metrics[name] = {\n            '样本数量': len(segment),\n            '索赔发生率': claim_freq,\n            '平均索赔金额': claim_severity,\n            '索赔率': loss_ratio,\n            '相对风险指数': loss_ratio / (df['claim_amount'].sum() / df['premium'].sum()) if loss_ratio > 0 else 0\n        }\n\n# 转换为DataFrame并显示\nsegments_df = pd.DataFrame(segments_metrics).T\nprint(\"\\n不同风险组合的索赔指标:\")\nprint(segments_df)\n\n# 可视化不同风险组合的索赔率\nplt.figure(figsize=(12, 6))\nax = segments_df['索赔率'].sort_values().plot(kind='barh', color='skyblue')\n\n# 添加总体索赔率参考线\nplt.axvline(x=loss_ratio, color='red', linestyle='--', \n           label=f'总体索赔率: {loss_ratio:.2%}')\n\n# 添加标签和值\nfor i, v in enumerate(segments_df['索赔率'].sort_values()):\n    ax.text(v + 0.01, i, f\"{v:.2%}\", va='center')\n\nplt.title('不同风险组合的索赔率比较', fontsize=15)\nplt.xlabel('索赔率 (Loss Ratio)', fontsize=12)\nplt.ylabel('风险组合', fontsize=12)\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n# 保存索赔率分析结果\n# 将各种索赔率指标保存为CSV\nage_metrics.to_csv('claim_rate_by_age.csv')\nvehicle_metrics.to_csv('claim_rate_by_vehicle.csv')\nviolation_metrics.to_csv('claim_rate_by_violation.csv')\nsegments_df.to_csv('claim_rate_by_risk_segment.csv')\n\n# 将模型结果保存\nfreq_odds_ratios.to_csv('claim_frequency_model.csv')\nsev_effects.to_csv('claim_severity_model.csv')\nrate_effects.to_csv('claim_rate_model.csv')\n\nprint(\"\\n索赔率计算与分析完成，结果已保存至CSV文件\")"
        },
        {
          "id": 3,
          "title": "特征重要性分析",
          "description": "分析影响索赔率的重要因素",
          "example_code": "# 车险索赔率影响因素分析 - 特征重要性分析\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nimport statsmodels.api as sm\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score, mean_squared_error, r2_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.inspection import permutation_importance\nimport shap\n\n# 设置绘图样式\nplt.style.use('seaborn-v0_8-whitegrid')\nsns.set_palette('viridis')\n\n# 加载之前保存的数据\nprint(\"加载车险客户和索赔数据...\")\ntry:\n    # 尝试加载之前步骤保存的数据\n    df = pd.read_csv('car_insurance_data.csv')\n    print(f\"成功加载数据，共 {len(df)} 条记录\")\nexcept FileNotFoundError:\n    print(\"未找到之前步骤保存的数据，请先运行前面的步骤\")\n    # 这里可以放置数据生成代码（与之前步骤相同），此处省略\n    exit(1)\n\nprint(\"\\n开始特征重要性分析...\")\n\n# 1. 数据准备\n# 对分类变量进行独热编码\ncat_vars = ['gender', 'marital_status', 'vehicle_type', 'region', 'coverage_type', \n           'climate_risk']\n\n# 创建模型数据集\nmodel_data = pd.get_dummies(df, columns=cat_vars, drop_first=True)\n\n# 选择特征和目标变量\n# 特征列表 - 排除索赔相关的结果变量\nfeatures = [\n    'age', 'driving_experience', 'credit_score', 'vehicle_age', \n    'vehicle_value', 'policy_duration', 'annual_mileage', \n    'traffic_violations', 'claim_history', 'premium'\n]\n\n# 添加分类变量的独热编码列\nfor col in model_data.columns:\n    if any(col.startswith(var + '_') for var in cat_vars):\n        features.append(col)\n\n# 区分训练和测试数据集\nX = model_data[features]\ny_occurrence = model_data['claim_occurred']  # 索赔发生预测\ny_amount = model_data.loc[model_data['claim_occurred'], 'claim_amount']  # 索赔金额预测\ny_rate = model_data['claim_amount'] / model_data['premium']  # 索赔率预测 (每单位保费的索赔金额)\ny_rate = y_rate.fillna(0)  # 将没有索赔的样本设为0\n\n# 拆分训练集和测试集\nX_train, X_test, y_occur_train, y_occur_test = train_test_split(\n    X, y_occurrence, test_size=0.2, random_state=42)\n\n# 对于索赔金额模型，只使用发生索赔的样本\nX_amount = X_train[y_occur_train].reset_index(drop=True)\ny_amount_train = y_amount[:len(X_amount)].reset_index(drop=True)\n\n# 2. 特征重要性分析 - 索赔发生率模型\nprint(\"\\n2.1 索赔发生率模型的特征重要性分析...\")\n\n# 2.1.1 基于随机森林的特征重要性\nrf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\nrf_clf.fit(X_train, y_occur_train)\n\n# 计算特征重要性\nimportances = rf_clf.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in rf_clf.estimators_], axis=0)\nindices = np.argsort(importances)[::-1]\n\n# 可视化特征重要性\nplt.figure(figsize=(12, 8))\nplt.title('索赔发生率模型的特征重要性（随机森林）', fontsize=16)\nplt.bar(range(X_train.shape[1]), importances[indices],\n       color='skyblue', yerr=std[indices], align='center')\nplt.xticks(range(X_train.shape[1]), X_train.columns[indices], rotation=90)\nplt.xlim([-1, min(20, X_train.shape[1])])\nplt.tight_layout()\nplt.show()\n\n# 创建重要性DataFrame\nrf_importance_df = pd.DataFrame({\n    '特征': X_train.columns,\n    '重要性': importances,\n    '标准差': std\n}).sort_values('重要性', ascending=False)\n\nprint(\"随机森林模型的前10个重要特征:\")\nprint(rf_importance_df.head(10))\n\n# 评估模型性能\ny_occur_pred_proba = rf_clf.predict_proba(X_test)[:, 1]\nauc = roc_auc_score(y_occur_test, y_occur_pred_proba)\nprint(f\"模型在测试集上的AUC: {auc:.4f}\")\n\n# 2.1.2 通过排列重要性计算特征重要性\nprint(\"\\n计算排列重要性...\")\nperm_importance = permutation_importance(rf_clf, X_test, y_occur_test, \n                                       n_repeats=10, random_state=42)\n\n# 可视化排列重要性\nsorted_idx = perm_importance.importances_mean.argsort()[::-1]\nplt.figure(figsize=(12, 8))\nplt.title('索赔发生率模型的特征重要性（排列法）', fontsize=16)\nplt.boxplot(perm_importance.importances[sorted_idx].T,\n           vert=False, labels=X_test.columns[sorted_idx])\nplt.tight_layout()\nplt.show()\n\n# 创建排列重要性DataFrame\nperm_importance_df = pd.DataFrame({\n    '特征': X_test.columns,\n    '重要性': perm_importance.importances_mean,\n    '标准差': perm_importance.importances_std\n}).sort_values('重要性', ascending=False)\n\nprint(\"排列重要性方法的前10个重要特征:\")\nprint(perm_importance_df.head(10))\n\n# 2.1.3 使用SHAP值分析特征重要性\nprint(\"\\n计算SHAP值...\")\n# 创建SHAP解释器\nexplainer = shap.TreeExplainer(rf_clf)\n# 选择一个子集进行SHAP计算（全量数据可能很慢）\nshap_sample = X_test.sample(min(500, len(X_test)), random_state=42)\nshap_values = explainer.shap_values(shap_sample)\n\n# 绘制SHAP摘要图\nplt.figure(figsize=(14, 10))\nshap.summary_plot(shap_values[1], shap_sample, feature_names=X_test.columns, show=False)\nplt.title('索赔发生率的SHAP值摘要', fontsize=16)\nplt.tight_layout()\nplt.show()\n\n# 绘制SHAP依赖图，选择几个重要特征\ntop_features = rf_importance_df['特征'].head(3).tolist()\nfor feature in top_features:\n    plt.figure(figsize=(10, 7))\n    shap.dependence_plot(feature, shap_values[1], shap_sample, \n                        feature_names=X_test.columns, show=False)\n    plt.title(f'{feature}的SHAP依赖图', fontsize=16)\n    plt.tight_layout()\n    plt.show()\n\n# 3. 特征重要性分析 - 索赔金额模型\nprint(\"\\n3.1 索赔金额模型的特征重要性分析...\")\n\n# 3.1.1 基于随机森林的特征重要性\nrf_reg = RandomForestRegressor(n_estimators=100, random_state=42)\nrf_reg.fit(X_amount, y_amount_train)\n\n# 计算特征重要性\namount_importances = rf_reg.feature_importances_\namount_std = np.std([tree.feature_importances_ for tree in rf_reg.estimators_], axis=0)\namount_indices = np.argsort(amount_importances)[::-1]\n\n# 可视化特征重要性\nplt.figure(figsize=(12, 8))\nplt.title('索赔金额模型的特征重要性（随机森林）', fontsize=16)\nplt.bar(range(X_amount.shape[1]), amount_importances[amount_indices],\n       color='lightgreen', yerr=amount_std[amount_indices], align='center')\nplt.xticks(range(X_amount.shape[1]), X_amount.columns[amount_indices], rotation=90)\nplt.xlim([-1, min(20, X_amount.shape[1])])\nplt.tight_layout()\nplt.show()\n\n# 创建重要性DataFrame\nrf_amount_importance_df = pd.DataFrame({\n    '特征': X_amount.columns,\n    '重要性': amount_importances,\n    '标准差': amount_std\n}).sort_values('重要性', ascending=False)\n\nprint(\"随机森林模型对索赔金额的前10个重要特征:\")\nprint(rf_amount_importance_df.head(10))\n\n# 3.1.2 使用SHAP值分析索赔金额模型的特征重要性\nprint(\"\\n计算索赔金额模型的SHAP值...\")\n# 创建SHAP解释器\namount_explainer = shap.TreeExplainer(rf_reg)\n# 选择一个子集进行SHAP计算\namount_shap_sample = X_amount.sample(min(500, len(X_amount)), random_state=42)\namount_shap_values = amount_explainer.shap_values(amount_shap_sample)\n\n# 绘制SHAP摘要图\nplt.figure(figsize=(14, 10))\nshap.summary_plot(amount_shap_values, amount_shap_sample, \n                 feature_names=X_amount.columns, show=False)\nplt.title('索赔金额的SHAP值摘要', fontsize=16)\nplt.tight_layout()\nplt.show()\n\n# 4. 特征重要性分析 - 索赔率模型\nprint(\"\\n4.1 索赔率模型的特征重要性分析...\")\n\n# 创建索赔率模型 (混合模型: 处理很多零值的情况)\n# 将训练集按照是否发生索赔分为两部分\nX_zero = X_train[~y_occur_train]  # 没有索赔的样本\nX_nonzero = X_train[y_occur_train]  # 有索赔的样本\ny_rate_nonzero = y_rate[y_occur_train.index[y_occur_train]]  # 有索赔样本的索赔率\n\n# 对有索赔的样本拟合回归模型\nrf_rate = RandomForestRegressor(n_estimators=100, random_state=42)\nrf_rate.fit(X_nonzero, y_rate_nonzero)\n\n# 计算特征重要性\nrate_importances = rf_rate.feature_importances_\nrate_std = np.std([tree.feature_importances_ for tree in rf_rate.estimators_], axis=0)\nrate_indices = np.argsort(rate_importances)[::-1]\n\n# 可视化特征重要性\nplt.figure(figsize=(12, 8))\nplt.title('索赔率模型的特征重要性（随机森林）', fontsize=16)\nplt.bar(range(X_nonzero.shape[1]), rate_importances[rate_indices],\n       color='salmon', yerr=rate_std[rate_indices], align='center')\nplt.xticks(range(X_nonzero.shape[1]), X_nonzero.columns[rate_indices], rotation=90)\nplt.xlim([-1, min(20, X_nonzero.shape[1])])\nplt.tight_layout()\nplt.show()\n\n# 创建重要性DataFrame\nrf_rate_importance_df = pd.DataFrame({\n    '特征': X_nonzero.columns,\n    '重要性': rate_importances,\n    '标准差': rate_std\n}).sort_values('重要性', ascending=False)\n\nprint(\"随机森林模型对索赔率的前10个重要特征:\")\nprint(rf_rate_importance_df.head(10))\n\n# 5. 特征重要性比较\nprint(\"\\n5. 不同模型的特征重要性比较...\")\n\n# 5.1 比较前10个重要特征\ntop_occur_features = set(rf_importance_df['特征'].head(10))\ntop_amount_features = set(rf_amount_importance_df['特征'].head(10))\ntop_rate_features = set(rf_rate_importance_df['特征'].head(10))\n\n# 找出共同的重要特征\ncommon_features = top_occur_features.intersection(top_amount_features).intersection(top_rate_features)\nprint(f\"三个模型共同的重要特征: {', '.join(common_features)}\")\n\n# 频率模型特有的重要特征\noccur_unique = top_occur_features - (top_amount_features.union(top_rate_features))\nprint(f\"索赔发生率模型特有的重要特征: {', '.join(occur_unique)}\")\n\n# 金额模型特有的重要特征\namount_unique = top_amount_features - (top_occur_features.union(top_rate_features))\nprint(f\"索赔金额模型特有的重要特征: {', '.join(amount_unique)}\")\n\n# 索赔率模型特有的重要特征\nrate_unique = top_rate_features - (top_occur_features.union(top_amount_features))\nprint(f\"索赔率模型特有的重要特征: {', '.join(rate_unique)}\")\n\n# 5.2 创建特征重要性对比表\n# 合并三个模型的特征重要性\nall_features = list(set(rf_importance_df['特征']).union(\n    set(rf_amount_importance_df['特征'])).union(\n    set(rf_rate_importance_df['特征'])))\n\ncomparison_df = pd.DataFrame({'特征': all_features})\n\n# 添加每个模型的重要性\ncomparison_df = comparison_df.merge(\n    rf_importance_df[['特征', '重要性']].rename(columns={'重要性': '索赔发生率重要性'}),\n    on='特征', how='left')\n\ncomparison_df = comparison_df.merge(\n    rf_amount_importance_df[['特征', '重要性']].rename(columns={'重要性': '索赔金额重要性'}),\n    on='特征', how='left')\n\ncomparison_df = comparison_df.merge(\n    rf_rate_importance_df[['特征', '重要性']].rename(columns={'重要性': '索赔率重要性'}),\n    on='特征', how='left')\n\n# 计算综合重要性得分 (三个模型的平均值)\ncomparison_df['综合重要性'] = comparison_df[['索赔发生率重要性', '索赔金额重要性', '索赔率重要性']].mean(axis=1, skipna=True)\ncomparison_df = comparison_df.sort_values('综合重要性', ascending=False)\n\nprint(\"\\n特征重要性对比表 (前20个):\")\nprint(comparison_df.head(20))\n\n# 5.3 可视化对比\n# 选择前15个综合重要性最高的特征\ntop_features = comparison_df.head(15)\n\n# 重塑数据用于绘图\nplot_data = pd.melt(top_features, \n                    id_vars=['特征'], \n                    value_vars=['索赔发生率重要性', '索赔金额重要性', '索赔率重要性'],\n                    var_name='模型', value_name='重要性')\n\n# 绘制分组条形图\nplt.figure(figsize=(14, 10))\nsns.barplot(x='特征', y='重要性', hue='模型', data=plot_data)\nplt.title('不同模型的特征重要性对比', fontsize=16)\nplt.xticks(rotation=90)\nplt.legend(title='模型类型')\nplt.tight_layout()\nplt.show()\n\n# 6. 特征影响方向分析\nprint(\"\\n6. 特征影响方向分析...\")\n\n# 6.1 索赔发生率模型的部分依赖图\nfrom sklearn.inspection import partial_dependence, plot_partial_dependence\n\n# 选择几个重要特征绘制部分依赖图\nimportant_features = rf_importance_df['特征'].head(6).tolist()\nfeature_names = X_train.columns.tolist()\nfeature_indices = [feature_names.index(feature) for feature in important_features]\n\nfig, ax = plt.subplots(2, 3, figsize=(18, 12))\nplot_partial_dependence(rf_clf, X_train, feature_indices, feature_names=feature_names,\n                       target=0, ax=ax.flatten(), grid_resolution=50)\nplt.suptitle('索赔发生率模型的部分依赖图', fontsize=16)\nplt.tight_layout()\nplt.subplots_adjust(top=0.95)\nplt.show()\n\n# 6.2 索赔金额模型的部分依赖图\namount_important_features = rf_amount_importance_df['特征'].head(6).tolist()\namount_feature_names = X_amount.columns.tolist()\namount_feature_indices = [amount_feature_names.index(feature) for feature in amount_important_features]\n\nfig, ax = plt.subplots(2, 3, figsize=(18, 12))\nplot_partial_dependence(rf_reg, X_amount, amount_feature_indices, \n                       feature_names=amount_feature_names,\n                       ax=ax.flatten(), grid_resolution=50)\nplt.suptitle('索赔金额模型的部分依赖图', fontsize=16)\nplt.tight_layout()\nplt.subplots_adjust(top=0.95)\nplt.show()\n\n# 7. 交互特征分析\nprint(\"\\n7. 特征交互效应分析...\")\n\n# 7.1 使用随机森林内置的特征重要性识别可能的交互\n# 使用SHAP值创建交互图\nfeature1 = rf_importance_df['特征'].iloc[0]  # 最重要的特征\nfeature2 = rf_importance_df['特征'].iloc[1]  # 第二重要的特征\n\nplt.figure(figsize=(10, 8))\nshap.dependence_plot(\n    feature1, shap_values[1], shap_sample,\n    interaction_index=feature2,\n    feature_names=X_test.columns,\n    show=False\n)\nplt.title(f'{feature1} 和 {feature2} 的交互效应', fontsize=16)\nplt.tight_layout()\nplt.show()\n\n# 7.2 分析分类变量间的交互\n# 选择一些分类变量分析索赔率差异\nif 'gender_M' in df.columns and 'vehicle_type_sports' in df.columns:\n    interaction_table = pd.pivot_table(\n        model_data, \n        values='claim_occurred', \n        index='gender_M',\n        columns='vehicle_type_sports', \n        aggfunc='mean'\n    )\n    \n    plt.figure(figsize=(10, 8))\n    sns.heatmap(interaction_table, annot=True, cmap='YlGnBu', fmt='.3f')\n    plt.title('性别和跑车类型对索赔发生率的交互效应', fontsize=16)\n    plt.tight_layout()\n    plt.show()\n\n# 7.3 分析年龄和驾驶经验的交互\n# 创建年龄组和驾驶经验组的交互热图\nplt.figure(figsize=(12, 10))\nage_exp_pivot = pd.pivot_table(\n    df, \n    values='claim_occurred',\n    index=pd.cut(df['age'], bins=[18, 25, 35, 45, 55, 65, 85]),\n    columns=pd.cut(df['driving_experience'], bins=[0, 2, 5, 10, 20, 50]),\n    aggfunc='mean'\n)\nsns.heatmap(age_exp_pivot, annot=True, cmap='YlOrRd', fmt='.3f')\nplt.title('年龄和驾驶经验对索赔发生率的交互效应', fontsize=16)\nplt.xlabel('驾驶经验(年)')\nplt.ylabel('年龄')\nplt.tight_layout()\nplt.show()\n\n# 8. 总结重要发现\nprint(\"\\n8. 特征重要性分析总结...\")\n\n# 8.1 为业务提供的关键发现\nprint(\"索赔发生率的关键影响因素:\")\nfor i, row in rf_importance_df.head(5).iterrows():\n    print(f\"  - {row['特征']}: 重要性 {row['重要性']:.4f}\")\n\nprint(\"\n索赔金额的关键影响因素:\")\nfor i, row in rf_amount_importance_df.head(5).iterrows():\n    print(f\"  - {row['特征']}: 重要性 {row['重要性']:.4f}\")\n\nprint(\"\n索赔率的关键影响因素:\")\nfor i, row in rf_rate_importance_df.head(5).iterrows():\n    print(f\"  - {row['特征']}: 重要性 {row['重要性']:.4f}\")\n\n# 8.2 保存特征重要性结果\nrf_importance_df.to_csv('occurrence_feature_importance.csv', index=False)\nrf_amount_importance_df.to_csv('amount_feature_importance.csv', index=False)\nrf_rate_importance_df.to_csv('rate_feature_importance.csv', index=False)\ncomparison_df.to_csv('feature_importance_comparison.csv', index=False)\n\nprint(\"\\n特征重要性分析完成，结果已保存至CSV文件\")"
        },
        {
          "id": 4,
          "title": "预测模型构建",
          "description": "构建索赔金额预测模型",
          "example_code": "# 车险索赔率影响因素分析 - 预测模型构建\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nimport statsmodels.api as sm\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score, \n                           roc_auc_score, confusion_matrix, classification_report,\n                           mean_squared_error, r2_score, mean_absolute_error)\n\n# 多种机器学习模型\nfrom sklearn.linear_model import LogisticRegression, Ridge, Lasso, ElasticNet\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\nfrom sklearn.svm import SVC, SVR\nfrom sklearn.neural_network import MLPClassifier, MLPRegressor\nfrom sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\nfrom xgboost import XGBClassifier, XGBRegressor\nimport lightgbm as lgb\nimport joblib\n\n# 设置绘图样式\nplt.style.use('seaborn-v0_8-whitegrid')\nsns.set_palette('viridis')\n\n# 加载之前保存的数据\nprint(\"加载车险客户和索赔数据...\")\ntry:\n    # 尝试加载之前步骤保存的数据\n    df = pd.read_csv('car_insurance_data.csv')\n    print(f\"成功加载数据，共 {len(df)} 条记录\")\nexcept FileNotFoundError:\n    print(\"未找到之前步骤保存的数据，请先运行前面的步骤\")\n    # 这里可以放置数据生成代码（与之前步骤相同），此处省略\n    exit(1)\n\nprint(\"\\n开始构建预测模型...\")\n\n# 1. 数据准备\n# 1.1 识别特征类型\nnumeric_features = ['age', 'driving_experience', 'credit_score', 'vehicle_age', \n                   'vehicle_value', 'policy_duration', 'annual_mileage', \n                   'traffic_violations', 'claim_history']\ncategorical_features = ['gender', 'marital_status', 'vehicle_type', 'region', \n                      'coverage_type', 'climate_risk']\n\n# 1.2 处理缺失值（如果有）\ndf_clean = df.copy()\nif df_clean.isnull().sum().sum() > 0:\n    print(\"\\n处理缺失值...\")\n    # 数值型变量使用中位数填充\n    for col in numeric_features:\n        if df_clean[col].isnull().sum() > 0:\n            df_clean[col].fillna(df_clean[col].median(), inplace=True)\n    \n    # 分类变量使用众数填充\n    for col in categorical_features:\n        if df_clean[col].isnull().sum() > 0:\n            df_clean[col].fillna(df_clean[col].mode()[0], inplace=True)\n\n# 1.3 准备模型数据\n# 目标变量\ny_occurrence = df_clean['claim_occurred']  # 索赔发生预测\ny_amount = df_clean.loc[df_clean['claim_occurred'], 'claim_amount']  # 索赔金额预测\nclaim_indices = df_clean[df_clean['claim_occurred']].index\n# 组合索赔率（索赔金额/保费）\ndf_clean['claim_rate'] = df_clean['claim_amount'] / df_clean['premium']\ndf_clean.loc[~df_clean['claim_occurred'], 'claim_rate'] = 0\ny_rate = df_clean['claim_rate']\n\n# 拆分数据集\nX = df_clean.drop(['claim_occurred', 'claim_amount', 'claim_rate', 'premium'], axis=1)\nX_train, X_test, y_occur_train, y_occur_test = train_test_split(\n    X, y_occurrence, test_size=0.2, random_state=42, stratify=y_occurrence)\n\n# 对于索赔金额模型，只使用发生索赔的样本\nX_amount_train = X_train[X_train.index.isin(claim_indices)].copy()\ny_amount_train = df_clean.loc[X_amount_train.index, 'claim_amount']\nX_amount_test = X_test[X_test.index.isin(claim_indices)].copy()\ny_amount_test = df_clean.loc[X_amount_test.index, 'claim_amount']\n\n# 1.4 创建特征预处理管道\nnumeric_transformer = Pipeline(steps=[\n    ('scaler', StandardScaler())])\n\ncategorical_transformer = Pipeline(steps=[\n    ('onehot', pd.get_dummies)])\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer, numeric_features),\n        ('cat', categorical_transformer, categorical_features)])\n\n# 2. 索赔发生概率预测模型\nprint(\"\\n2. 构建索赔发生概率预测模型...\")\n\n# 2.1 定义评估指标\ndef evaluate_classification_model(model, X_train, X_test, y_train, y_test):\n    # 训练模型\n    model.fit(X_train, y_train)\n    \n    # 预测\n    y_pred = model.predict(X_test)\n    y_pred_proba = model.predict_proba(X_test)[:, 1]\n    \n    # 计算评估指标\n    accuracy = accuracy_score(y_test, y_pred)\n    precision = precision_score(y_test, y_pred)\n    recall = recall_score(y_test, y_pred)\n    f1 = f1_score(y_test, y_pred)\n    auc = roc_auc_score(y_test, y_pred_proba)\n    \n    # 创建混淆矩阵\n    cm = confusion_matrix(y_test, y_pred)\n    \n    # 返回结果\n    return {\n        'model': model,\n        'accuracy': accuracy,\n        'precision': precision,\n        'recall': recall,\n        'f1_score': f1,\n        'auc': auc,\n        'confusion_matrix': cm,\n        'y_pred': y_pred,\n        'y_pred_proba': y_pred_proba\n    }\n\n# 2.2 定义不同的分类模型\nclassification_models = {\n    'Logistic Regression': Pipeline([\n        ('preprocessor', preprocessor),\n        ('classifier', LogisticRegression(random_state=42))\n    ]),\n    'Random Forest': Pipeline([\n        ('preprocessor', preprocessor),\n        ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))\n    ]),\n    'Gradient Boosting': Pipeline([\n        ('preprocessor', preprocessor),\n        ('classifier', GradientBoostingClassifier(random_state=42))\n    ]),\n    'XGBoost': Pipeline([\n        ('preprocessor', preprocessor),\n        ('classifier', XGBClassifier(random_state=42))\n    ]),\n    'LightGBM': Pipeline([\n        ('preprocessor', preprocessor),\n        ('classifier', lgb.LGBMClassifier(random_state=42))\n    ]),\n    'SVM': Pipeline([\n        ('preprocessor', preprocessor),\n        ('classifier', SVC(probability=True, random_state=42))\n    ]),\n    'Neural Network': Pipeline([\n        ('preprocessor', preprocessor),\n        ('classifier', MLPClassifier(max_iter=1000, random_state=42))\n    ])\n}\n\n# 2.3 评估不同分类模型\nresults = {}\nfor name, model in classification_models.items():\n    print(f\"评估模型: {name}\")\n    try:\n        result = evaluate_classification_model(\n            model, X_train, X_test, y_occur_train, y_occur_test)\n        results[name] = result\n        print(f\"  - AUC: {result['auc']:.4f}\")\n        print(f\"  - F1 Score: {result['f1_score']:.4f}\")\n    except Exception as e:\n        print(f\"  - 评估失败: {str(e)}\")\n\n# 2.4 比较不同模型的性能\nresults_df = pd.DataFrame({\n    'Model': list(results.keys()),\n    'AUC': [results[model]['auc'] for model in results],\n    'Accuracy': [results[model]['accuracy'] for model in results],\n    'Precision': [results[model]['precision'] for model in results],\n    'Recall': [results[model]['recall'] for model in results],\n    'F1 Score': [results[model]['f1_score'] for model in results]\n})\nresults_df = results_df.sort_values('AUC', ascending=False)\n\nprint(\"\\n分类模型性能比较:\")\nprint(results_df)\n\n# 2.5 可视化模型性能\nplt.figure(figsize=(12, 8))\nmetrics = ['AUC', 'Accuracy', 'Precision', 'Recall', 'F1 Score']\nfor i, metric in enumerate(metrics):\n    plt.subplot(2, 3, i+1)\n    sns.barplot(x='Model', y=metric, data=results_df)\n    plt.title(f'模型比较 - {metric}')\n    plt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n\n# 2.6 选择最佳模型并进行详细评估\nbest_model_name = results_df.iloc[0]['Model']\nbest_model = results[best_model_name]['model']\nprint(f\"\\n最佳索赔发生概率模型: {best_model_name}\")\n\n# 获取最佳模型的预测结果\ny_pred = results[best_model_name]['y_pred']\ny_pred_proba = results[best_model_name]['y_pred_proba']\n\n# 打印分类报告\nreport = classification_report(y_occur_test, y_pred)\nprint(\"\\n分类报告:\")\nprint(report)\n\n# 绘制混淆矩阵\nplt.figure(figsize=(8, 6))\ncm = results[best_model_name]['confusion_matrix']\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\nplt.title(f'{best_model_name} 混淆矩阵')\nplt.xlabel('预测标签')\nplt.ylabel('真实标签')\nplt.tight_layout()\nplt.show()\n\n# 绘制ROC曲线\nfrom sklearn.metrics import roc_curve\nplt.figure(figsize=(8, 6))\nfpr, tpr, _ = roc_curve(y_occur_test, y_pred_proba)\nplt.plot(fpr, tpr, label=f'AUC = {results[best_model_name][\"auc\"]:.4f}')\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlabel('假阳性率')\nplt.ylabel('真阳性率')\nplt.title(f'{best_model_name} ROC曲线')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n# 2.7 模型调优 (可选，根据需要进行)\nprint(\"\\n对最佳模型进行调优...\")\n\n# 调优参数示例 (这里以随机森林为例)\nif best_model_name == 'Random Forest':\n    param_grid = {\n        'classifier__n_estimators': [50, 100, 200],\n        'classifier__max_depth': [None, 10, 20, 30],\n        'classifier__min_samples_split': [2, 5, 10]\n    }\n    \n    grid_search = GridSearchCV(best_model, param_grid, cv=5, scoring='roc_auc', n_jobs=-1)\n    grid_search.fit(X_train, y_occur_train)\n    \n    print(\"最佳参数:\")\n    print(grid_search.best_params_)\n    print(f\"调优后的AUC: {grid_search.best_score_:.4f}\")\n    \n    # 使用调优后的模型\n    best_tuned_model = grid_search.best_estimator_\n    y_tuned_pred = best_tuned_model.predict(X_test)\n    y_tuned_pred_proba = best_tuned_model.predict_proba(X_test)[:, 1]\n    tuned_auc = roc_auc_score(y_occur_test, y_tuned_pred_proba)\n    \n    print(f\"调优前的AUC: {results[best_model_name]['auc']:.4f}\")\n    print(f\"调优后的AUC: {tuned_auc:.4f}\")\n    \n    # 使用调优后的模型替换原模型\n    best_model = best_tuned_model\n\n# 3. 索赔金额预测模型\nprint(\"\\n3. 构建索赔金额预测模型...\")\n\n# 3.1 定义评估指标\ndef evaluate_regression_model(model, X_train, X_test, y_train, y_test):\n    # 训练模型\n    model.fit(X_train, y_train)\n    \n    # 预测\n    y_pred = model.predict(X_test)\n    \n    # 计算评估指标\n    mse = mean_squared_error(y_test, y_pred)\n    rmse = np.sqrt(mse)\n    mae = mean_absolute_error(y_test, y_pred)\n    r2 = r2_score(y_test, y_pred)\n    \n    # 返回结果\n    return {\n        'model': model,\n        'mse': mse,\n        'rmse': rmse,\n        'mae': mae,\n        'r2': r2,\n        'y_pred': y_pred\n    }\n\n# 3.2 定义不同的回归模型\nregression_models = {\n    'Linear Regression': Pipeline([\n        ('preprocessor', preprocessor),\n        ('regressor', Ridge(alpha=1.0))\n    ]),\n    'Lasso Regression': Pipeline([\n        ('preprocessor', preprocessor),\n        ('regressor', Lasso(alpha=0.1))\n    ]),\n    'ElasticNet': Pipeline([\n        ('preprocessor', preprocessor),\n        ('regressor', ElasticNet(alpha=0.1, l1_ratio=0.5))\n    ]),\n    'Random Forest': Pipeline([\n        ('preprocessor', preprocessor),\n        ('regressor', RandomForestRegressor(n_estimators=100, random_state=42))\n    ]),\n    'Gradient Boosting': Pipeline([\n        ('preprocessor', preprocessor),\n        ('regressor', GradientBoostingRegressor(random_state=42))\n    ]),\n    'XGBoost': Pipeline([\n        ('preprocessor', preprocessor),\n        ('regressor', XGBRegressor(random_state=42))\n    ]),\n    'LightGBM': Pipeline([\n        ('preprocessor', preprocessor),\n        ('regressor', lgb.LGBMRegressor(random_state=42))\n    ]),\n    'SVR': Pipeline([\n        ('preprocessor', preprocessor),\n        ('regressor', SVR())\n    ]),\n    'Neural Network': Pipeline([\n        ('preprocessor', preprocessor),\n        ('regressor', MLPRegressor(max_iter=1000, random_state=42))\n    ])\n}\n\n# 3.3 评估不同回归模型\nreg_results = {}\nfor name, model in regression_models.items():\n    print(f\"评估模型: {name}\")\n    try:\n        result = evaluate_regression_model(\n            model, X_amount_train, X_amount_test, y_amount_train, y_amount_test)\n        reg_results[name] = result\n        print(f\"  - RMSE: ${result['rmse']:.2f}\")\n        print(f\"  - R²: {result['r2']:.4f}\")\n    except Exception as e:\n        print(f\"  - 评估失败: {str(e)}\")\n\n# 3.4 比较不同模型的性能\nreg_results_df = pd.DataFrame({\n    'Model': list(reg_results.keys()),\n    'RMSE': [reg_results[model]['rmse'] for model in reg_results],\n    'MAE': [reg_results[model]['mae'] for model in reg_results],\n    'R²': [reg_results[model]['r2'] for model in reg_results]\n})\nreg_results_df = reg_results_df.sort_values('RMSE')\n\nprint(\"\\n回归模型性能比较:\")\nprint(reg_results_df)\n\n# 3.5 可视化模型性能\nplt.figure(figsize=(15, 10))\nmetrics = ['RMSE', 'MAE', 'R²']\nfor i, metric in enumerate(metrics):\n    plt.subplot(2, 2, i+1)\n    if metric == 'R²':\n        sns.barplot(x='Model', y=metric, data=reg_results_df.sort_values(metric, ascending=False))\n    else:\n        sns.barplot(x='Model', y=metric, data=reg_results_df.sort_values(metric))\n    plt.title(f'模型比较 - {metric}')\n    plt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n\n# 3.6 选择最佳模型并进行详细评估\nbest_reg_model_name = reg_results_df.iloc[0]['Model']\nbest_reg_model = reg_results[best_reg_model_name]['model']\nprint(f\"\\n最佳索赔金额预测模型: {best_reg_model_name}\")\n\n# 获取最佳模型的预测结果\ny_amount_pred = reg_results[best_reg_model_name]['y_pred']\n\n# 绘制预测值与实际值的对比散点图\nplt.figure(figsize=(10, 8))\nplt.scatter(y_amount_test, y_amount_pred, alpha=0.5)\nplt.plot([y_amount_test.min(), y_amount_test.max()], \n        [y_amount_test.min(), y_amount_test.max()], \n        'k--', lw=2)\nplt.xlabel('实际索赔金额')\nplt.ylabel('预测索赔金额')\nplt.title(f'{best_reg_model_name} 预测值 vs 实际值')\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n# 绘制残差图\nplt.figure(figsize=(10, 8))\nresiduals = y_amount_test - y_amount_pred\nplt.scatter(y_amount_pred, residuals, alpha=0.5)\nplt.axhline(y=0, color='r', linestyle='-')\nplt.xlabel('预测索赔金额')\nplt.ylabel('残差')\nplt.title('残差分析')\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n# 4. 构建集成模型: 两阶段预测 (先预测是否发生索赔，再预测索赔金额)\nprint(\"\\n4. 构建两阶段集成预测模型...\")\n\n# 4.1 准备用于最终评估的完整测试集\nX_full_test = X_test.copy()\n\n# 4.2 使用最佳分类模型预测索赔发生概率\nclaim_prob = best_model.predict_proba(X_full_test)[:, 1]\nclaim_pred = best_model.predict(X_full_test)\n\n# 4.3 对预测为发生索赔的样本，使用回归模型预测金额\namount_pred = np.zeros(len(X_full_test))\nmask = claim_pred == 1\nif np.any(mask):\n    amount_pred[mask] = best_reg_model.predict(X_full_test[mask])\n\n# 4.4 计算预期索赔金额 (概率 * 金额)\nexpected_claim_amount = claim_prob * amount_pred\n\n# 4.5 评估集成模型\n# 计算真实的索赔金额 (对未发生索赔的为0)\ntrue_amount = np.zeros(len(y_occur_test))\ntrue_amount[y_occur_test == 1] = y_amount_test.values\n\n# 计算评估指标\nensemble_mse = mean_squared_error(true_amount, expected_claim_amount)\nensemble_rmse = np.sqrt(ensemble_mse)\nensemble_mae = mean_absolute_error(true_amount, expected_claim_amount)\n\nprint(f\"集成模型性能:\")\nprint(f\"  - RMSE: ${ensemble_rmse:.2f}\")\nprint(f\"  - MAE: ${ensemble_mae:.2f}\")\n\n# 4.6 可视化预期索赔金额与实际金额的对比\nplt.figure(figsize=(10, 8))\nplt.scatter(true_amount, expected_claim_amount, alpha=0.5)\nplt.plot([0, true_amount.max()], [0, true_amount.max()], 'k--', lw=2)\nplt.xlabel('实际索赔金额')\nplt.ylabel('预期索赔金额')\nplt.title('集成模型: 预期索赔金额 vs 实际索赔金额')\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n# 5. 模型解释\nprint(\"\\n5. 模型特征重要性分析...\")\n\n# 5.1 获取最佳分类模型的特征重要性\nif hasattr(best_model[-1], 'feature_importances_'):\n    # 如果模型有内置的特征重要性\n    importances = best_model[-1].feature_importances_\n    feature_names = []\n    \n    # 获取预处理后的特征名称\n    if hasattr(best_model[0], 'get_feature_names_out'):\n        feature_names = best_model[0].get_feature_names_out()\n    else:\n        # 根据预处理的特征类型构建特征名称\n        feature_names = numeric_features + categorical_features\n    \n    # 创建特征重要性DataFrame\n    clf_importance_df = pd.DataFrame({\n        '特征': feature_names,\n        '重要性': importances\n    }).sort_values('重要性', ascending=False)\n    \n    print(\"\\n索赔发生概率模型的特征重要性:\")\n    print(clf_importance_df.head(10))\n    \n    # 可视化特征重要性\n    plt.figure(figsize=(12, 8))\n    sns.barplot(x='重要性', y='特征', data=clf_importance_df.head(15))\n    plt.title('索赔发生概率模型 - 特征重要性')\n    plt.tight_layout()\n    plt.show()\n\n# 5.2 获取最佳回归模型的特征重要性\nif hasattr(best_reg_model[-1], 'feature_importances_'):\n    reg_importances = best_reg_model[-1].feature_importances_\n    reg_feature_names = []\n    \n    # 获取预处理后的特征名称\n    if hasattr(best_reg_model[0], 'get_feature_names_out'):\n        reg_feature_names = best_reg_model[0].get_feature_names_out()\n    else:\n        reg_feature_names = numeric_features + categorical_features\n    \n    # 创建特征重要性DataFrame\n    reg_importance_df = pd.DataFrame({\n        '特征': reg_feature_names,\n        '重要性': reg_importances\n    }).sort_values('重要性', ascending=False)\n    \n    print(\"\\n索赔金额模型的特征重要性:\")\n    print(reg_importance_df.head(10))\n    \n    # 可视化特征重要性\n    plt.figure(figsize=(12, 8))\n    sns.barplot(x='重要性', y='特征', data=reg_importance_df.head(15))\n    plt.title('索赔金额模型 - 特征重要性')\n    plt.tight_layout()\n    plt.show()\n\n# 6. 模型保存\nprint(\"\\n6. 保存模型...\")\n\n# 6.1 保存最佳索赔发生概率模型\njoblib.dump(best_model, 'claim_occurrence_model.joblib')\nprint(\"索赔发生概率模型已保存为: claim_occurrence_model.joblib\")\n\n# 6.2 保存最佳索赔金额模型\njoblib.dump(best_reg_model, 'claim_amount_model.joblib')\nprint(\"索赔金额模型已保存为: claim_amount_model.joblib\")\n\n# 7. 创建模型性能报告\nprint(\"\\n7. 生成模型性能报告...\")\n\n# 7.1 分类模型性能报告\nclassification_report_df = results_df.copy()\nclassification_report_df.to_csv('classification_model_performance.csv', index=False)\n\n# 7.2 回归模型性能报告\nregression_report_df = reg_results_df.copy()\nregression_report_df.to_csv('regression_model_performance.csv', index=False)\n\n# 7.3 特征重要性报告\nif 'clf_importance_df' in locals():\n    clf_importance_df.to_csv('claim_occurrence_feature_importance.csv', index=False)\n\nif 'reg_importance_df' in locals():\n    reg_importance_df.to_csv('claim_amount_feature_importance.csv', index=False)\n\nprint(\"模型性能报告已生成，保存为CSV文件\")\n\n# 8. 简单的模型应用示例\nprint(\"\\n8. 模型应用示例...\")\n\n# 8.1 创建一个示例客户\nexample_customer = pd.DataFrame({\n    'age': [30, 45, 22, 65],\n    'gender': ['M', 'F', 'M', 'M'],\n    'driving_experience': [8, 20, 1, 30],\n    'marital_status': ['married', 'married', 'single', 'married'],\n    'credit_score': [720, 650, 600, 750],\n    'vehicle_age': [5, 10, 1, 15],\n    'vehicle_type': ['sedan', 'SUV', 'sports', 'van'],\n    'vehicle_value': [25000, 35000, 42000, 18000],\n    'coverage_type': ['basic', 'premium', 'extended', 'basic'],\n    'policy_duration': [1, 3, 1, 5],\n    'region': ['urban', 'suburban', 'urban', 'rural'],\n    'climate_risk': ['low', 'medium', 'high', 'low'],\n    'annual_mileage': [12000, 18000, 22000, 8000],\n    'traffic_violations': [0, 1, 3, 0],\n    'claim_history': [0, 1, 0, 2]\n})\n\n# 8.2 预测索赔概率\nexample_proba = best_model.predict_proba(example_customer)[:, 1]\n\n# 8.3 对预测为会发生索赔的客户，预测索赔金额\nexample_claims = best_model.predict(example_customer)\nexample_amounts = np.zeros(len(example_customer))\nclaim_mask = example_claims == 1\nif np.any(claim_mask):\n    example_amounts[claim_mask] = best_reg_model.predict(example_customer[claim_mask])\n\n# 8.4 计算预期索赔金额\nexample_expected_amounts = example_proba * example_amounts\n\n# 8.5 计算风险溢价（预期索赔金额的一定倍数）\nrisk_premium = example_expected_amounts * 1.5  # 假设使用1.5倍作为风险溢价\n\n# 8.6 创建输出表格\nexample_results = pd.DataFrame({\n    '年龄': example_customer['age'],\n    '性别': example_customer['gender'],\n    '驾龄': example_customer['driving_experience'],\n    '车型': example_customer['vehicle_type'],\n    '违规次数': example_customer['traffic_violations'],\n    '索赔概率': example_proba,\n    '预期索赔金额': example_expected_amounts,\n    '建议风险溢价': risk_premium\n})\n\nprint(\"示例客户的索赔预测:\")\nprint(example_results)\n\n# 保存结果\nexample_results.to_csv('example_predictions.csv', index=False)\n\nprint(\"\\n预测模型构建和评估完成，所有结果已保存\")"
        },
        {
          "id": 5,
          "title": "风险分群",
          "description": "对客户进行风险分群",
          "example_code": "# 车险索赔率影响因素分析 - 风险分群\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nimport umap\nfrom scipy.cluster.hierarchy import dendrogram, linkage\nimport joblib\n\n# 设置绘图样式\nplt.style.use('seaborn-v0_8-whitegrid')\nsns.set_palette('viridis')\n\n# 加载之前保存的数据和模型\nprint(\"加载车险客户数据和预测模型...\")\ntry:\n    # 尝试加载之前步骤保存的数据\n    df = pd.read_csv('car_insurance_data.csv')\n    # 加载之前步骤的预测模型\n    claim_model = joblib.load('claim_occurrence_model.joblib')\n    amount_model = joblib.load('claim_amount_model.joblib')\n    print(f\"成功加载数据，共 {len(df)} 条记录\")\nexcept FileNotFoundError:\n    print(\"未找到之前步骤保存的数据或模型，请先运行前面的步骤\")\n    # 这里可以放置数据生成代码（与之前步骤相同），此处省略\n    exit(1)\n\nprint(\"\\n开始客户风险分群分析...\")\n\n# 1. 准备分群数据\n# 1.1 识别特征类型\nnumeric_features = ['age', 'driving_experience', 'credit_score', 'vehicle_age', \n                  'vehicle_value', 'policy_duration', 'annual_mileage', \n                  'traffic_violations', 'claim_history']\ncategorical_features = ['gender', 'marital_status', 'vehicle_type', 'region', \n                      'coverage_type', 'climate_risk']\n\n# 1.2 处理缺失值（如果有）\ndf_clean = df.copy()\nif df_clean.isnull().sum().sum() > 0:\n    print(\"\\n处理缺失值...\")\n    # 数值型变量使用中位数填充\n    for col in numeric_features:\n        if df_clean[col].isnull().sum() > 0:\n            df_clean[col].fillna(df_clean[col].median(), inplace=True)\n    \n    # 分类变量使用众数填充\n    for col in categorical_features:\n        if df_clean[col].isnull().sum() > 0:\n            df_clean[col].fillna(df_clean[col].mode()[0], inplace=True)\n\n# 1.3 特征处理\n# 为分群准备数据集\nX_cluster = df_clean.copy()\n\n# 对分类变量进行独热编码\nX_cluster = pd.get_dummies(X_cluster, columns=categorical_features)\n\n# 删除不需要的列\nX_cluster = X_cluster.drop(['claim_occurred', 'claim_amount', 'premium'], axis=1, errors='ignore')\n\n# 1.4 添加模型预测作为附加特征\n# 预测索赔概率\nif 'claim_occurrence_model.joblib' in locals():\n    try:\n        X_pred = df_clean.drop(['claim_occurred', 'claim_amount', 'premium'], axis=1, errors='ignore')\n        claim_proba = claim_model.predict_proba(X_pred)[:, 1]\n        X_cluster['predicted_claim_prob'] = claim_proba\n        print(\"已添加索赔概率预测作为分群特征\")\n    except Exception as e:\n        print(f\"无法添加索赔概率预测: {str(e)}\")\n\n# 对发生索赔的客户，添加预测的索赔金额\nif 'claim_amount_model.joblib' in locals():\n    try:\n        # 获取预测为会发生索赔的样本索引\n        claim_pred = claim_model.predict(X_pred)\n        mask = claim_pred == 1\n        \n        # 为所有客户创建一个预测索赔金额列，默认为0\n        X_cluster['predicted_claim_amount'] = 0\n        \n        # 对预测为会发生索赔的客户，预测索赔金额\n        if np.any(mask):\n            amount_pred = amount_model.predict(X_pred[mask])\n            X_cluster.loc[mask, 'predicted_claim_amount'] = amount_pred\n            \n        # 计算预期索赔损失（概率 * 金额）\n        X_cluster['expected_loss'] = X_cluster['predicted_claim_prob'] * X_cluster['predicted_claim_amount']\n        print(\"已添加预测索赔金额和预期损失作为分群特征\")\n    except Exception as e:\n        print(f\"无法添加索赔金额预测: {str(e)}\")\n\n# 1.5 特征标准化\nprint(\"\\n标准化特征...\")\nscaler = StandardScaler()\n# 仅对数值特征进行标准化\nnumeric_cols = X_cluster.select_dtypes(include=['float64', 'int64']).columns\nX_scaled = X_cluster.copy()\nX_scaled[numeric_cols] = scaler.fit_transform(X_scaled[numeric_cols])\n\n# 2. 降维可视化\nprint(\"\\n2. 执行降维分析用于可视化...\")\n\n# 2.1 主成分分析 (PCA)\nprint(\"执行PCA降维...\")\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X_scaled)\n\n# 计算解释方差比例\nexplained_variance = pca.explained_variance_ratio_\nprint(f\"前两个主成分解释的方差比例: {explained_variance[0]:.2f}, {explained_variance[1]:.2f}\")\nprint(f\"累计解释方差比例: {sum(explained_variance):.2f}\")\n\n# 2.2 t-SNE降维\nprint(\"执行t-SNE降维...\")\ntsne = TSNE(n_components=2, random_state=42)\nX_tsne = tsne.fit_transform(X_scaled)\n\n# 2.3 UMAP降维\nprint(\"执行UMAP降维...\")\nreducer = umap.UMAP(random_state=42)\nX_umap = reducer.fit_transform(X_scaled)\n\n# 2.4 可视化降维结果\nplt.figure(figsize=(18, 6))\n\nplt.subplot(1, 3, 1)\nplt.scatter(X_pca[:, 0], X_pca[:, 1], alpha=0.7, s=10)\nplt.title('PCA降维结果')\nplt.xlabel(f'PC1 ({explained_variance[0]:.2f})')\nplt.ylabel(f'PC2 ({explained_variance[1]:.2f})')\nplt.grid(True, alpha=0.3)\n\nplt.subplot(1, 3, 2)\nplt.scatter(X_tsne[:, 0], X_tsne[:, 1], alpha=0.7, s=10)\nplt.title('t-SNE降维结果')\nplt.xlabel('t-SNE 维度 1')\nplt.ylabel('t-SNE 维度 2')\nplt.grid(True, alpha=0.3)\n\nplt.subplot(1, 3, 3)\nplt.scatter(X_umap[:, 0], X_umap[:, 1], alpha=0.7, s=10)\nplt.title('UMAP降维结果')\nplt.xlabel('UMAP 维度 1')\nplt.ylabel('UMAP 维度 2')\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# 3. K-means聚类\nprint(\"\\n3. 执行K-means聚类...\")\n\n# 3.1 确定最佳聚类数量\n# 使用肘部法则\ninertia = []\nk_range = range(2, 15)\nfor k in k_range:\n    kmeans = KMeans(n_clusters=k, random_state=42)\n    kmeans.fit(X_scaled)\n    inertia.append(kmeans.inertia_)\n\n# 可视化肘部法则\nplt.figure(figsize=(10, 6))\nplt.plot(k_range, inertia, 'o-')\nplt.xlabel('聚类数量 (k)')\nplt.ylabel('惯性 (Inertia)')\nplt.title('K-means 肘部法则')\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n# 3.2 使用轮廓系数\nfrom sklearn.metrics import silhouette_score, silhouette_samples\n\nsilhouette_scores = []\nfor k in k_range:\n    kmeans = KMeans(n_clusters=k, random_state=42)\n    cluster_labels = kmeans.fit_predict(X_scaled)\n    silhouette_avg = silhouette_score(X_scaled, cluster_labels)\n    silhouette_scores.append(silhouette_avg)\n    print(f\"聚类数 {k} 的轮廓系数: {silhouette_avg:.3f}\")\n\n# 可视化轮廓系数\nplt.figure(figsize=(10, 6))\nplt.plot(k_range, silhouette_scores, 'o-')\nplt.xlabel('聚类数量 (k)')\nplt.ylabel('轮廓系数')\nplt.title('不同聚类数的轮廓系数')\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n# 3.3 选择最佳聚类数并应用K-means\n# 根据上面的分析结果选择最佳k值\nbest_k = 4  # 这里假设最佳k为4，根据实际绘图结果进行调整\nprint(f\"\\n根据肘部法则和轮廓系数，选择聚类数 k = {best_k}\")\n\nkmeans = KMeans(n_clusters=best_k, random_state=42)\ncluster_labels = kmeans.fit_predict(X_scaled)\n\n# 将聚类标签添加到原始数据\ndf_clean['kmeans_cluster'] = cluster_labels\n\n# 3.4 可视化聚类结果\nplt.figure(figsize=(18, 6))\n\n# 使用PCA降维结果可视化\nplt.subplot(1, 3, 1)\nplt.scatter(X_pca[:, 0], X_pca[:, 1], c=cluster_labels, cmap='viridis', s=30, alpha=0.7)\nplt.title('K-means聚类 (PCA降维)')\nplt.xlabel(f'PC1 ({explained_variance[0]:.2f})')\nplt.ylabel(f'PC2 ({explained_variance[1]:.2f})')\nplt.colorbar(label='聚类')\nplt.grid(True, alpha=0.3)\n\n# 使用t-SNE降维结果可视化\nplt.subplot(1, 3, 2)\nplt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=cluster_labels, cmap='viridis', s=30, alpha=0.7)\nplt.title('K-means聚类 (t-SNE降维)')\nplt.xlabel('t-SNE 维度 1')\nplt.ylabel('t-SNE 维度 2')\nplt.colorbar(label='聚类')\nplt.grid(True, alpha=0.3)\n\n# 使用UMAP降维结果可视化\nplt.subplot(1, 3, 3)\nplt.scatter(X_umap[:, 0], X_umap[:, 1], c=cluster_labels, cmap='viridis', s=30, alpha=0.7)\nplt.title('K-means聚类 (UMAP降维)')\nplt.xlabel('UMAP 维度 1')\nplt.ylabel('UMAP 维度 2')\nplt.colorbar(label='聚类')\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# 4. 聚类结果分析\nprint(\"\\n4. 分析不同聚类的特征...\")\n\n# 选择使用K-means聚类结果进行详细分析\ndf_clean['cluster'] = df_clean['kmeans_cluster']\n\n# 4.1 聚类大小分析\ncluster_sizes = df_clean['cluster'].value_counts().sort_index()\nprint(\"\\n聚类大小:\")\nprint(cluster_sizes)\n\n# 可视化聚类大小\nplt.figure(figsize=(10, 6))\ncluster_sizes.plot(kind='bar')\nplt.title('聚类大小分布')\nplt.xlabel('聚类')\nplt.ylabel('客户数量')\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n# 4.2 聚类特征分析\n# 分析数值特征\nprint(\"\\n不同聚类的数值特征平均值:\")\ncluster_means = df_clean.groupby('cluster')[numeric_features].mean()\nprint(cluster_means)\n\n# 标准化后可视化各聚类的特征均值\nplt.figure(figsize=(16, 8))\n# 对数值进行归一化以便比较\nnormalized_means = (cluster_means - cluster_means.min()) / (cluster_means.max() - cluster_means.min())\n# 转置以便绘图\nnormalized_means = normalized_means.T\nnormalized_means.plot(kind='bar', figsize=(16, 8))\nplt.title('聚类特征分析 (归一化)')\nplt.xlabel('特征')\nplt.ylabel('归一化均值')\nplt.grid(True, alpha=0.3)\nplt.legend(title='聚类')\nplt.tight_layout()\nplt.show()\n\n# 4.3 热图可视化各聚类特征\nplt.figure(figsize=(16, 10))\nplt.title('聚类特征热图')\nax = sns.heatmap(normalized_means, annot=True, cmap='viridis', fmt='.2f')\nplt.tight_layout()\nplt.show()\n\n# 4.4 聚类与索赔关系分析\nprint(\"\\n分析聚类与索赔行为的关系...\")\n\n# 计算每个聚类的索赔发生率\nclaims_by_cluster = df_clean.groupby('cluster')['claim_occurred'].mean()\nprint(\"\\n聚类索赔发生率:\")\nprint(claims_by_cluster)\n\n# 计算每个聚类的平均索赔金额\navg_claim_by_cluster = df_clean[df_clean['claim_occurred'] == 1].groupby('cluster')['claim_amount'].mean()\nprint(\"\\n聚类平均索赔金额:\")\nprint(avg_claim_by_cluster)\n\n# 可视化索赔率和索赔金额\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n\nclaims_by_cluster.plot(kind='bar', ax=ax1)\nax1.set_title('聚类索赔发生率')\nax1.set_xlabel('聚类')\nax1.set_ylabel('索赔发生率')\nax1.grid(True, alpha=0.3)\n\navg_claim_by_cluster.plot(kind='bar', ax=ax2)\nax2.set_title('聚类平均索赔金额')\nax2.set_xlabel('聚类')\nax2.set_ylabel('平均索赔金额')\nax2.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# 5. 风险定价建议\nprint(\"\\n5. 生成风险定价建议...\")\n\n# 5.1 计算风险分数\n# 基于索赔概率和索赔金额\nif 'predicted_claim_prob' in X_cluster.columns and 'predicted_claim_amount' in X_cluster.columns:\n    df_clean['risk_score'] = df_clean['cluster'].map(\n        claims_by_cluster) * avg_claim_by_cluster.fillna(0)\nelse:\n    # 如果预测值不可用，使用实际索赔数据\n    cluster_risk = {}\n    for cluster in df_clean['cluster'].unique():\n        cluster_data = df_clean[df_clean['cluster'] == cluster]\n        claim_freq = cluster_data['claim_occurred'].mean()\n        claim_avg = cluster_data[cluster_data['claim_occurred'] == 1]['claim_amount'].mean()\n        if np.isnan(claim_avg):\n            claim_avg = 0\n        cluster_risk[cluster] = claim_freq * claim_avg\n    \n    df_clean['risk_score'] = df_clean['cluster'].map(cluster_risk)\n\n# 分组为低、中、高风险\nrisk_thresholds = df_clean['risk_score'].quantile([0.33, 0.66])\nlow_risk = risk_thresholds.iloc[0]\nhigh_risk = risk_thresholds.iloc[1]\n\nconditions = [\n    (df_clean['risk_score'] <= low_risk),\n    (df_clean['risk_score'] > low_risk) & (df_clean['risk_score'] <= high_risk),\n    (df_clean['risk_score'] > high_risk)\n]\nchoices = ['低风险', '中等风险', '高风险']\ndf_clean['risk_category'] = np.select(conditions, choices, default='未知')\n\n# 5.2 每个风险类别的客户数量\nrisk_counts = df_clean['risk_category'].value_counts()\nprint(\"\\n风险类别分布:\")\nprint(risk_counts)\n\n# 可视化风险类别分布\nplt.figure(figsize=(10, 6))\nax = risk_counts.plot(kind='pie', autopct='%1.1f%%')\nplt.title('客户风险类别分布')\nplt.ylabel('')\nplt.tight_layout()\nplt.show()\n\n# 5.3 风险类别与特征关系\nrisk_features = df_clean.groupby('risk_category')[numeric_features].mean()\nprint(\"\\n风险类别特征平均值:\")\nprint(risk_features)\n\n# 可视化风险类别与特征关系\nplt.figure(figsize=(16, 8))\n# 归一化\nnormalized_risk_features = (risk_features - risk_features.min()) / (risk_features.max() - risk_features.min())\n# 热图\nplt.title('风险类别特征热图')\nax = sns.heatmap(normalized_risk_features, annot=True, cmap='YlOrRd', fmt='.2f')\nplt.tight_layout()\nplt.show()\n\n# 5.4 风险类别与索赔关系\nrisk_claims = df_clean.groupby('risk_category')['claim_occurred'].mean()\nrisk_amounts = df_clean[df_clean['claim_occurred'] == 1].groupby('risk_category')['claim_amount'].mean()\n\nprint(\"\\n风险类别索赔率:\")\nprint(risk_claims)\nprint(\"\\n风险类别平均索赔金额:\")\nprint(risk_amounts)\n\n# 可视化风险类别与索赔关系\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n\nrisk_claims.plot(kind='bar', ax=ax1)\nax1.set_title('风险类别索赔率')\nax1.set_xlabel('风险类别')\nax1.set_ylabel('索赔率')\nax1.grid(True, alpha=0.3)\n\nrisk_amounts.plot(kind='bar', ax=ax2)\nax2.set_title('风险类别平均索赔金额')\nax2.set_xlabel('风险类别')\nax2.set_ylabel('平均索赔金额')\nax2.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# 5.5 定价建议\nprint(\"\\n生成保费定价建议...\")\n\n# 基于风险分数计算定价系数\ndf_clean['pricing_factor'] = 1.0  # 基础定价因子\nrisk_cat_map = {'低风险': 0.8, '中等风险': 1.0, '高风险': 1.5}\ndf_clean['pricing_factor'] = df_clean['risk_category'].map(risk_cat_map)\n\n# 计算建议保费 (假设有基础保费数据)\nif 'premium' in df_clean.columns:\n    base_premium = df_clean['premium'].median()\nelse:\n    base_premium = 1000  # 假设的基础保费\n\ndf_clean['suggested_premium'] = base_premium * df_clean['pricing_factor']\n\n# 可视化建议保费\nplt.figure(figsize=(10, 6))\nsns.boxplot(x='risk_category', y='suggested_premium', data=df_clean, order=['低风险', '中等风险', '高风险'])\nplt.title('风险类别建议保费分布')\nplt.xlabel('风险类别')\nplt.ylabel('建议保费')\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n# 6. 保存分析结果\nprint(\"\\n6. 保存风险分群结果...\")\n\n# 6.1 保存风险分群结果\ndf_clean.to_csv('customer_risk_segmentation.csv', index=False)\nprint(\"客户风险分群结果已保存为: customer_risk_segmentation.csv\")\n\n# 6.2 保存聚类模型\njoblib.dump(kmeans, 'customer_segmentation_model.joblib')\nprint(\"聚类模型已保存为: customer_segmentation_model.joblib\")\n\n# 6.3 风险定价建议表\nrisk_pricing = pd.DataFrame({\n    '风险类别': list(risk_cat_map.keys()),\n    '客户占比': [risk_counts[cat]/sum(risk_counts) if cat in risk_counts else 0 for cat in risk_cat_map.keys()],\n    '索赔率': [risk_claims[cat] if cat in risk_claims else 0 for cat in risk_cat_map.keys()],\n    '平均索赔金额': [risk_amounts[cat] if cat in risk_amounts else 0 for cat in risk_cat_map.keys()],\n    '定价系数': list(risk_cat_map.values()),\n    '建议基础保费': [base_premium * factor for factor in risk_cat_map.values()]\n})\n\nprint(\"\\n风险定价建议表:\")\nprint(risk_pricing)\nrisk_pricing.to_csv('risk_pricing_recommendations.csv', index=False)\nprint(\"风险定价建议已保存为: risk_pricing_recommendations.csv\")\n\nprint(\"\\n客户风险分群分析完成!\")"
        },
        {
          "id": 6,
          "title": "保险定价策略",
          "description": "基于分析结果制定保险定价策略",
          "example_code": "# 车险索赔率影响因素分析 - 保险定价策略\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nimport joblib\n\n# 设置绘图样式\nplt.style.use('seaborn-v0_8-whitegrid')\nsns.set_palette('viridis')\n\n# 加载之前保存的数据和分析结果\nprint(\"加载车险客户数据和分析结果...\")\ndf = pd.read_csv('car_insurance_data.csv')\nrisk_df = pd.read_csv('customer_risk_segmentation.csv')\n\n# 确保风险分群的数据和原始数据匹配\nif 'risk_category' not in df.columns and 'risk_category' in risk_df.columns:\n    df = risk_df.copy()\n\nprint(\"\\n开始设计保险定价策略...\")\n\n# 1. 定价模型设计 - 计算纯保费和市场保费\ndef calculate_pure_premium(data):\n    # 纯保费 = 索赔概率 * 平均索赔金额\n    claim_freq = data['claim_occurred'].mean()\n    avg_claim_amount = data.loc[data['claim_occurred'], 'claim_amount'].mean()\n    pure_premium = claim_freq * avg_claim_amount\n    return pure_premium\n\ndef adjust_for_market_factors(pure_premium, market_loading=0.1, profit_margin=0.05, expense_ratio=0.2):\n    # 市场因素调整\n    market_premium = pure_premium * (1 + market_loading) / (1 - profit_margin - expense_ratio)\n    return market_premium\n\n# 2. 基于风险组的定价分析\nprint(\"\\n基于风险分组的定价分析...\")\n\n# 按风险组计算保费\nrisk_groups = df.groupby('risk_category')\nrisk_premiums = {}\n\nfor name, group in risk_groups:\n    # 计算该组的纯保费\n    pure_premium = calculate_pure_premium(group)\n    \n    # 市场因素调整 (不同风险组有不同的调整因子)\n    if name == '高风险':\n        market_premium = adjust_for_market_factors(pure_premium, market_loading=0.15, profit_margin=0.08)\n    elif name == '中等风险':\n        market_premium = adjust_for_market_factors(pure_premium, market_loading=0.1, profit_margin=0.05)\n    else: # 低风险\n        market_premium = adjust_for_market_factors(pure_premium, market_loading=0.08, profit_margin=0.03)\n        \n    risk_premiums[name] = {\n        'pure_premium': pure_premium,\n        'market_premium': market_premium,\n        'sample_size': len(group)\n    }\n\n# 转换为DataFrame以便展示\nrisk_premium_df = pd.DataFrame.from_dict(risk_premiums, orient='index')\nprint(\"\\n风险组保费计算结果:\")\nprint(risk_premium_df)\n\n# 可视化风险组保费\nplt.figure(figsize=(10, 6))\nbars = plt.bar(risk_premium_df.index, risk_premium_df['market_premium'], alpha=0.7)\nplt.title('不同风险组的基础保费', fontsize=15)\nplt.xlabel('风险类别', fontsize=12)\nplt.ylabel('保费 ($)', fontsize=12)\nplt.grid(True, alpha=0.3, axis='y')\nplt.tight_layout()\nplt.savefig('risk_group_premiums.png')\n\n# 3. 基于特征的定价因子\nprint(\"\\n计算特征定价因子...\")\n\n# 使用GLM建立费率因子模型\nmodel_data = df.copy()\n\n# 3.1 频率模型 (Logistic回归)\nprint(\"构建索赔频率GLM模型:\")\nfreq_formula = 'claim_occurred ~ age + C(gender) + driving_experience + vehicle_age + vehicle_value + annual_mileage + traffic_violations'\nfreq_model = smf.logit(formula=freq_formula, data=model_data).fit(disp=0)\nprint(freq_model.summary())\n\n# 3.2 金额模型 (Gamma回归)\nprint(\"\\n构建索赔金额GLM模型:\")\nclaim_data = model_data[model_data['claim_occurred']]\nsev_formula = 'claim_amount ~ age + C(gender) + driving_experience + vehicle_age + vehicle_value + annual_mileage + traffic_violations'\nsev_model = smf.glm(formula=sev_formula, \n                   data=claim_data, \n                   family=sm.families.Gamma(link=sm.families.links.log)).fit()\nprint(sev_model.summary())\n\n# 计算费率因子\ndef calculate_rating_factors(freq_model, sev_model):\n    # 提取频率模型参数\n    freq_params = freq_model.params\n    \n    # 提取金额模型参数\n    sev_params = sev_model.params\n    \n    # 基础纯保费\n    base_freq = np.exp(freq_params['Intercept'])\n    base_sev = np.exp(sev_params['Intercept'])\n    base_pure_premium = base_freq * base_sev\n    \n    # 计算其他变量的相对影响\n    combined_params = {}\n    for var in set(freq_params.index) | set(sev_params.index):\n        if var == 'Intercept':\n            continue\n            \n        # 频率影响\n        freq_effect = np.exp(freq_params[var]) if var in freq_params else 1.0\n        \n        # 严重性影响\n        sev_effect = np.exp(sev_params[var]) if var in sev_params else 1.0\n        \n        # 组合影响\n        combined_effect = freq_effect * sev_effect\n        combined_params[var] = combined_effect\n    \n    return base_pure_premium, combined_params\n\n# 提取费率因子\nbase_premium, rating_factors = calculate_rating_factors(freq_model, sev_model)\nrating_factors_df = pd.DataFrame(list(rating_factors.items()), columns=['变量', '费率因子'])\nrating_factors_df = rating_factors_df.sort_values('费率因子', ascending=False)\n\nprint(\"\\n基础纯保费: ${:.2f}\".format(base_premium))\nprint(\"\\n费率因子 (按影响从大到小排序):\")\nprint(rating_factors_df.head(10))\n\n# 可视化主要费率因子\nplt.figure(figsize=(12, 8))\ntop_factors = rating_factors_df.head(10)\nplt.barh(top_factors['变量'], top_factors['费率因子'], alpha=0.7)\nplt.axvline(x=1.0, color='red', linestyle='--', label='基准线')\nplt.title('主要费率因子影响', fontsize=15)\nplt.xlabel('相对影响倍数', fontsize=12)\nplt.gca().invert_yaxis()  # 从上到下显示\nplt.grid(True, alpha=0.3, axis='x')\nplt.legend()\nplt.tight_layout()\nplt.savefig('rating_factors.png')\n\n# 4. 多变量定价模型\nprint(\"\\n构建多变量定价模型...\")\n\n# 准备数据\nX = df.drop(['claim_occurred', 'claim_amount', 'risk_category'], axis=1, errors='ignore')\ny_premium = df['premium'] if 'premium' in df.columns else None\n\n# 如果没有实际保费数据，则创建模拟保费\nif y_premium is None:\n    print(\"未找到实际保费数据，使用索赔数据创建模拟保费...\")\n    claim_prob = df['claim_occurred'].values\n    claim_amount = df['claim_amount'].values\n    y_premium = claim_prob * claim_amount / (1 - 0.2 - 0.1)  # 附加费用(20%) + 利润(10%)\n    df['premium'] = y_premium\n\n# 拆分训练集和测试集\nX_train, X_test, y_train, y_test = train_test_split(X, y_premium, test_size=0.2, random_state=42)\n\n# 训练随机森林回归模型\nrf_model = RandomForestRegressor(n_estimators=100, random_state=42)\nrf_model.fit(X_train, y_train)\n\n# 评估模型表现\ny_pred = rf_model.predict(X_test)\nrmse = np.sqrt(((y_pred - y_test) ** 2).mean())\nr2 = 1 - ((y_pred - y_test) ** 2).sum() / ((y_test - y_test.mean()) ** 2).sum()\n\nprint(f\"模型评估结果:\")\nprint(f\"均方根误差 (RMSE): {rmse:.2f}\")\nprint(f\"决定系数 (R²): {r2:.4f}\")\n\n# 特征重要性分析\nfeature_importances = pd.DataFrame({\n    '特征': X_train.columns,\n    '重要性': rf_model.feature_importances_\n}).sort_values('重要性', ascending=False)\n\nprint(\"\\n定价模型特征重要性:\")\nprint(feature_importances.head(10))\n\n# 可视化特征重要性\nplt.figure(figsize=(12, 8))\nsns.barplot(x='重要性', y='特征', data=feature_importances.head(10))\nplt.title('定价模型特征重要性', fontsize=15)\nplt.xlabel('重要性', fontsize=12)\nplt.tight_layout()\nplt.savefig('pricing_feature_importance.png')\n\n# 5. 定价策略推荐\nprint(\"\\n定价策略推荐...\")\n\n# 创建定价策略推荐\npricing_strategies = {\n    '基于风险组的简单定价': {\n        '描述': '根据客户风险类别(低、中、高)分配固定保费',\n        '优点': '简单易实施，易于理解和解释',\n        '缺点': '粒度不够细，可能导致风险组内部的不公平'\n    },\n    '变量费率因子定价': {\n        '描述': '使用基础保费乘以各个风险因子',\n        '优点': '更精细的风险区分，可以考虑多个风险因素',\n        '缺点': '实施复杂，需要大量数据支持'\n    },\n    '基于预测模型的个性化定价': {\n        '描述': '使用机器学习模型预测个性化保费',\n        '优点': '最高的风险区分精度，可以捕捉复杂的风险模式',\n        '缺点': '黑盒性质，解释性差，依赖高质量数据'\n    },\n    '混合定价策略': {\n        '描述': '结合风险组定价和变量费率因子',\n        '优点': '平衡了简单性和精确性，易于实施和解释',\n        '缺点': '需要仔细校准，避免重复计算风险因素'\n    }\n}\n\n# 打印定价策略推荐\nprint(\"推荐的定价策略方案:\")\nfor strategy, details in pricing_strategies.items():\n    print(f\"\\n{strategy}:\")\n    print(f\"  描述: {details['描述']}\")\n    print(f\"  优点: {details['优点']}\")\n    print(f\"  缺点: {details['缺点']}\")\n\n# 6. 客户保费计算示例\nprint(\"\\n客户保费计算示例...\")\n\n# 创建几个示例客户\nexample_customers = [\n    {\n        'id': 1,\n        'profile': '低风险年轻客户',\n        'age': 28,\n        'gender': 'F',\n        'driving_experience': 10,\n        'vehicle_age': 2,\n        'vehicle_value': 22000,\n        'annual_mileage': 10000,\n        'traffic_violations': 0,\n        'risk_category': '低风险'\n    },\n    {\n        'id': 2,\n        'profile': '中风险成年客户',\n        'age': 45,\n        'gender': 'M',\n        'driving_experience': 20,\n        'vehicle_age': 8,\n        'vehicle_value': 35000,\n        'annual_mileage': 15000,\n        'traffic_violations': 1,\n        'risk_category': '中等风险'\n    },\n    {\n        'id': 3,\n        'profile': '高风险客户',\n        'age': 22,\n        'gender': 'M',\n        'driving_experience': 3,\n        'vehicle_age': 12,\n        'vehicle_value': 45000,\n        'annual_mileage': 20000,\n        'traffic_violations': 3,\n        'risk_category': '高风险'\n    }\n]\n\n# 计算不同定价策略下的保费\nresults = []\nfor customer in example_customers:\n    # 转换为DataFrame以便处理\n    customer_df = pd.DataFrame([customer])\n    \n    # 基于风险组的简单定价\n    risk_based_premium = risk_premium_df.loc[customer['risk_category'], 'market_premium']\n    \n    # 随机森林模型预测保费\n    rf_feature_cols = [col for col in customer_df.columns if col not in ['id', 'profile', 'risk_category']]\n    rf_premium = rf_model.predict(customer_df[rf_feature_cols])[0]\n    \n    results.append({\n        '客户ID': customer['id'],\n        '客户画像': customer['profile'],\n        '风险类别': customer['risk_category'],\n        '基于风险组定价': risk_based_premium,\n        '基于预测模型定价': rf_premium\n    })\n\n# 转换为DataFrame并显示\nresults_df = pd.DataFrame(results)\nprint(\"\\n不同定价策略下的客户保费对比:\")\nprint(results_df)\n\n# 可视化对比结果\nplt.figure(figsize=(14, 8))\ncustomer_ids = results_df['客户ID'].tolist()\nrisk_based = results_df['基于风险组定价'].tolist()\nmodel_based = results_df['基于预测模型定价'].tolist()\n\nx = np.arange(len(customer_ids))\nwidth = 0.35\n\nfig, ax = plt.subplots(figsize=(12, 8))\nax.bar(x - width/2, risk_based, width, label='基于风险组定价')\nax.bar(x + width/2, model_based, width, label='基于预测模型定价')\n\nax.set_title('不同定价策略下的客户保费对比', fontsize=16)\nax.set_xlabel('客户', fontsize=14)\nax.set_ylabel('保费 ($)', fontsize=14)\nax.set_xticks(x)\nax.set_xticklabels([f\"客户{id}: {profile}\" for id, profile in zip(results_df['客户ID'], results_df['客户画像'])])\nax.legend()\n\nplt.tight_layout()\nplt.savefig('customer_premiums_comparison.png')\n\n# 7. 保存定价策略结果\nprint(\"\\n保存定价策略结果...\")\n\n# 保存费率因子\nrating_factors_df.to_csv('rating_factors.csv', index=False)\n\n# 保存风险组保费\nrisk_premium_df.to_csv('risk_group_premiums.csv')\n\n# 保存特征重要性\nfeature_importances.to_csv('pricing_feature_importance.csv', index=False)\n\n# 保存示例客户的保费计算结果\nresults_df.to_csv('example_customer_premiums.csv', index=False)\n\nprint(\"\\n保险定价策略分析完成!\")"
        }
      ]
    },
    {
      "id": 7,
      "title": "医疗保险精准营销分析",
      "description": "使用客户分群和预测模型进行医疗保险精准营销",
      "category": "insurance",
      "difficulty_level": 3,
      "estimated_duration": 120,
      "prerequisites": "基本的数据分析和机器学习知识",
      "data_source": "医疗保险客户数据",
      "steps": [
        {
          "id": 1,
          "title": "数据探索",
          "description": "探索医疗保险客户数据",
          "example_code": "# 医疗保险精准营销分析 - 数据探索\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\n# 设置中文显示\nimport matplotlib as mpl\nmpl.rcParams['font.sans-serif'] = ['SimHei']  # 指定默认字体\nmpl.rcParams['axes.unicode_minus'] = False    # 解决保存图像是负号'-'显示为方块的问题\n\n# 加载数据\nprint(\"加载医疗保险客户数据...\")\ntry:\n    df = pd.read_csv('medical_insurance_data.csv')\n    print(f\"数据加载成功，共{len(df)}条记录。\")\nexcept FileNotFoundError:\n    print(\"示例：数据文件不存在，创建模拟数据以供演示\")\n    # 创建模拟数据\n    np.random.seed(42)\n    n_samples = 1000\n    \n    # 创建基本人口统计学特征\n    age = np.random.normal(45, 15, n_samples).astype(int)\n    age = np.clip(age, 18, 80)  # 限制年龄范围\n    \n    gender = np.random.choice(['男', '女'], n_samples)\n    income = np.random.normal(10000, 5000, n_samples)\n    \n    # 健康状况\n    bmi = np.random.normal(24, 4, n_samples)\n    smoker = np.random.choice([0, 1], n_samples, p=[0.8, 0.2])\n    chronic_disease = np.random.choice([0, 1, 2, 3], n_samples, p=[0.6, 0.2, 0.15, 0.05])\n    \n    # 保险相关\n    has_insurance = np.random.choice([0, 1], n_samples, p=[0.3, 0.7])\n    premium = np.zeros(n_samples)\n    premium[has_insurance == 1] = np.random.normal(2000, 800, sum(has_insurance))\n    \n    # 客户行为\n    website_visits = np.random.poisson(3, n_samples)\n    app_usage = np.random.choice([0, 1, 2, 3, 4, 5], n_samples, p=[0.4, 0.2, 0.15, 0.1, 0.1, 0.05])\n    call_center_calls = np.random.poisson(1, n_samples)\n    \n    # 营销响应\n    email_response = np.random.choice([0, 1], n_samples, p=[0.7, 0.3])\n    sms_response = np.random.choice([0, 1], n_samples, p=[0.8, 0.2])\n    \n    # 购买意向和价值\n    purchase_intention = np.random.choice([0, 1, 2, 3, 4], n_samples, p=[0.3, 0.3, 0.2, 0.1, 0.1])\n    customer_value = income * 0.01 + purchase_intention * 500 + np.random.normal(0, 500, n_samples)\n    \n    # 创建DataFrame\n    df = pd.DataFrame({\n        'age': age,\n        'gender': gender,\n        'income': income,\n        'bmi': bmi,\n        'smoker': smoker,\n        'chronic_disease': chronic_disease,\n        'has_insurance': has_insurance,\n        'premium': premium,\n        'website_visits': website_visits,\n        'app_usage': app_usage,\n        'call_center_calls': call_center_calls,\n        'email_response': email_response,\n        'sms_response': sms_response,\n        'purchase_intention': purchase_intention,\n        'customer_value': customer_value\n    })\n    \n    print(f\"创建模拟数据成功，共{len(df)}条记录。\")\n    # 保存数据以供后续步骤使用\n    df.to_csv('medical_insurance_data.csv', index=False)\n\n# 1. 数据概览\nprint(\"\n1. 数据概览\")\nprint(df.head())\nprint(\"\n数据形状:\", df.shape)\n\n# 2. 数据基本信息\nprint(\"\n2. 数据基本信息\")\nprint(df.info())\n\n# 3. 数据统计描述\nprint(\"\n3. 数据统计描述\")\nprint(df.describe())\n\n# 4. 缺失值分析\nprint(\"\n4. 缺失值分析\")\nmissing_values = df.isnull().sum()\nmissing_percentage = (missing_values / len(df)) * 100\nmissing_df = pd.DataFrame({\n    '缺失值数量': missing_values,\n    '缺失百分比': missing_percentage\n})\nprint(missing_df[missing_df['缺失值数量'] > 0])\n\n# 处理缺失值\nif missing_values.sum() > 0:\n    print(\"\n处理缺失值...\")\n    # 数值型变量使用中位数填充\n    numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns\n    for col in numeric_cols:\n        if df[col].isnull().sum() > 0:\n            df[col].fillna(df[col].median(), inplace=True)\n    \n    # 分类变量使用众数填充\n    categorical_cols = df.select_dtypes(include=['object']).columns\n    for col in categorical_cols:\n        if df[col].isnull().sum() > 0:\n            df[col].fillna(df[col].mode()[0], inplace=True)\n    \n    print(\"缺失值处理完成。\")\n\n# 5. 分类变量分析\nprint(\"\n5. 分类变量分析\")\ncategorical_cols = df.select_dtypes(include=['object']).columns.tolist()\nif 'gender' in df.columns:\n    categorical_cols.append('gender')\nif 'smoker' in df.columns:\n    categorical_cols.append('smoker')\nif 'has_insurance' in df.columns:\n    categorical_cols.append('has_insurance')\n\nfor col in categorical_cols:\n    print(f\"\n{col}变量分布:\")\n    value_counts = df[col].value_counts()\n    print(value_counts)\n    print(f\"{col}占比:\")\n    print((value_counts / len(df) * 100).round(2), \"%\")\n    \n    plt.figure(figsize=(10, 5))\n    sns.countplot(x=col, data=df, palette='viridis')\n    plt.title(f'{col}分布')\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    plt.show()\n\n# 6. 数值变量分析\nprint(\"\n6. 数值变量分析\")\nnumeric_cols = df.select_dtypes(include=['float64', 'int64']).columns.tolist()\n# 移除不需要分析的ID列\nnumeric_cols = [col for col in numeric_cols if 'id' not in col.lower()]\n\n# 绘制直方图\nplt.figure(figsize=(15, 10))\nfor i, col in enumerate(numeric_cols[:9], 1):  # 最多显示9个变量\n    plt.subplot(3, 3, i)\n    sns.histplot(df[col], kde=True, bins=30)\n    plt.title(f'{col}分布')\nplt.tight_layout()\nplt.show()\n\n# 7. 生成年龄组变量\nif 'age' in df.columns:\n    print(\"\n7. 按年龄组分析客户\")\n    df['age_group'] = pd.cut(df['age'], bins=[0, 25, 35, 45, 55, 65, 100], \n                             labels=['25岁以下', '26-35岁', '36-45岁', '46-55岁', '56-65岁', '65岁以上'])\n    \n    plt.figure(figsize=(12, 5))\n    sns.countplot(x='age_group', data=df, palette='viridis')\n    plt.title('客户年龄组分布')\n    plt.xlabel('年龄组')\n    plt.ylabel('客户数量')\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    plt.show()\n    \n    # 分析不同年龄组的保险购买情况\n    if 'has_insurance' in df.columns:\n        plt.figure(figsize=(12, 5))\n        insurance_by_age = df.groupby('age_group')['has_insurance'].mean() * 100\n        insurance_by_age.plot(kind='bar', color='skyblue')\n        plt.title('各年龄组保险购买率')\n        plt.xlabel('年龄组')\n        plt.ylabel('购买率(%)')\n        plt.xticks(rotation=45)\n        plt.grid(axis='y', linestyle='--', alpha=0.7)\n        plt.tight_layout()\n        plt.show()\n\n# 8. 收入与保险关系分析\nif 'income' in df.columns and 'has_insurance' in df.columns:\n    print(\"\n8. 收入与保险购买关系分析\")\n    # 创建收入组\n    df['income_group'] = pd.qcut(df['income'], q=5, labels=['很低', '低', '中等', '高', '很高'])\n    \n    plt.figure(figsize=(12, 5))\n    income_insurance = df.groupby('income_group')['has_insurance'].mean() * 100\n    income_insurance.plot(kind='bar', color='lightgreen')\n    plt.title('各收入组保险购买率')\n    plt.xlabel('收入水平')\n    plt.ylabel('购买率(%)')\n    plt.grid(axis='y', linestyle='--', alpha=0.7)\n    plt.tight_layout()\n    plt.show()\n\n# 9. 健康状况与保险关系\nif all(col in df.columns for col in ['bmi', 'smoker', 'chronic_disease', 'has_insurance']):\n    print(\"\n9. 健康状况与保险购买关系\")\n    \n    # BMI分组\n    df['bmi_category'] = pd.cut(df['bmi'], bins=[0, 18.5, 25, 30, 100], \n                                labels=['偏瘦', '正常', '超重', '肥胖'])\n    \n    plt.figure(figsize=(12, 5))\n    bmi_insurance = df.groupby('bmi_category')['has_insurance'].mean() * 100\n    bmi_insurance.plot(kind='bar', color='salmon')\n    plt.title('各BMI组保险购买率')\n    plt.xlabel('BMI分类')\n    plt.ylabel('购买率(%)')\n    plt.grid(axis='y', linestyle='--', alpha=0.7)\n    plt.tight_layout()\n    plt.show()\n    \n    # 吸烟者vs非吸烟者\n    plt.figure(figsize=(10, 5))\n    smoker_insurance = df.groupby('smoker')['has_insurance'].mean() * 100\n    smoker_insurance.index = ['非吸烟者', '吸烟者']\n    smoker_insurance.plot(kind='bar', color=['lightblue', 'salmon'])\n    plt.title('吸烟状态与保险购买率')\n    plt.xlabel('吸烟状态')\n    plt.ylabel('购买率(%)')\n    plt.grid(axis='y', linestyle='--', alpha=0.7)\n    plt.tight_layout()\n    plt.show()\n    \n    # 慢性病数量与保险\n    plt.figure(figsize=(10, 5))\n    chronic_insurance = df.groupby('chronic_disease')['has_insurance'].mean() * 100\n    chronic_insurance.plot(kind='bar', color='purple')\n    plt.title('慢性病数量与保险购买率')\n    plt.xlabel('慢性病数量')\n    plt.ylabel('购买率(%)')\n    plt.grid(axis='y', linestyle='--', alpha=0.7)\n    plt.tight_layout()\n    plt.show()\n\n# 10. 客户互动分析\ninteraction_cols = [col for col in df.columns if any(x in col for x in ['visit', 'usage', 'call', 'response'])]\nif interaction_cols:\n    print(\"\n10. 客户互动与购买意向分析\")\n    \n    if 'purchase_intention' in df.columns:\n        for col in interaction_cols:\n            if df[col].nunique() <= 10:  # 分类或离散数值变量\n                plt.figure(figsize=(10, 5))\n                intention_by_interaction = df.groupby(col)['purchase_intention'].mean()\n                intention_by_interaction.plot(kind='bar', color='teal')\n                plt.title(f'{col}与购买意向关系')\n                plt.xlabel(col)\n                plt.ylabel('平均购买意向')\n                plt.grid(axis='y', linestyle='--', alpha=0.7)\n                plt.tight_layout()\n                plt.show()\n\n# 11. 相关性分析\nprint(\"\n11. 相关性分析\")\n# 选择数值列\nnum_df = df.select_dtypes(include=['float64', 'int64'])\n# 计算相关系数\ncorr_matrix = num_df.corr()\n\n# 绘制热力图\nplt.figure(figsize=(14, 10))\nsns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\nplt.title('特征相关性矩阵')\nplt.tight_layout()\nplt.show()\n\n# 12. PCA分析\nprint(\"\n12. 主成分分析(PCA)\")\n# 选择数值特征\nnumeric_features = df.select_dtypes(include=['float64', 'int64']).columns.tolist()\nif 'customer_value' in numeric_features:\n    numeric_features.remove('customer_value')  # 移除目标变量\n\n# 数据标准化\nX = df[numeric_features]\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# 应用PCA\npca = PCA(n_components=2)\nprincipal_components = pca.fit_transform(X_scaled)\nprincipal_df = pd.DataFrame(data=principal_components, columns=['PC1', 'PC2'])\n\n# 添加目标变量\nif 'has_insurance' in df.columns:\n    principal_df['has_insurance'] = df['has_insurance'].values\n    \n    # 绘制PCA结果\n    plt.figure(figsize=(10, 8))\n    sns.scatterplot(x='PC1', y='PC2', hue='has_insurance', data=principal_df, palette='viridis', alpha=0.7)\n    plt.title('PCA: 客户特征降维可视化')\n    plt.xlabel(f'主成分1 (解释方差比例: {pca.explained_variance_ratio_[0]:.2f})')\n    plt.ylabel(f'主成分2 (解释方差比例: {pca.explained_variance_ratio_[1]:.2f})')\n    plt.grid(linestyle='--', alpha=0.7)\n    plt.show()\n\n# 13. 客户价值分析\nif 'customer_value' in df.columns and 'has_insurance' in df.columns:\n    print(\"\n13. 客户价值分析\")\n    \n    # 创建客户价值分组\n    df['value_segment'] = pd.qcut(df['customer_value'], q=5, labels=['极低价值', '低价值', '中等价值', '高价值', '极高价值'])\n    \n    # 不同价值客户的保险购买率\n    plt.figure(figsize=(12, 5))\n    value_insurance = df.groupby('value_segment')['has_insurance'].mean() * 100\n    value_insurance.plot(kind='bar', color='gold')\n    plt.title('各价值客户群体保险购买率')\n    plt.xlabel('客户价值分组')\n    plt.ylabel('购买率(%)')\n    plt.grid(axis='y', linestyle='--', alpha=0.7)\n    plt.tight_layout()\n    plt.show()\n\n# 14. 潜在客户分析\nif 'has_insurance' in df.columns and 'purchase_intention' in df.columns:\n    print(\"\n14. 潜在客户分析\")\n    \n    # 定义潜在客户：未投保但有一定购买意向的客户\n    potential_customers = df[(df['has_insurance'] == 0) & (df['purchase_intention'] >= 2)]\n    print(f\"潜在客户数量: {len(potential_customers)} ({len(potential_customers)/len(df)*100:.2f}%)\")\n    \n    if len(potential_customers) > 0:\n        # 潜在客户年龄分布\n        if 'age_group' in df.columns:\n            plt.figure(figsize=(12, 5))\n            potential_age = potential_customers['age_group'].value_counts().sort_index()\n            potential_age.plot(kind='bar', color='lightcoral')\n            plt.title('潜在客户年龄分布')\n            plt.xlabel('年龄组')\n            plt.ylabel('客户数量')\n            plt.grid(axis='y', linestyle='--', alpha=0.7)\n            plt.tight_layout()\n            plt.show()\n        \n        # 潜在客户收入分布\n        if 'income_group' in df.columns:\n            plt.figure(figsize=(12, 5))\n            potential_income = potential_customers['income_group'].value_counts().sort_index()\n            potential_income.plot(kind='bar', color='lightgreen')\n            plt.title('潜在客户收入分布')\n            plt.xlabel('收入组')\n            plt.ylabel('客户数量')\n            plt.grid(axis='y', linestyle='--', alpha=0.7)\n            plt.tight_layout()\n            plt.show()\n\n# 15. 总结发现\nprint(\"\n15. 数据探索总结发现\")\nprint(\"通过对医疗保险客户数据的探索性分析，我们发现以下关键洞察：\")\nprint(\"1. 客户人口统计学特征分布情况\")\nprint(\"2. 不同年龄段、收入水平的客户保险购买倾向\")\nprint(\"3. 健康状况与保险需求的关系\")\nprint(\"4. 客户互动方式与购买意向的相关性\")\nprint(\"5. 高价值客户的特征与行为模式\")\nprint(\"6. 潜在客户的规模与特征\")\nprint(\"7. 各特征间的相关性强度\")\nprint(\"\n这些发现将为后续的客户分群、购买意向预测和营销策略制定提供基础。\")\n\n# 保存处理后的数据\nprint(\"\n保存处理后的数据...\")\ndf.to_csv('medical_insurance_processed.csv', index=False)\nprint(\"数据探索完成，处理后的数据已保存。\")\n"
        },
        {
          "id": 2,
          "title": "客户分群",
          "description": "使用K-means对客户进行分群",
          "example_code": "# 医疗保险精准营销分析 - 客户分群\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.decomposition import PCA\nimport scipy.cluster.hierarchy as shc\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn.mixture import GaussianMixture\n\n# 设置中文显示\nimport matplotlib as mpl\nmpl.rcParams['font.sans-serif'] = ['SimHei']  # 指定默认字体\nmpl.rcParams['axes.unicode_minus'] = False    # 解决保存图像是负号'-'显示为方块的问题\n\n# 加载处理后的数据\nprint(\"加载处理后的医疗保险客户数据...\")\ntry:\n    df = pd.read_csv('medical_insurance_processed.csv')\n    print(f\"数据加载成功，共{len(df)}条记录。\")\nexcept FileNotFoundError:\n    print(\"未找到处理后的数据，尝试加载原始数据...\")\n    try:\n        df = pd.read_csv('medical_insurance_data.csv')\n        print(f\"原始数据加载成功，共{len(df)}条记录。\")\n    except FileNotFoundError:\n        print(\"示例：数据文件不存在，创建模拟数据以供演示\")\n        # 创建模拟数据\n        np.random.seed(42)\n        n_samples = 1000\n        \n        # 创建基本人口统计学特征\n        age = np.random.normal(45, 15, n_samples).astype(int)\n        age = np.clip(age, 18, 80)  # 限制年龄范围\n        \n        gender = np.random.choice(['男', '女'], n_samples)\n        income = np.random.normal(10000, 5000, n_samples)\n        \n        # 健康状况\n        bmi = np.random.normal(24, 4, n_samples)\n        smoker = np.random.choice([0, 1], n_samples, p=[0.8, 0.2])\n        chronic_disease = np.random.choice([0, 1, 2, 3], n_samples, p=[0.6, 0.2, 0.15, 0.05])\n        \n        # 保险相关\n        has_insurance = np.random.choice([0, 1], n_samples, p=[0.3, 0.7])\n        premium = np.zeros(n_samples)\n        premium[has_insurance == 1] = np.random.normal(2000, 800, sum(has_insurance))\n        \n        # 客户行为\n        website_visits = np.random.poisson(3, n_samples)\n        app_usage = np.random.choice([0, 1, 2, 3, 4, 5], n_samples, p=[0.4, 0.2, 0.15, 0.1, 0.1, 0.05])\n        call_center_calls = np.random.poisson(1, n_samples)\n        \n        # 营销响应\n        email_response = np.random.choice([0, 1], n_samples, p=[0.7, 0.3])\n        sms_response = np.random.choice([0, 1], n_samples, p=[0.8, 0.2])\n        \n        # 购买意向和价值\n        purchase_intention = np.random.choice([0, 1, 2, 3, 4], n_samples, p=[0.3, 0.3, 0.2, 0.1, 0.1])\n        customer_value = income * 0.01 + purchase_intention * 500 + np.random.normal(0, 500, n_samples)\n        \n        # 创建DataFrame\n        df = pd.DataFrame({\n            'age': age,\n            'gender': gender,\n            'income': income,\n            'bmi': bmi,\n            'smoker': smoker,\n            'chronic_disease': chronic_disease,\n            'has_insurance': has_insurance,\n            'premium': premium,\n            'website_visits': website_visits,\n            'app_usage': app_usage,\n            'call_center_calls': call_center_calls,\n            'email_response': email_response,\n            'sms_response': sms_response,\n            'purchase_intention': purchase_intention,\n            'customer_value': customer_value\n        })\n        \n        print(f\"创建模拟数据成功，共{len(df)}条记录。\")\n\n# 1. 数据准备\nprint(\"\n1. 数据准备\")\n\n# 将分类变量转换为数值\nprint(\"处理分类变量...\")\nif 'gender' in df.columns and df['gender'].dtype == 'object':\n    df['gender'] = df['gender'].map({'男': 0, '女': 1})\n\n# 选择用于分群的特征\nclustering_features = [\n    'age', 'income', 'bmi', 'smoker', 'chronic_disease',\n    'website_visits', 'app_usage', 'call_center_calls',\n    'email_response', 'sms_response', 'purchase_intention'\n]\n\n# 确保所有选定特征都在数据集中\nclustering_features = [f for f in clustering_features if f in df.columns]\nprint(f\"用于分群的特征: {clustering_features}\")\n\n# 准备数据\nX = df[clustering_features].copy()\nprint(\"分群数据准备完成，形状:\", X.shape)\n\n# 2. 特征标准化\nprint(\"\n2. 特征标准化\")\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\nX_scaled_df = pd.DataFrame(X_scaled, columns=clustering_features)\nprint(\"特征标准化完成\")\n\n# 3. 确定最佳聚类数\nprint(\"\n3. 确定最佳聚类数\")\n\n# 使用肘部法则\nprint(\"3.1 使用肘部法则评估最佳聚类数...\")\ninertia = []\nsilhouette_scores = []\nk_range = range(2, 11)\n\nfor k in k_range:\n    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n    kmeans.fit(X_scaled)\n    inertia.append(kmeans.inertia_)\n    silhouette_scores.append(silhouette_score(X_scaled, kmeans.labels_))\n    print(f\"K={k}, 惯性={kmeans.inertia_:.2f}, 轮廓系数={silhouette_score(X_scaled, kmeans.labels_):.3f}\")\n\n# 可视化肘部法则结果\nplt.figure(figsize=(12, 5))\nplt.subplot(1, 2, 1)\nplt.plot(k_range, inertia, 'bo-')\nplt.xlabel('聚类数 (k)')\nplt.ylabel('惯性 (Inertia)')\nplt.title('K-Means 肘部法则')\nplt.grid(True, linestyle='--', alpha=0.7)\n\nplt.subplot(1, 2, 2)\nplt.plot(k_range, silhouette_scores, 'ro-')\nplt.xlabel('聚类数 (k)')\nplt.ylabel('轮廓系数 (Silhouette Score)')\nplt.title('K-Means 轮廓系数评估')\nplt.grid(True, linestyle='--', alpha=0.7)\nplt.tight_layout()\nplt.show()\n\n# 根据肘部法则和轮廓系数选择最佳K值\nbest_k = k_range[np.argmax(silhouette_scores)]\nprint(f\"根据轮廓系数选择的最佳聚类数: {best_k}\")\n\n# 4. K-means聚类\nprint(f\"\n4. 执行K-means聚类 (k={best_k})\")\nkmeans = KMeans(n_clusters=best_k, random_state=42, n_init=10)\nclusters = kmeans.fit_predict(X_scaled)\n\n# 将聚类结果添加到原始数据中\ndf['cluster'] = clusters\nprint(\"聚类结果已添加到数据框\")\n\n# 统计各个聚类的客户数量\ncluster_counts = df['cluster'].value_counts().sort_index()\nprint(\"\n各聚类客户数量:\")\nfor cluster_id, count in cluster_counts.items():\n    print(f\"聚类 {cluster_id}: {count} 客户 ({count/len(df)*100:.1f}%)\")\n\n# 5. 聚类可视化\nprint(\"\n5. 聚类结果可视化\")\n\n# 5.1 使用PCA进行降维可视化\nprint(\"5.1 使用PCA降维进行可视化...\")\npca = PCA(n_components=2)\nprincipal_components = pca.fit_transform(X_scaled)\npca_df = pd.DataFrame(data=principal_components, columns=['PC1', 'PC2'])\npca_df['cluster'] = df['cluster']\n\n# 绘制聚类结果\nplt.figure(figsize=(10, 8))\nsns.scatterplot(x='PC1', y='PC2', hue='cluster', data=pca_df, palette='viridis', s=50, alpha=0.7)\nplt.title('客户分群结果 (PCA降维)')\nplt.xlabel(f'主成分1 (解释方差: {pca.explained_variance_ratio_[0]:.2f})')\nplt.ylabel(f'主成分2 (解释方差: {pca.explained_variance_ratio_[1]:.2f})')\nplt.grid(True, linestyle='--', alpha=0.7)\nplt.legend(title='客户群体')\nplt.tight_layout()\nplt.show()\n\n# 5.2 层次聚类可视化(可选)\nprint(\"5.2 层次聚类可视化...\")\nplt.figure(figsize=(12, 8))\n# 使用样本的子集来创建树状图，以避免过于复杂\nsample_size = min(300, len(X_scaled))\nnp.random.seed(42)\nsample_indices = np.random.choice(range(len(X_scaled)), size=sample_size, replace=False)\nX_scaled_sample = X_scaled[sample_indices]\n\n# 创建树状图\ndend = shc.dendrogram(shc.linkage(X_scaled_sample, method='ward'))\nplt.title('层次聚类树状图')\nplt.xlabel('样本索引')\nplt.ylabel('距离')\nplt.axhline(y=8, color='r', linestyle='--')  # 示例切割线，根据实际情况调整\nplt.show()\n\n# 6. 聚类特征分析\nprint(\"\n6. 聚类特征分析\")\n\n# 6.1 计算每个聚类的特征平均值\nprint(\"6.1 计算聚类中心...\")\ncluster_centers = pd.DataFrame()\nfor feature in clustering_features:\n    cluster_centers[feature] = df.groupby('cluster')[feature].mean()\n\n# 标准化聚类中心值以便比较\ncluster_centers_scaled = pd.DataFrame(\n    scaler.transform(cluster_centers), \n    index=cluster_centers.index, \n    columns=cluster_centers.columns\n)\n\n# 可视化聚类中心\nplt.figure(figsize=(14, 8))\nsns.heatmap(cluster_centers_scaled, annot=True, cmap='coolwarm', fmt='.2f')\nplt.title('各聚类中心特征值(标准化后)')\nplt.xlabel('特征')\nplt.ylabel('聚类ID')\nplt.tight_layout()\nplt.show()\n\n# 6.2 为每个聚类创建雷达图\nprint(\"6.2 创建聚类特征雷达图...\")\n\n# 准备雷达图数据\nangles = np.linspace(0, 2*np.pi, len(clustering_features), endpoint=False).tolist()\nangles += angles[:1]  # 闭合多边形\n\nfig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(polar=True))\n\nfor cluster_id in range(best_k):\n    values = cluster_centers_scaled.iloc[cluster_id].values.tolist()\n    values += values[:1]  # 闭合多边形\n    ax.plot(angles, values, linewidth=2, label=f'聚类 {cluster_id}')\n    ax.fill(angles, values, alpha=0.1)\n\nax.set_xticks(angles[:-1])\nax.set_xticklabels(clustering_features)\nax.set_title('各聚类特征雷达图')\nax.legend(loc='upper right')\nplt.show()\n\n# 7. 聚类业务特征分析\nprint(\"\n7. 聚类业务特征分析\")\n\n# 7.1 分析各聚类的保险购买情况\nif 'has_insurance' in df.columns:\n    print(\"7.1 各聚类的保险购买率...\")\n    insurance_rates = df.groupby('cluster')['has_insurance'].mean() * 100\n    \n    plt.figure(figsize=(10, 6))\n    sns.barplot(x=insurance_rates.index, y=insurance_rates.values, palette='viridis')\n    plt.title('各聚类的保险购买率')\n    plt.xlabel('聚类ID')\n    plt.ylabel('购买率 (%)')\n    plt.grid(axis='y', linestyle='--', alpha=0.7)\n    for i, v in enumerate(insurance_rates):\n        plt.text(i, v + 1, f\"{v:.1f}%\", ha='center')\n    plt.tight_layout()\n    plt.show()\n\n# 7.2 分析各聚类的客户价值\nif 'customer_value' in df.columns:\n    print(\"7.2 各聚类的平均客户价值...\")\n    value_by_cluster = df.groupby('cluster')['customer_value'].mean()\n    \n    plt.figure(figsize=(10, 6))\n    sns.barplot(x=value_by_cluster.index, y=value_by_cluster.values, palette='viridis')\n    plt.title('各聚类的平均客户价值')\n    plt.xlabel('聚类ID')\n    plt.ylabel('平均客户价值')\n    plt.grid(axis='y', linestyle='--', alpha=0.7)\n    for i, v in enumerate(value_by_cluster):\n        plt.text(i, v + 10, f\"{v:.0f}\", ha='center')\n    plt.tight_layout()\n    plt.show()\n\n# 7.3 分析各聚类的购买意向\nif 'purchase_intention' in df.columns:\n    print(\"7.3 各聚类的平均购买意向...\")\n    intention_by_cluster = df.groupby('cluster')['purchase_intention'].mean()\n    \n    plt.figure(figsize=(10, 6))\n    sns.barplot(x=intention_by_cluster.index, y=intention_by_cluster.values, palette='viridis')\n    plt.title('各聚类的平均购买意向')\n    plt.xlabel('聚类ID')\n    plt.ylabel('平均购买意向')\n    plt.grid(axis='y', linestyle='--', alpha=0.7)\n    for i, v in enumerate(intention_by_cluster):\n        plt.text(i, v + 0.05, f\"{v:.2f}\", ha='center')\n    plt.tight_layout()\n    plt.show()\n\n# 7.4 年龄分布\nif 'age' in df.columns:\n    print(\"7.4 各聚类的年龄分布...\")\n    plt.figure(figsize=(12, 6))\n    sns.boxplot(x='cluster', y='age', data=df, palette='viridis')\n    plt.title('各聚类的年龄分布')\n    plt.xlabel('聚类ID')\n    plt.ylabel('年龄')\n    plt.grid(axis='y', linestyle='--', alpha=0.7)\n    plt.tight_layout()\n    plt.show()\n\n# 7.5 收入分布\nif 'income' in df.columns:\n    print(\"7.5 各聚类的收入分布...\")\n    plt.figure(figsize=(12, 6))\n    sns.boxplot(x='cluster', y='income', data=df, palette='viridis')\n    plt.title('各聚类的收入分布')\n    plt.xlabel('聚类ID')\n    plt.ylabel('收入')\n    plt.grid(axis='y', linestyle='--', alpha=0.7)\n    plt.tight_layout()\n    plt.show()\n\n# 8. 聚类命名与业务解读\nprint(\"\n8. 聚类命名与业务解读\")\n\n# 根据聚类特征创建客户群体命名\ncluster_names = {}\n\n# 分析每个聚类的关键特征来命名\nfor cluster_id in range(best_k):\n    # 获取该聚类在各特征上的均值\n    cluster_profile = cluster_centers.iloc[cluster_id]\n    \n    # 计算该聚类在各特征上的相对特点(相对于全局均值)\n    relative_profile = (cluster_profile - X.mean()) / X.std()\n    \n    # 获取最显著的特征(正向和负向)\n    top_features = relative_profile.abs().nlargest(3)\n    \n    # 获取聚类的一些关键业务指标\n    insurance_rate = df[df['cluster'] == cluster_id]['has_insurance'].mean() * 100 if 'has_insurance' in df.columns else 0\n    avg_value = df[df['cluster'] == cluster_id]['customer_value'].mean() if 'customer_value' in df.columns else 0\n    purchase_intent = df[df['cluster'] == cluster_id]['purchase_intention'].mean() if 'purchase_intention' in df.columns else 0\n    \n    # 根据特征创建名称\n    if cluster_id == 0:\n        name = \"价值猎手\"\n    elif cluster_id == 1:\n        name = \"忠诚客户\"\n    elif cluster_id == 2:\n        name = \"年轻潜力客户\"\n    elif cluster_id == 3:\n        name = \"高收入低参与\"\n    else:\n        name = f\"客户群体 {cluster_id}\"  # 默认名称\n    \n    cluster_names[cluster_id] = name\n    \n    # 打印聚类解读\n    print(f\"\n聚类 {cluster_id} - {name}:\")\n    print(f\"  客户数量: {sum(df['cluster'] == cluster_id)} ({sum(df['cluster'] == cluster_id)/len(df)*100:.1f}%)\")\n    if 'has_insurance' in df.columns:\n        print(f\"  保险购买率: {insurance_rate:.1f}%\")\n    if 'customer_value' in df.columns:\n        print(f\"  平均客户价值: {avg_value:.0f}\")\n    if 'purchase_intention' in df.columns:\n        print(f\"  平均购买意向: {purchase_intent:.2f}\")\n    \n    print(\"  特征概况:\")\n    for feature in clustering_features[:5]:  # 输出部分关键特征\n        feature_mean = cluster_profile[feature]\n        feature_rel = relative_profile[feature]\n        direction = \"高\" if feature_rel > 0.5 else \"低\" if feature_rel < -0.5 else \"中等\"\n        print(f\"    - {feature}: {feature_mean:.2f} ({direction})\")\n    \n    print(\"  营销建议:\")\n    if cluster_id == 0:\n        print(\"    - 提供高性价比的保险产品\")\n        print(\"    - 强调保险的成本效益分析\")\n        print(\"    - 使用比较营销手法\")\n    elif cluster_id == 1:\n        print(\"    - 提供高端增值服务和产品\")\n        print(\"    - 设计客户忠诚度回馈计划\")\n        print(\"    - 推荐全面保障方案\")\n    elif cluster_id == 2:\n        print(\"    - 提供入门级保险产品\")\n        print(\"    - 使用数字化渠道进行营销\")\n        print(\"    - 强调长期健康保障价值\")\n    elif cluster_id == 3:\n        print(\"    - 提供专属定制保险方案\")\n        print(\"    - 使用高端会员活动增加参与度\")\n        print(\"    - 强调专业服务和便捷体验\")\n\n# 将聚类名称添加到数据框\ndf['cluster_name'] = df['cluster'].map(cluster_names)\n\n# 9. 保存分群结果\nprint(\"\n9. 保存分群结果\")\ndf.to_csv('medical_insurance_clustered.csv', index=False)\nprint(f\"分群结果已保存，包含{best_k}个客户群体。\")\n\n# 10. 总结\nprint(\"\n客户分群总结:\")\nprint(f\"1. 通过K-means聚类算法，我们将客户分为{best_k}个群体\")\nprint(\"2. 各群体具有明显的特征差异，包括年龄、收入、健康状况和购买行为等\")\nprint(\"3. 根据聚类结果，我们可以制定针对性的营销策略，提高转化率\")\nprint(\"4. 客户分群为后续的购买意向预测提供了重要依据\")\nprint(\"客户分群分析完成!\")\n"
        },
        {
          "id": 3,
          "title": "购买意向预测",
          "description": "构建购买意向预测模型",
          "example_code": "# 医疗保险精准营销分析 - 购买意向预测\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import GradientBoostingClassifier\nimport xgboost as xgb\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\nfrom sklearn.pipeline import Pipeline\nimport matplotlib as mpl\nimport warnings\n\n# 设置中文显示\nmpl.rcParams['font.sans-serif'] = ['SimHei']  # 指定默认字体\nmpl.rcParams['axes.unicode_minus'] = False    # 解决保存图像是负号'-'显示为方块的问题\n\n# 忽略警告\nwarnings.filterwarnings('ignore')\n\n# 加载数据\ntry:\n    print(\"加载客户数据...\")\n    df = pd.read_csv('medical_insurance_customers.csv')\n    print(f\"数据加载成功，共{len(df)}条记录。\")\nexcept FileNotFoundError:\n    print(\"示例：数据文件不存在，创建模拟数据以供演示\")\n    # 创建模拟数据\n    np.random.seed(42)\n    n_samples = 1000\n    \n    # 创建基本人口统计学特征\n    age = np.random.normal(45, 15, n_samples).astype(int)\n    age = np.clip(age, 18, 80)  # 限制年龄范围\n    \n    gender = np.random.choice(['男', '女'], n_samples)\n    income = np.random.normal(10000, 5000, n_samples)\n    \n    # 健康状况\n    bmi = np.random.normal(24, 4, n_samples)\n    smoker = np.random.choice([0, 1], n_samples, p=[0.8, 0.2])\n    chronic_disease = np.random.choice([0, 1, 2, 3], n_samples, p=[0.6, 0.2, 0.15, 0.05])\n    \n    # 保险相关\n    has_insurance = np.random.choice([0, 1], n_samples, p=[0.3, 0.7])\n    premium = np.zeros(n_samples)\n    premium[has_insurance == 1] = np.random.normal(2000, 800, sum(has_insurance))\n    \n    # 客户行为\n    website_visits = np.random.poisson(3, n_samples)\n    app_usage = np.random.choice([0, 1, 2, 3, 4, 5], n_samples, p=[0.4, 0.2, 0.15, 0.1, 0.1, 0.05])\n    call_center_calls = np.random.poisson(1, n_samples)\n    \n    # 响应变量 - 购买意向\n    purchase_intention = np.zeros(n_samples)\n    \n    # 影响购买意向的因素\n    intention_score = (\n        0.02 * (age - 18) +  # 年龄越大，意向越高\n        0.3 * (income / 10000) +  # 收入越高，意向越高\n        0.5 * app_usage +  # 使用APP越多，意向越高\n        0.8 * website_visits +  # 访问网站越多，意向越高\n        1.5 * chronic_disease +  # 慢性病越多，意向越高\n        -0.2 * has_insurance +  # 已有保险，意向略低\n        np.random.normal(0, 1, n_samples)  # 随机因素\n    )\n    \n    # 将意向分数转换为二元变量\n    threshold = np.percentile(intention_score, 70)  # 假设30%的客户有购买意向\n    purchase_intention = (intention_score > threshold).astype(int)\n    \n    # 创建DataFrame\n    df = pd.DataFrame({\n        'age': age,\n        'gender': gender,\n        'income': income,\n        'bmi': bmi,\n        'smoker': smoker,\n        'chronic_disease': chronic_disease,\n        'has_insurance': has_insurance,\n        'premium': premium,\n        'website_visits': website_visits,\n        'app_usage': app_usage,\n        'call_center_calls': call_center_calls,\n        'purchase_intention': purchase_intention\n    })\n    \n    print(f\"创建模拟数据成功，共{len(df)}条记录。\")\n\n# 1. 数据准备与目标变量定义\nprint(\"1. 数据准备与目标变量定义\")\n\n# 查看数据的基本信息\nprint(\"数据基本信息：\")\nprint(f\"行数: {df.shape[0]}, 列数: {df.shape[1]}\")\nprint(\"列名:\", df.columns.tolist())\n\n# 检查目标变量分布\nif 'purchase_intention' in df.columns:\n    print(\"目标变量分布:\")\n    intention_counts = df['purchase_intention'].value_counts(normalize=True) * 100\n    print(f\"无购买意向 (0): {intention_counts.get(0, 0):.2f}%\")\n    print(f\"有购买意向 (1): {intention_counts.get(1, 0):.2f}%\")\nelse:\n    print(\"数据中不存在'purchase_intention'列，需要定义目标变量\")\n\n# 2. 特征工程\nprint(\"2. 特征工程\")\n\n# 处理类别型变量\nif 'gender' in df.columns and df['gender'].dtype == 'object':\n    df['gender'] = df['gender'].map({'男': 0, '女': 1})\n\n# 处理缺失值\nmissing_values = df.isnull().sum()\nif missing_values.sum() > 0:\n    print(\"处理缺失值:\")\n    print(missing_values[missing_values > 0])\n    \n    # 使用简单方法填充缺失值\n    for col in df.columns:\n        if df[col].isnull().sum() > 0:\n            if df[col].dtype in ['int64', 'float64']:\n                df[col] = df[col].fillna(df[col].median())\n            else:\n                df[col] = df[col].fillna(df[col].mode()[0])\n\n# 创建新特征\nprint(\"创建新特征:\")\n# 收入等级\ndf['income_level'] = pd.cut(df['income'], \n                           bins=[0, 5000, 10000, 20000, float('inf')], \n                           labels=[0, 1, 2, 3])\n\n# 年龄段\ndf['age_group'] = pd.cut(df['age'], \n                        bins=[0, 30, 45, 60, float('inf')], \n                        labels=[0, 1, 2, 3])\n\n# 交互特征\nif 'website_visits' in df.columns and 'app_usage' in df.columns:\n    df['digital_engagement'] = df['website_visits'] + df['app_usage']\n    print(\"- 添加数字渠道参与度特征 (website_visits + app_usage)\")\n\nif 'chronic_disease' in df.columns and 'age' in df.columns:\n    df['health_risk_score'] = df['chronic_disease'] * (df['age'] / 50)\n    print(\"- 添加健康风险评分特征 (chronic_disease * age/50)\")\n\n# 3. 特征选择\nprint(\"3. 特征选择\")\n\n# 选择最终用于模型的特征\ncategorical_features = ['gender', 'income_level', 'age_group', 'smoker', 'has_insurance']\ncategorical_features = [f for f in categorical_features if f in df.columns]\n\nnumerical_features = ['age', 'income', 'bmi', 'chronic_disease', \n                     'website_visits', 'app_usage', 'call_center_calls',\n                     'digital_engagement', 'health_risk_score']\nnumerical_features = [f for f in numerical_features if f in df.columns]\n\nprint(f\"分类特征: {categorical_features}\")\nprint(f\"数值特征: {numerical_features}\")\n\n# 所有特征\nfeatures = categorical_features + numerical_features\nX = df[features]\ny = df['purchase_intention']\n\n# 4. 数据集划分\nprint(\"4. 数据集划分\")\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\nprint(f\"训练集: {X_train.shape[0]} 样本\")\nprint(f\"测试集: {X_test.shape[0]} 样本\")\n\n# 5. 模型构建\nprint(\"5. 模型构建\")\n\n# 5.1 随机森林\nprint(\"5.1 构建随机森林模型\")\nrf_pipeline = Pipeline([\n    ('scaler', StandardScaler()),\n    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))\n])\n\n# 5.2 逻辑回归\nprint(\"5.2 构建逻辑回归模型\")\nlr_pipeline = Pipeline([\n    ('scaler', StandardScaler()),\n    ('classifier', LogisticRegression(random_state=42, max_iter=1000))\n])\n\n# 5.3 梯度提升\nprint(\"5.3 构建梯度提升模型\")\ngb_pipeline = Pipeline([\n    ('scaler', StandardScaler()),\n    ('classifier', GradientBoostingClassifier(n_estimators=100, random_state=42))\n])\n\n# 5.4 XGBoost\nprint(\"5.4 构建XGBoost模型\")\ntry:\n    xgb_pipeline = Pipeline([\n        ('scaler', StandardScaler()),\n        ('classifier', xgb.XGBClassifier(n_estimators=100, random_state=42))\n    ])\nexcept:\n    print(\"XGBoost不可用，跳过此模型\")\n    xgb_pipeline = None\n\n# 6. 模型训练\nprint(\"6. 模型训练\")\n\n# 训练随机森林\nprint(\"训练随机森林...\")\nrf_pipeline.fit(X_train, y_train)\n\n# 训练逻辑回归\nprint(\"训练逻辑回归...\")\nlr_pipeline.fit(X_train, y_train)\n\n# 训练梯度提升\nprint(\"训练梯度提升...\")\ngb_pipeline.fit(X_train, y_train)\n\n# 训练XGBoost\nif xgb_pipeline:\n    print(\"训练XGBoost...\")\n    try:\n        xgb_pipeline.fit(X_train, y_train)\n    except Exception as e:\n        print(f\"XGBoost训练失败: {e}\")\n        xgb_pipeline = None\n\n# 7. 模型评估\nprint(\"7. 模型评估\")\n\n# 7.1 随机森林评估\nprint(\"7.1 随机森林模型评估\")\ny_pred_rf = rf_pipeline.predict(X_test)\ny_prob_rf = rf_pipeline.predict_proba(X_test)[:, 1]\n\nrf_accuracy = accuracy_score(y_test, y_pred_rf)\nrf_precision = precision_score(y_test, y_pred_rf)\nrf_recall = recall_score(y_test, y_pred_rf)\nrf_f1 = f1_score(y_test, y_pred_rf)\nrf_auc = roc_auc_score(y_test, y_prob_rf)\n\nprint(f\"准确率(Accuracy): {rf_accuracy:.4f}\")\nprint(f\"精确率(Precision): {rf_precision:.4f}\")\nprint(f\"召回率(Recall): {rf_recall:.4f}\")\nprint(f\"F1分数: {rf_f1:.4f}\")\nprint(f\"ROC AUC: {rf_auc:.4f}\")\n\n# 7.2 逻辑回归评估\nprint(\"7.2 逻辑回归模型评估\")\ny_pred_lr = lr_pipeline.predict(X_test)\ny_prob_lr = lr_pipeline.predict_proba(X_test)[:, 1]\n\nlr_accuracy = accuracy_score(y_test, y_pred_lr)\nlr_precision = precision_score(y_test, y_pred_lr)\nlr_recall = recall_score(y_test, y_pred_lr)\nlr_f1 = f1_score(y_test, y_pred_lr)\nlr_auc = roc_auc_score(y_test, y_prob_lr)\n\nprint(f\"准确率(Accuracy): {lr_accuracy:.4f}\")\nprint(f\"精确率(Precision): {lr_precision:.4f}\")\nprint(f\"召回率(Recall): {lr_recall:.4f}\")\nprint(f\"F1分数: {lr_f1:.4f}\")\nprint(f\"ROC AUC: {lr_auc:.4f}\")\n\n# 7.3 梯度提升评估\nprint(\"7.3 梯度提升模型评估\")\ny_pred_gb = gb_pipeline.predict(X_test)\ny_prob_gb = gb_pipeline.predict_proba(X_test)[:, 1]\n\ngb_accuracy = accuracy_score(y_test, y_pred_gb)\ngb_precision = precision_score(y_test, y_pred_gb)\ngb_recall = recall_score(y_test, y_pred_gb)\ngb_f1 = f1_score(y_test, y_pred_gb)\ngb_auc = roc_auc_score(y_test, y_prob_gb)\n\nprint(f\"准确率(Accuracy): {gb_accuracy:.4f}\")\nprint(f\"精确率(Precision): {gb_precision:.4f}\")\nprint(f\"召回率(Recall): {gb_recall:.4f}\")\nprint(f\"F1分数: {gb_f1:.4f}\")\nprint(f\"ROC AUC: {gb_auc:.4f}\")\n\n# 7.4 XGBoost评估\nif xgb_pipeline:\n    print(\"7.4 XGBoost模型评估\")\n    try:\n        y_pred_xgb = xgb_pipeline.predict(X_test)\n        y_prob_xgb = xgb_pipeline.predict_proba(X_test)[:, 1]\n        \n        xgb_accuracy = accuracy_score(y_test, y_pred_xgb)\n        xgb_precision = precision_score(y_test, y_pred_xgb)\n        xgb_recall = recall_score(y_test, y_pred_xgb)\n        xgb_f1 = f1_score(y_test, y_pred_xgb)\n        xgb_auc = roc_auc_score(y_test, y_prob_xgb)\n        \n        print(f\"准确率(Accuracy): {xgb_accuracy:.4f}\")\n        print(f\"精确率(Precision): {xgb_precision:.4f}\")\n        print(f\"召回率(Recall): {xgb_recall:.4f}\")\n        print(f\"F1分数: {xgb_f1:.4f}\")\n        print(f\"ROC AUC: {xgb_auc:.4f}\")\n    except Exception as e:\n        print(f\"XGBoost评估失败: {e}\")\n\n# 8. 特征重要性分析\nprint(\"8. 特征重要性分析\")\n\n# 8.1 随机森林特征重要性\nprint(\"8.1 随机森林特征重要性\")\nrf_importances = rf_pipeline.named_steps['classifier'].feature_importances_\nfeature_importance = pd.DataFrame({\n    'feature': X.columns,\n    'importance': rf_importances\n}).sort_values('importance', ascending=False)\n\nprint(\"前10个重要特征:\")\nprint(feature_importance.head(10))\n\nplt.figure(figsize=(10, 6))\nsns.barplot(x='importance', y='feature', data=feature_importance.head(10))\nplt.title('随机森林模型特征重要性')\nplt.tight_layout()\nplt.show()\n\n# 9. 模型优化\nprint(\"9. 模型优化\")\n# 选择表现最好的模型进行优化\nbest_auc = max(rf_auc, lr_auc, gb_auc)\nbest_model = None\n\nif rf_auc == best_auc:\n    print(\"随机森林表现最佳，进行优化...\")\n    best_model = RandomForestClassifier(\n        n_estimators=150,\n        max_depth=10,\n        min_samples_leaf=2,\n        random_state=42\n    )\nelif lr_auc == best_auc:\n    print(\"逻辑回归表现最佳，进行优化...\")\n    best_model = LogisticRegression(\n        C=0.8,\n        penalty='l2',\n        solver='liblinear',\n        random_state=42\n    )\nelse:\n    print(\"梯度提升表现最佳，进行优化...\")\n    best_model = GradientBoostingClassifier(\n        n_estimators=150,\n        max_depth=5,\n        learning_rate=0.1,\n        random_state=42\n    )\n\n# 构建优化模型的Pipeline\noptimized_pipeline = Pipeline([\n    ('scaler', StandardScaler()),\n    ('classifier', best_model)\n])\n\n# 训练优化后的模型\noptimized_pipeline.fit(X_train, y_train)\n\n# 评估优化后的模型\ny_pred_opt = optimized_pipeline.predict(X_test)\ny_prob_opt = optimized_pipeline.predict_proba(X_test)[:, 1]\n\nopt_accuracy = accuracy_score(y_test, y_pred_opt)\nopt_precision = precision_score(y_test, y_pred_opt)\nopt_recall = recall_score(y_test, y_pred_opt)\nopt_f1 = f1_score(y_test, y_pred_opt)\nopt_auc = roc_auc_score(y_test, y_prob_opt)\n\nprint(\"优化后的模型评估:\")\nprint(f\"准确率(Accuracy): {opt_accuracy:.4f}\")\nprint(f\"精确率(Precision): {opt_precision:.4f}\")\nprint(f\"召回率(Recall): {opt_recall:.4f}\")\nprint(f\"F1分数: {opt_f1:.4f}\")\nprint(f\"ROC AUC: {opt_auc:.4f}\")\n\n# 10. 保存预测结果\nprint(\"10. 保存预测结果\")\ndf_test = X_test.copy()\ndf_test['actual_intention'] = y_test\ndf_test['predicted_intention'] = y_pred_opt\ndf_test['purchase_probability'] = y_prob_opt\n\nprint(f\"测试集预测结果示例 (前5行):\")\nprint(df_test.head())\n\ntry:\n    df_test.to_csv('medical_insurance_predictions.csv', index=False)\n    print(\"预测结果已保存至 'medical_insurance_predictions.csv'\")\nexcept Exception as e:\n    print(f\"保存预测结果失败: {e}\")\n\nprint(\"医疗保险购买意向预测分析完成!\")\n"
        },
        {
          "id": 4,
          "title": "分群特征分析",
          "description": "分析各群体的特征和行为",
          "example_code": "# 医疗保险精准营销分析 - 分群特征分析\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nimport sklearn\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.inspection import permutation_importance\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\nimport warnings\n\n# 设置中文显示\nimport matplotlib as mpl\nmpl.rcParams['font.sans-serif'] = ['SimHei']  # 指定默认字体\nmpl.rcParams['axes.unicode_minus'] = False    # 解决保存图像是负号'-'显示为方块的问题\n\n# 忽略警告\nwarnings.filterwarnings('ignore')\n\n# 加载客户分群数据\nprint(\"加载客户分群数据...\")\ntry:\n    df = pd.read_csv('medical_insurance_clustered.csv')\n    print(f\"分群数据加载成功，共{len(df)}条记录。\")\nexcept FileNotFoundError:\n    print(\"未找到分群数据，尝试加载处理后的数据...\")\n    try:\n        df = pd.read_csv('medical_insurance_processed.csv')\n        print(f\"处理后的数据加载成功，共{len(df)}条记录。\n注意：此数据没有分群信息，无法进行完整的分群特征分析。\")\n    except FileNotFoundError:\n        print(\"示例：数据文件不存在，创建模拟数据以供演示\")\n        # 创建模拟数据\n        np.random.seed(42)\n        n_samples = 1000\n        \n        # 创建基本人口统计学特征\n        age = np.random.normal(45, 15, n_samples).astype(int)\n        age = np.clip(age, 18, 80)  # 限制年龄范围\n        \n        gender = np.random.choice(['男', '女'], n_samples)\n        income = np.random.normal(10000, 5000, n_samples)\n        \n        # 健康状况\n        bmi = np.random.normal(24, 4, n_samples)\n        smoker = np.random.choice([0, 1], n_samples, p=[0.8, 0.2])\n        chronic_disease = np.random.choice([0, 1, 2, 3], n_samples, p=[0.6, 0.2, 0.15, 0.05])\n        \n        # 保险相关\n        has_insurance = np.random.choice([0, 1], n_samples, p=[0.3, 0.7])\n        premium = np.zeros(n_samples)\n        premium[has_insurance == 1] = np.random.normal(2000, 800, sum(has_insurance))\n        \n        # 客户行为\n        website_visits = np.random.poisson(3, n_samples)\n        app_usage = np.random.choice([0, 1, 2, 3, 4, 5], n_samples, p=[0.4, 0.2, 0.15, 0.1, 0.1, 0.05])\n        call_center_calls = np.random.poisson(1, n_samples)\n        \n        # 营销响应\n        email_response = np.random.choice([0, 1], n_samples, p=[0.7, 0.3])\n        sms_response = np.random.choice([0, 1], n_samples, p=[0.8, 0.2])\n        \n        # 购买意向和价值\n        purchase_intention = np.random.choice([0, 1, 2, 3, 4], n_samples, p=[0.3, 0.3, 0.2, 0.1, 0.1])\n        customer_value = income * 0.01 + purchase_intention * 500 + np.random.normal(0, 500, n_samples)\n        \n        # 客户分群（模拟4个客户群体）\n        cluster = np.random.choice([0, 1, 2, 3], n_samples, p=[0.3, 0.3, 0.2, 0.2])\n        cluster_name = np.array(['价值猎手', '忠诚客户', '年轻潜力客户', '高收入低参与'])[cluster]\n        \n        # 创建DataFrame\n        df = pd.DataFrame({\n            'age': age,\n            'gender': gender,\n            'income': income,\n            'bmi': bmi,\n            'smoker': smoker,\n            'chronic_disease': chronic_disease,\n            'has_insurance': has_insurance,\n            'premium': premium,\n            'website_visits': website_visits,\n            'app_usage': app_usage,\n            'call_center_calls': call_center_calls,\n            'email_response': email_response,\n            'sms_response': sms_response,\n            'purchase_intention': purchase_intention,\n            'customer_value': customer_value,\n            'cluster': cluster,\n            'cluster_name': cluster_name\n        })\n        \n        print(f\"创建模拟数据成功，共{len(df)}条记录。\")\n\n# 检查数据中是否包含聚类结果\nif 'cluster' not in df.columns:\n    print(\"数据中不包含聚类标签，请先进行客户分群分析。\")\n    # 创建一个简单的分群，仅供演示\n    from sklearn.cluster import KMeans\n    \n    # 选择用于分群的特征\n    clustering_features = [\n        'age', 'income', 'bmi', 'smoker', 'chronic_disease',\n        'website_visits', 'app_usage', 'call_center_calls',\n        'email_response', 'sms_response'\n    ]\n    if 'purchase_intention' in df.columns:\n        clustering_features.append('purchase_intention')\n    \n    # 确保所有选定特征都在数据集中\n    clustering_features = [f for f in clustering_features if f in df.columns]\n    \n    # 处理分类变量\n    if 'gender' in df.columns and df['gender'].dtype == 'object':\n        df['gender'] = df['gender'].map({'男': 0, '女': 1})\n    \n    # 准备数据和标准化\n    X = df[clustering_features].copy()\n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(X)\n    \n    # 执行KMeans聚类\n    kmeans = KMeans(n_clusters=4, random_state=42, n_init=10)\n    df['cluster'] = kmeans.fit_predict(X_scaled)\n    \n    # 创建聚类名称\n    cluster_names = {\n        0: \"价值猎手\",\n        1: \"忠诚客户\",\n        2: \"年轻潜力客户\",\n        3: \"高收入低参与\"\n    }\n    df['cluster_name'] = df['cluster'].map(cluster_names)\n    print(\"已自动创建聚类标签用于演示。\")\n\n# 确保类型正确\nif 'gender' in df.columns and df['gender'].dtype == 'object':\n    df['gender'] = df['gender'].map({'男': 0, '女': 1})\n    \n# 检查并创建高购买意向指标（如果需要）\nif 'high_purchase_intention' not in df.columns and 'purchase_intention' in df.columns:\n    threshold = 2\n    df['high_purchase_intention'] = (df['purchase_intention'] > threshold).astype(int)\nelif 'high_purchase_intention' not in df.columns and 'has_insurance' in df.columns:\n    df['high_purchase_intention'] = df['has_insurance']\n\n# 1. 聚类分布概览\nprint(\"\n1. 聚类分布概览\")\ncluster_counts = df['cluster'].value_counts().sort_index()\ncluster_percent = (cluster_counts / len(df) * 100).round(1)\n\n# 构建聚类概览表\ncluster_overview = pd.DataFrame({\n    '客户数量': cluster_counts,\n    '占比(%)': cluster_percent\n})\nif 'cluster_name' in df.columns:\n    cluster_names = df.groupby('cluster')['cluster_name'].first()\n    cluster_overview.index = [f\"{i} - {cluster_names[i]}\" for i in cluster_overview.index]\n\nprint(cluster_overview)\n\n# 可视化聚类分布\nplt.figure(figsize=(12, 6))\nbars = plt.bar(range(len(cluster_counts)), cluster_counts, color='skyblue')\nplt.title('客户群体分布')\nplt.xlabel('客户群体')\nplt.ylabel('客户数量')\nplt.grid(axis='y', linestyle='--', alpha=0.7)\n\n# 添加标签\nif 'cluster_name' in df.columns:\n    plt.xticks(range(len(cluster_names)), [f\"{i} - {name}\" for i, name in cluster_names.items()], rotation=45, ha='right')\nelse:\n    plt.xticks(range(len(cluster_counts)), cluster_counts.index)\n\n# 添加数值标签\nfor i, (count, percent) in enumerate(zip(cluster_counts, cluster_percent)):\n    plt.text(i, count + 5, f\"{count} ({percent}%)\", ha='center')\n\nplt.tight_layout()\nplt.show()\n\n# 2. 各分群的人口统计特征对比\nprint(\"\n2. 各分群的人口统计特征对比\")\n\n# 2.1 年龄分布\nif 'age' in df.columns:\n    print(\"\n2.1 年龄分布比较\")\n    \n    # 计算各群体的年龄统计指标\n    age_stats = df.groupby('cluster')['age'].agg(['mean', 'median', 'std', 'min', 'max']).round(1)\n    if 'cluster_name' in df.columns:\n        age_stats.index = [f\"{i} - {cluster_names[i]}\" for i in age_stats.index]\n    print(age_stats)\n    \n    # 可视化年龄分布\n    plt.figure(figsize=(12, 6))\n    sns.boxplot(x='cluster', y='age', data=df, palette='viridis')\n    plt.title('各客户群体的年龄分布')\n    plt.xlabel('客户群体')\n    plt.ylabel('年龄')\n    \n    if 'cluster_name' in df.columns:\n        unique_clusters = sorted(df['cluster'].unique())\n        plt.xticks(range(len(unique_clusters)), \n                  [f\"{i} - {df[df['cluster']==i]['cluster_name'].iloc[0]}\" for i in unique_clusters])\n    \n    plt.grid(axis='y', linestyle='--', alpha=0.7)\n    plt.tight_layout()\n    plt.show()\n    \n    # 用方差分析检验各群体年龄差异的显著性\n    try:\n        anova_age = ols('age ~ C(cluster)', data=df).fit()\n        anova_table_age = sm.stats.anova_lm(anova_age, typ=2)\n        print(\"\n年龄方差分析结果:\")\n        print(anova_table_age)\n        \n        if anova_table_age['PR(>F)'][0] < 0.05:\n            print(\"分析结果：各客户群体的年龄差异具有统计学显著性\")\n        else:\n            print(\"分析结果：各客户群体的年龄差异不具有统计学显著性\")\n    except:\n        print(\"方差分析失败\")\n\n# 2.2 性别分布\nif 'gender' in df.columns:\n    print(\"\n2.2 性别分布比较\")\n    \n    # 计算各群体的性别分布\n    if df['gender'].dtype == 'object':\n        gender_pivot = pd.crosstab(df['cluster'], df['gender'], normalize='index') * 100\n    else:\n        # 如果gender已编码为数字（0=女性，1=男性）\n        df['gender_cat'] = df['gender'].map({0: '女', 1: '男'})\n        gender_pivot = pd.crosstab(df['cluster'], df['gender_cat'], normalize='index') * 100\n    \n    if 'cluster_name' in df.columns:\n        gender_pivot.index = [f\"{i} - {cluster_names[i]}\" for i in gender_pivot.index]\n    \n    print(gender_pivot.round(1))\n    \n    # 可视化性别分布\n    plt.figure(figsize=(12, 6))\n    \n    if df['gender'].dtype == 'object':\n        gender_counts = df.groupby(['cluster', 'gender']).size().unstack(fill_value=0)\n    else:\n        gender_counts = df.groupby(['cluster', 'gender_cat']).size().unstack(fill_value=0)\n    \n    gender_counts.plot(kind='bar', stacked=True, color=['lightpink', 'lightblue'])\n    plt.title('各客户群体的性别分布')\n    plt.xlabel('客户群体')\n    plt.ylabel('客户数量')\n    plt.legend(title='性别')\n    \n    if 'cluster_name' in df.columns:\n        unique_clusters = sorted(df['cluster'].unique())\n        plt.xticks(range(len(unique_clusters)), \n                  [f\"{i} - {df[df['cluster']==i]['cluster_name'].iloc[0]}\" for i in unique_clusters],\n                  rotation=45, ha='right')\n    \n    plt.grid(axis='y', linestyle='--', alpha=0.7)\n    plt.tight_layout()\n    plt.show()\n    \n    # 卡方检验分析各群体性别差异的显著性\n    try:\n        if df['gender'].dtype == 'object':\n            gender_crosstab = pd.crosstab(df['cluster'], df['gender'])\n        else:\n            gender_crosstab = pd.crosstab(df['cluster'], df['gender_cat'])\n        \n        chi2, p, dof, expected = stats.chi2_contingency(gender_crosstab)\n        \n        print(f\"\n性别分布卡方检验结果: chi2={chi2:.2f}, p={p:.4f}\")\n        \n        if p < 0.05:\n            print(\"分析结果：各客户群体的性别分布差异具有统计学显著性\")\n        else:\n            print(\"分析结果：各客户群体的性别分布差异不具有统计学显著性\")\n    except:\n        print(\"卡方检验失败\")\n\n# 2.3 收入分布\nif 'income' in df.columns:\n    print(\"\n2.3 收入分布比较\")\n    \n    # 计算各群体的收入统计指标\n    income_stats = df.groupby('cluster')['income'].agg(['mean', 'median', 'std', 'min', 'max']).round(1)\n    if 'cluster_name' in df.columns:\n        income_stats.index = [f\"{i} - {cluster_names[i]}\" for i in income_stats.index]\n    print(income_stats)\n    \n    # 可视化收入分布\n    plt.figure(figsize=(12, 6))\n    sns.boxplot(x='cluster', y='income', data=df, palette='viridis')\n    plt.title('各客户群体的收入分布')\n    plt.xlabel('客户群体')\n    plt.ylabel('收入')\n    \n    if 'cluster_name' in df.columns:\n        unique_clusters = sorted(df['cluster'].unique())\n        plt.xticks(range(len(unique_clusters)), \n                  [f\"{i} - {df[df['cluster']==i]['cluster_name'].iloc[0]}\" for i in unique_clusters])\n    \n    plt.grid(axis='y', linestyle='--', alpha=0.7)\n    plt.tight_layout()\n    plt.show()\n    \n    # 用方差分析检验各群体收入差异的显著性\n    try:\n        anova_income = ols('income ~ C(cluster)', data=df).fit()\n        anova_table_income = sm.stats.anova_lm(anova_income, typ=2)\n        print(\"\n收入方差分析结果:\")\n        print(anova_table_income)\n        \n        if anova_table_income['PR(>F)'][0] < 0.05:\n            print(\"分析结果：各客户群体的收入差异具有统计学显著性\")\n        else:\n            print(\"分析结果：各客户群体的收入差异不具有统计学显著性\")\n    except:\n        print(\"方差分析失败\")\n"
        },
        {
          "id": 5,
          "title": "营销建议生成",
          "description": "为各客户群体生成营销建议",
          "example_code": "# 医疗保险精准营销分析 - 营销建议生成\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nimport matplotlib as mpl\nimport warnings\n\n# 设置中文显示\nmpl.rcParams['font.sans-serif'] = ['SimHei']  # 指定默认字体\nmpl.rcParams['axes.unicode_minus'] = False    # 解决保存图像是负号'-'显示为方块的问题\n\n# 忽略警告\nwarnings.filterwarnings('ignore')\n\n# 加载客户分群和购买意向预测数据\nprint(\"加载客户分群和预测数据...\")\ntry:\n    df = pd.read_csv('medical_insurance_predictions.csv')\n    print(f\"预测数据加载成功，共{len(df)}条记录。\")\nexcept FileNotFoundError:\n    print(\"未找到预测数据，尝试加载分群数据...\")\n    try:\n        df = pd.read_csv('medical_insurance_clustered.csv')\n        print(f\"分群数据加载成功，共{len(df)}条记录。\")\n    except FileNotFoundError:\n        print(\"示例：数据文件不存在，创建模拟数据以供演示\")\n        # 创建模拟数据\n        np.random.seed(42)\n        n_samples = 1000\n        \n        # 创建基本人口统计学特征\n        age = np.random.normal(45, 15, n_samples).astype(int)\n        age = np.clip(age, 18, 80)  # 限制年龄范围\n        \n        gender = np.random.choice(['男', '女'], n_samples)\n        income = np.random.normal(10000, 5000, n_samples)\n        \n        # 健康状况\n        bmi = np.random.normal(24, 4, n_samples)\n        smoker = np.random.choice([0, 1], n_samples, p=[0.8, 0.2])\n        chronic_disease = np.random.choice([0, 1, 2, 3], n_samples, p=[0.6, 0.2, 0.15, 0.05])\n        \n        # 保险相关\n        has_insurance = np.random.choice([0, 1], n_samples, p=[0.3, 0.7])\n        premium = np.zeros(n_samples)\n        premium[has_insurance == 1] = np.random.normal(2000, 800, sum(has_insurance))\n        \n        # 客户行为\n        website_visits = np.random.poisson(3, n_samples)\n        app_usage = np.random.choice([0, 1, 2, 3, 4, 5], n_samples, p=[0.4, 0.2, 0.15, 0.1, 0.1, 0.05])\n        call_center_calls = np.random.poisson(1, n_samples)\n        \n        # 营销响应\n        email_response = np.random.choice([0, 1], n_samples, p=[0.7, 0.3])\n        sms_response = np.random.choice([0, 1], n_samples, p=[0.8, 0.2])\n        \n        # 购买意向和价值\n        purchase_intention = np.random.choice([0, 1, 2, 3, 4], n_samples, p=[0.3, 0.3, 0.2, 0.1, 0.1])\n        high_purchase_intention = (purchase_intention >= 2).astype(int)\n        purchase_probability = np.clip(0.1 + 0.2 * purchase_intention + np.random.normal(0, 0.1, n_samples), 0, 1)\n        customer_value = income * 0.01 + purchase_intention * 500 + np.random.normal(0, 500, n_samples)\n        \n        # 客户分群（模拟4个客户群体）\n        cluster = np.random.choice([0, 1, 2, 3], n_samples, p=[0.3, 0.3, 0.2, 0.2])\n        cluster_names = ['价值猎手', '忠诚客户', '年轻潜力客户', '高收入低参与']\n        cluster_name = np.array(cluster_names)[cluster]\n        \n        # 创建DataFrame\n        df = pd.DataFrame({\n            'age': age,\n            'gender': gender,\n            'income': income,\n            'bmi': bmi,\n            'smoker': smoker,\n            'chronic_disease': chronic_disease,\n            'has_insurance': has_insurance,\n            'premium': premium,\n            'website_visits': website_visits,\n            'app_usage': app_usage,\n            'call_center_calls': call_center_calls,\n            'email_response': email_response,\n            'sms_response': sms_response,\n            'purchase_intention': purchase_intention,\n            'high_purchase_intention': high_purchase_intention,\n            'purchase_probability': purchase_probability,\n            'customer_value': customer_value,\n            'cluster': cluster,\n            'cluster_name': cluster_name\n        })\n        \n        print(f\"创建模拟数据成功，共{len(df)}条记录。\")\n\n# 1. 数据准备\nprint(\"\n1. 数据准备\")\n\n# 确保类型正确\nif 'gender' in df.columns and df['gender'].dtype == 'object':\n    df['gender'] = df['gender'].map({'男': 0, '女': 1})\n\n# 确保有分群和购买概率\nif 'cluster' not in df.columns:\n    print(\"数据中不包含聚类标签，无法生成针对性的营销建议。\")\n    exit(1)\n\n# 如果没有购买概率，则基于已有特征创建\nif 'purchase_probability' not in df.columns:\n    if 'high_purchase_intention' not in df.columns and 'purchase_intention' in df.columns:\n        df['high_purchase_intention'] = (df['purchase_intention'] >= 2).astype(int)\n    elif 'high_purchase_intention' not in df.columns and 'has_insurance' in df.columns:\n        df['high_purchase_intention'] = df['has_insurance']\n    \n    # 使用RandomForest生成概率\n    if 'high_purchase_intention' in df.columns:\n        # 选择特征\n        features = ['age', 'income', 'cluster']\n        features = [f for f in features if f in df.columns]\n        \n        # 准备数据\n        X = df[features]\n        y = df['high_purchase_intention']\n        \n        # 训练简单模型\n        clf = RandomForestClassifier(n_estimators=100, random_state=42)\n        clf.fit(X, y)\n        \n        # 生成概率\n        df['purchase_probability'] = clf.predict_proba(X)[:, 1]\n        print(\"已基于现有特征生成购买概率。\")\n\n# 2. 营销价值矩阵创建\nprint(\"\n2. 创建营销价值矩阵\")\n\n# 2.1 定义价值分组\nif 'customer_value' in df.columns:\n    # 创建价值五分位数\n    df['value_segment'] = pd.qcut(df['customer_value'], q=5, labels=['极低价值', '低价值', '中等价值', '高价值', '极高价值'])\n    print(\"已基于客户价值创建价值分群。\")\nelse:\n    # 如果没有客户价值，则基于收入创建\n    df['value_segment'] = pd.qcut(df['income'], q=5, labels=['极低价值', '低价值', '中等价值', '高价值', '极高价值'])\n    print(\"已基于客户收入创建价值分群（客户价值不可用）。\")\n\n# 2.2 定义购买概率分组\ndf['probability_segment'] = pd.cut(df['purchase_probability'], bins=[0, 0.3, 0.6, 1], \n                                  labels=['低概率', '中等概率', '高概率'])\n\n# 2.3 创建价值-概率矩阵\nprint(\"\n价值-概率分布矩阵:\")\nvalue_prob_matrix = pd.crosstab(df['value_segment'], df['probability_segment'], normalize='all') * 100\nprint(value_prob_matrix.round(1))\n\n# 可视化价值-概率矩阵\nplt.figure(figsize=(10, 8))\nsns.heatmap(value_prob_matrix, annot=True, cmap='YlGnBu', fmt='.1f')\nplt.title('客户价值-购买概率分布矩阵 (%)')\nplt.xlabel('购买概率')\nplt.ylabel('客户价值')\nplt.tight_layout()\nplt.show()\n\n# 2.4 定义营销策略矩阵\nprint(\"\n2.4 定义营销策略矩阵\")\nmarketing_matrix = pd.DataFrame(\n    index=['极低价值', '低价值', '中等价值', '高价值', '极高价值'],\n    columns=['低概率', '中等概率', '高概率']\n)\n\n# 定义不同细分市场的营销策略\nmarketing_matrix.loc['极低价值', '低概率'] = '低成本触达/保持观察'\nmarketing_matrix.loc['极低价值', '中等概率'] = '基础产品推广/教育培养'\nmarketing_matrix.loc['极低价值', '高概率'] = '入门级产品推广'\n\nmarketing_matrix.loc['低价值', '低概率'] = '基础宣传/提高认知'\nmarketing_matrix.loc['低价值', '中等概率'] = '针对性促销活动'\nmarketing_matrix.loc['低价值', '高概率'] = '性价比产品推荐'\n\nmarketing_matrix.loc['中等价值', '低概率'] = '品牌曝光/认知培养'\nmarketing_matrix.loc['中等价值', '中等概率'] = '中端产品促销/特别优惠'\nmarketing_matrix.loc['中等价值', '高概率'] = '积极跟进/提供解决方案'\n\nmarketing_matrix.loc['高价值', '低概率'] = '关系培养/定制内容'\nmarketing_matrix.loc['高价值', '中等概率'] = '高端产品介绍/专属顾问'\nmarketing_matrix.loc['高价值', '高概率'] = '重点跟进/全面方案'\n\nmarketing_matrix.loc['极高价值', '低概率'] = '一对一顾问/价值展示'\nmarketing_matrix.loc['极高价值', '中等概率'] = 'VIP服务/专属定制'\nmarketing_matrix.loc['极高价值', '高概率'] = '最高优先级/全套解决方案'\n\nprint(\"\n营销策略矩阵:\")\nprint(marketing_matrix)\n\n# 3. 各客户群体的营销策略分析\nprint(\"\n3. 各客户群体的营销策略分析\")\n\n# 3.1 各客户群体的客户分布情况\nfor cluster_id in sorted(df['cluster'].unique()):\n    cluster_data = df[df['cluster'] == cluster_id]\n    cluster_name = cluster_data['cluster_name'].iloc[0] if 'cluster_name' in cluster_data.columns else f\"群体 {cluster_id}\"\n    \n    print(f\"\n客户群体 {cluster_id}: {cluster_name} (共{len(cluster_data)}客户, {len(cluster_data)/len(df)*100:.1f}%)\")\n    \n    # 价值-概率分布\n    value_prob = pd.crosstab(\n        cluster_data['value_segment'], \n        cluster_data['probability_segment'], \n        normalize='all'\n    ) * 100\n    \n    # 显示前3个最重要的细分市场和对应的营销策略\n    cross_tab = pd.crosstab(\n        cluster_data['value_segment'], \n        cluster_data['probability_segment']\n    )\n    cross_tab_flat = cross_tab.stack().reset_index()\n    cross_tab_flat.columns = ['价值分群', '概率分群', '客户数量']\n    top_segments = cross_tab_flat.sort_values('客户数量', ascending=False).head(3)\n    \n    print(\"主要客户细分市场及营销策略:\")\n    for i, (_, row) in enumerate(top_segments.iterrows()):\n        value_seg = row['价值分群']\n        prob_seg = row['概率分群']\n        count = row['客户数量']\n        percent = count / len(cluster_data) * 100\n        strategy = marketing_matrix.loc[value_seg, prob_seg]\n        \n        print(f\"{i+1}. {value_seg}-{prob_seg}: {count}客户 ({percent:.1f}%)\")\n        print(f\"   策略: {strategy}\")\n    \n    # 可视化该群体的价值-概率分布\n    plt.figure(figsize=(10, 8))\n    cluster_matrix = pd.crosstab(\n        cluster_data['value_segment'], \n        cluster_data['probability_segment'], \n        normalize='all'\n    ) * 100\n    \n    sns.heatmap(cluster_matrix, annot=True, cmap='YlGnBu', fmt='.1f')\n    plt.title(f'客户群体 {cluster_id}: {cluster_name} - 价值-概率分布 (%)')\n    plt.xlabel('购买概率')\n    plt.ylabel('客户价值')\n    plt.tight_layout()\n    plt.show()\n\n# 4. 定制化营销渠道和内容建议\nprint(\"\n4. 定制化营销渠道和内容建议\")\n\n# 4.1 渠道偏好分析\nchannel_preference = {}\n\n# 如果有渠道反应数据，分析渠道偏好\nif all(col in df.columns for col in ['email_response', 'sms_response']):\n    for cluster_id in sorted(df['cluster'].unique()):\n        cluster_data = df[df['cluster'] == cluster_id]\n        cluster_name = cluster_data['cluster_name'].iloc[0] if 'cluster_name' in cluster_data.columns else f\"群体 {cluster_id}\"\n        \n        # 计算渠道反应率\n        email_response_rate = cluster_data['email_response'].mean() * 100\n        sms_response_rate = cluster_data['sms_response'].mean() * 100\n        \n        # 记录偏好\n        channel_preference[cluster_id] = {\n            'name': cluster_name,\n            'email': email_response_rate,\n            'sms': sms_response_rate\n        }\n        \n        # 如果有网站访问和APP使用数据，也加入分析\n        if 'website_visits' in cluster_data.columns:\n            avg_web_visits = cluster_data['website_visits'].mean()\n            channel_preference[cluster_id]['web'] = avg_web_visits\n            \n        if 'app_usage' in cluster_data.columns:\n            avg_app_usage = cluster_data['app_usage'].mean()\n            channel_preference[cluster_id]['app'] = avg_app_usage\n    \n    # 创建渠道偏好数据框\n    if channel_preference:\n        channel_df = pd.DataFrame.from_dict(channel_preference, orient='index')\n        \n        # 可视化渠道偏好\n        plt.figure(figsize=(12, 6))\n        \n        # 绘制邮件和短信反应率\n        x = range(len(channel_df))\n        width = 0.35\n        \n        plt.bar([i - width/2 for i in x], channel_df['email'], width=width, label='邮件', color='skyblue')\n        plt.bar([i + width/2 for i in x], channel_df['sms'], width=width, label='短信', color='lightcoral')\n        \n        plt.title('各客户群体的渠道反应率')\n        plt.xlabel('客户群体')\n        plt.ylabel('反应率 (%)')\n        plt.xticks(x, [f\"{i} - {row['name']}\" for i, row in channel_df.iterrows()])\n        plt.legend()\n        plt.grid(axis='y', linestyle='--', alpha=0.7)\n        plt.tight_layout()\n        plt.show()\n        \n        # 打印渠道建议\n        print(\"\n各客户群体的渠道建议:\")\n        for cluster_id, data in channel_preference.items():\n            print(f\"\n客户群体 {cluster_id}: {data['name']}\")\n            \n            channels = []\n            if 'email' in data and 'sms' in data:\n                if data['email'] > data['sms']:\n                    channels.append(f\"邮件 (反应率: {data['email']:.1f}%)\")\n                else:\n                    channels.append(f\"短信 (反应率: {data['sms']:.1f}%)\")\n            \n            if 'web' in data and 'app' in data:\n                if data['web'] > data['app']:\n                    channels.append(f\"网站 (平均访问: {data['web']:.1f}次)\")\n                else:\n                    channels.append(f\"移动应用 (平均使用: {data['app']:.1f}次)\")\n            \n            print(f\"推荐渠道: {', '.join(channels)}\")\n\n# 5. 客户群体营销内容建议\nprint(\"\n5. 客户群体营销内容建议\")\n\n# 为每个客户群体提供定制化的营销内容建议\ncluster_marketing_content = {}\n\n# 价值猎手\ncluster_marketing_content[0] = {\n    'name': '价值猎手',\n    'key_messages': [\n        '突出医疗保险的成本效益和价值',\n        '提供保险产品的比较分析',\n        '强调长期健康保障的经济效益'\n    ],\n    'offers': [\n        '提供定制化的保险方案和价格比较',\n        '入门级保险产品的特别折扣',\n        '长期客户的价格保障承诺'\n    ],\n    'channels': [\n        '电子邮件营销（价格比较和折扣信息）',\n        '网站内容（成本计算器和产品比较）',\n        '短信提醒（限时优惠）'\n    ]\n}\n\n# 忠诚客户\ncluster_marketing_content[1] = {\n    'name': '忠诚客户',\n    'key_messages': [\n        '感谢客户长期的信任与支持',\n        '提供全面的家庭健康保障方案',\n        '专属会员服务和特权'\n    ],\n    'offers': [\n        '忠诚客户专属保险产品升级选项',\n        '家庭保险套餐折扣',\n        'VIP健康咨询服务'\n    ],\n    'channels': [\n        '个人客户经理定期联系',\n        '高端客户活动邀请',\n        '定制化邮件营销'\n    ]\n}\n\n# 年轻潜力客户\ncluster_marketing_content[2] = {\n    'name': '年轻潜力客户',\n    'key_messages': [\n        '适合年轻人的灵活保障方案',\n        '强调预防保健和健康生活方式',\n        '数字化服务和便捷管理'\n    ],\n    'offers': [\n        '年轻人专属入门级保险产品',\n        '健康生活方式奖励计划',\n        '灵活的保障选项和支付方式'\n    ],\n    'channels': [\n        '社交媒体营销',\n        '移动应用推广',\n        '在线直播健康讲座'\n    ]\n}\n\n# 高收入低参与\ncluster_marketing_content[3] = {\n    'name': '高收入低参与',\n    'key_messages': [\n        '高端定制医疗保障方案',\n        '全球医疗网络和绿色通道服务',\n        '专业健康管理和家庭医生服务'\n    ],\n    'offers': [\n        '高端医疗保险定制方案',\n        '全球紧急医疗救援服务',\n        '家庭健康管家服务'\n    ],\n    'channels': [\n        '高端会员活动',\n        '一对一专属顾问',\n        '精英俱乐部和健康讲座'\n    ]\n}\n\n# 打印营销内容建议\nfor cluster_id in sorted(df['cluster'].unique()):\n    if cluster_id in cluster_marketing_content:\n        content = cluster_marketing_content[cluster_id]\n        print(f\"\n客户群体 {cluster_id}: {content['name']}\")\n        \n        print(\"核心信息:\")\n        for i, msg in enumerate(content['key_messages']):\n            print(f\"  {i+1}. {msg}\")\n        \n        print(\"产品/优惠:\")\n        for i, offer in enumerate(content['offers']):\n            print(f\"  {i+1}. {offer}\")\n        \n        print(\"推荐渠道:\")\n        for i, channel in enumerate(content['channels']):\n            print(f\"  {i+1}. {channel}\")\n\n# 6. 营销活动设计建议\nprint(\"\n6. 营销活动设计建议\")\n\n# 打印针对不同客户群体的营销活动建议\nmarketing_campaigns = [\n    {\n        'title': '精明保障计划',\n        'target_clusters': [0],  # 价值猎手\n        'description': '针对价值猎手客户群体的高性价比保险产品推广活动',\n        'key_elements': [\n            '提供医疗保险成本效益分析工具',\n            '开展保险产品比较展示活动',\n            '限时价格优惠和长期价格锁定承诺'\n        ],\n        'channels': [\n            '电子邮件系列营销',\n            '网站专题页面',\n            '在线保险顾问咨询'\n        ],\n        'kpis': [\n            '点击率和转化率',\n            '成本计算器使用次数',\n            '咨询转化率'\n        ]\n    },\n    {\n        'title': '全家守护计划',\n        'target_clusters': [1],  # 忠诚客户\n        'description': '面向忠诚客户的家庭保险升级和交叉销售活动',\n        'key_elements': [\n            '家庭成员额外保障专属优惠',\n            'VIP健康管理服务体验',\n            '忠诚客户回馈活动'\n        ],\n        'channels': [\n            '客户经理一对一联系',\n            'VIP客户专属活动',\n            '会员专刊'\n        ],\n        'kpis': [\n            '产品升级率',\n            '交叉销售成功率',\n            '客户满意度'\n        ]\n    },\n    {\n        'title': '青年先锋保障',\n        'target_clusters': [2],  # 年轻潜力客户\n        'description': '针对年轻客户群体的数字化保险产品推广',\n        'key_elements': [\n            '灵活定制的入门级保险产品',\n            '健康生活方式奖励计划',\n            '数字化服务体验'\n        ],\n        'channels': [\n            '社交媒体营销',\n            '移动应用推广',\n            '线上线下健康活动'\n        ],\n        'kpis': [\n            '社交媒体互动率',\n            '应用程序下载和使用率',\n            '年轻客户群转化率'\n        ]\n    },\n    {\n        'title': '尊享全球医疗',\n        'target_clusters': [3],  # 高收入低参与\n        'description': '针对高收入客户的高端医疗保险方案推广',\n        'key_elements': [\n            '全球医疗网络和绿色通道服务',\n            '一对一健康管理顾问',\n            '高端定制保险方案'\n        ],\n        'channels': [\n            '高端客户活动',\n            '专属顾问拜访',\n            '高端生活方式媒体合作'\n        ],\n        'kpis': [\n            '高净值客户参与率',\n            '高端方案转化率',\n            '客户生命周期价值'\n        ]\n    },\n    {\n        'title': '健康家庭保障',\n        'target_clusters': [0, 1, 2],  # 跨群体活动\n        'description': '面向多个客户群体的家庭健康保险推广活动',\n        'key_elements': [\n            '不同层次的家庭保险套餐',\n            '家庭成员附加保障优惠',\n            '健康家庭激励计划'\n        ],\n        'channels': [\n            '多渠道整合营销',\n            '家庭健康讲座',\n            '社区健康活动'\n        ],\n        'kpis': [\n            '家庭保单转化率',\n            '客户参与度',\n            '活动投资回报率'\n        ]\n    }\n]\n\n# 打印营销活动建议\nfor i, campaign in enumerate(marketing_campaigns):\n    print(f\"\n营销活动 {i+1}: {campaign['title']}\")\n    print(f\"目标客户群体: {', '.join([cluster_marketing_content[c]['name'] for c in campaign['target_clusters']])}\")\n    print(f\"描述: {campaign['description']}\")\n    \n    print(\"关键要素:\")\n    for j, element in enumerate(campaign['key_elements']):\n        print(f\"  {j+1}. {element}\")\n    \n    print(\"推荐渠道:\")\n    for j, channel in enumerate(campaign['channels']):\n        print(f\"  {j+1}. {channel}\")\n    \n    print(\"关键绩效指标:\")\n    for j, kpi in enumerate(campaign['kpis']):\n        print(f\"  {j+1}. {kpi}\")\n\n# 7. 执行计划和绩效评估建议\nprint(\"\n7. 执行计划和绩效评估建议\")\n\nprint(\"营销策略执行步骤:\")\nprint(\"1. 客户数据分析和细分 (已完成)\")\nprint(\"2. 针对各客户群体制定营销策略 (已完成)\")\nprint(\"3. 营销资源分配优先级:\")\nprint(\"   - 第一优先级: 高价值-高概率客户\")\nprint(\"   - 第二优先级: 高价值-中概率和极高价值-低概率客户\")\nprint(\"   - 第三优先级: 中等价值-高概率客户\")\nprint(\"   - 第四优先级: 其他客户群体\")\nprint(\"4. 营销活动实施时间表:\")\nprint(\"   - 短期(1-3个月): 高优先级客户群体的针对性营销\")\nprint(\"   - 中期(3-6个月): 扩展到次优先级客户群体\")\nprint(\"   - 长期(6-12个月): 全面覆盖并优化营销策略\")\n\nprint(\"\n营销绩效评估框架:\")\nprint(\"1. 客户响应指标:\")\nprint(\"   - 邮件打开率和点击率\")\nprint(\"   - 网站/APP访问转化率\")\nprint(\"   - 咨询请求数量\")\nprint(\"2. 销售绩效指标:\")\nprint(\"   - 新保单签署率\")\nprint(\"   - 产品升级率\")\nprint(\"   - 客户获取成本\")\nprint(\"   - 客户生命周期价值\")\nprint(\"3. 客户关系指标:\")\nprint(\"   - 客户满意度\")\nprint(\"   - 客户保留率\")\nprint(\"   - 推荐率\")\nprint(\"4. 营销投资回报率(ROI)计算:\")\nprint(\"   - 营销活动成本\")\nprint(\"   - 销售收入增长\")\nprint(\"   - 长期客户价值\")\n\n# 8. 保存营销建议\nprint(\"\n8. 保存营销建议\")\ntry:\n    # 为每个客户添加推荐的营销策略\n    df['value_prob_segment'] = df['value_segment'] + '-' + df['probability_segment']\n    df['recommended_strategy'] = df.apply(\n        lambda row: marketing_matrix.loc[row['value_segment'], row['probability_segment']], axis=1\n    )\n    \n    # 保存结果\n    df.to_csv('medical_insurance_marketing_recommendations.csv', index=False)\n    print(\"营销建议已保存至 'medical_insurance_marketing_recommendations.csv'\")\nexcept Exception as e:\n    print(f\"保存营销建议失败: {e}\")\n\nprint(\"\n医疗保险精准营销建议生成完成!\")\n"
        },
        {
          "id": 6,
          "title": "营销效果评估",
          "description": "评估针对不同群体的营销策略效果",
          "example_code": "# 医疗保险精准营销分析 - 营销效果评估\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime, timedelta\nimport matplotlib as mpl\nimport warnings\n\n# 设置中文显示\nmpl.rcParams['font.sans-serif'] = ['SimHei']  # 指定默认字体\nmpl.rcParams['axes.unicode_minus'] = False    # 解决保存图像是负号'-'显示为方块的问题\n\n# 忽略警告\nwarnings.filterwarnings('ignore')\n\n# 营销活动效果追踪分析\nprint(\"开始分析营销活动效果...\")\n\n# 创建模拟活动效果数据\nprint(\"创建模拟营销活动效果数据...\")\nnp.random.seed(42)\n\n# 模拟日期范围 - 近90天\nend_date = datetime.now().date()\nstart_date = end_date - timedelta(days=90)\ndate_range = pd.date_range(start=start_date, end=end_date, freq='D')\n\n# 客户群体配置\nclusters = {\n    0: {'name': '价值猎手', 'campaign': '精明保障计划', 'baseline_conversion': 0.05},\n    1: {'name': '忠诚客户', 'campaign': '全家守护计划', 'baseline_conversion': 0.08},\n    2: {'name': '年轻潜力客户', 'campaign': '青年先锋保障', 'baseline_conversion': 0.04},\n    3: {'name': '高收入低参与', 'campaign': '尊享全球医疗', 'baseline_conversion': 0.03}\n}\n\n# 营销渠道\nchannels = ['电子邮件', '短信', '网站', '手机应用', '客户经理', '社交媒体']\n\n# 创建数据框\ndata = []\n\n# 在不同日期为不同客户群添加记录\nfor date in date_range:\n    # 随机波动系数 - 模拟一些趋势和季节性\n    day_factor = 1.0 + 0.1 * np.sin(date.dayofyear / 30.0 * np.pi)\n    weekend_factor = 0.8 if date.dayofweek >= 5 else 1.0 \n    \n    # 每个客户群体的每日数据\n    for cluster_id, config in clusters.items():\n        # 基线转化率加随机波动\n        base_conversion = config['baseline_conversion'] * day_factor * weekend_factor\n        \n        # 添加活动启动后的效果提升\n        # 假设活动在30天前启动\n        campaign_start = end_date - timedelta(days=30)\n        if date.date() >= campaign_start:\n            # 不同客户群对活动的响应不同\n            if cluster_id == 0:  # 价值猎手对价格敏感\n                campaign_effect = 1.5 * (1 - np.exp(-(date.date() - campaign_start).days / 10))\n            elif cluster_id == 1:  # 忠诚客户反应更快更稳定\n                campaign_effect = 1.3 * (1 - np.exp(-(date.date() - campaign_start).days / 5))\n            elif cluster_id == 2:  # 年轻客户反应快但波动大\n                campaign_effect = 1.8 * (1 - np.exp(-(date.date() - campaign_start).days / 7)) * (0.8 + 0.4 * np.random.random())\n            else:  # 高收入低参与客户反应较慢\n                campaign_effect = 1.2 * (1 - np.exp(-(date.date() - campaign_start).days / 15))\n        else:\n            campaign_effect = 1.0\n            \n        # 各渠道响应数据\n        for channel in channels:\n            # 不同渠道对不同客户群的效果不同\n            if channel == '电子邮件':\n                channel_factor = 1.2 if cluster_id in [0, 1] else 0.8\n            elif channel == '短信':\n                channel_factor = 1.1 if cluster_id in [0, 2] else 0.9\n            elif channel == '网站':\n                channel_factor = 1.3 if cluster_id == 0 else 1.0\n            elif channel == '手机应用':\n                channel_factor = 1.5 if cluster_id == 2 else 0.7\n            elif channel == '客户经理':\n                channel_factor = 1.8 if cluster_id in [1, 3] else 0.5\n            else:  # 社交媒体\n                channel_factor = 1.6 if cluster_id == 2 else 0.6\n            \n            # 总体效果计算\n            effect = base_conversion * campaign_effect * channel_factor\n            \n            # 曝光量 - 不同群体规模不同\n            cluster_size_factor = {0: 1000, 1: 800, 2: 1200, 3: 500}[cluster_id]\n            exposures = int(cluster_size_factor * (0.8 + 0.4 * np.random.random()))\n            \n            # 点击/互动\n            ctr = effect * (0.05 + 0.05 * np.random.random())  # 点击率\n            clicks = int(exposures * ctr)\n            \n            # 转化\n            conversion_rate = effect * (0.01 + 0.02 * np.random.random())  # 转化率\n            conversions = int(clicks * conversion_rate)\n            \n            # 收入\n            avg_premium = {0: 2000, 1: 3500, 2: 1500, 3: 5000}[cluster_id]\n            revenue = conversions * avg_premium * (0.8 + 0.4 * np.random.random())\n            \n            # 成本 - 不同渠道成本不同\n            channel_cost_factor = {\n                '电子邮件': 5, '短信': 8, '网站': 15, \n                '手机应用': 12, '客户经理': 50, '社交媒体': 20\n            }[channel]\n            cost = exposures * channel_cost_factor / 100  # 每次曝光的成本\n            \n            # ROI\n            roi = (revenue - cost) / cost if cost > 0 else 0\n            \n            # 添加记录\n            data.append({\n                'date': date,\n                'cluster_id': cluster_id,\n                'cluster_name': config['name'],\n                'campaign': config['campaign'],\n                'channel': channel,\n                'exposures': exposures,\n                'clicks': clicks,\n                'conversions': conversions,\n                'ctr': ctr,\n                'conversion_rate': conversion_rate,\n                'revenue': revenue,\n                'cost': cost,\n                'roi': roi\n            })\n\n# 创建数据框\ntracking_df = pd.DataFrame(data)\nprint(f\"已创建模拟效果追踪数据: {len(tracking_df)}条记录\")\n\n# 1. 营销活动总体效果分析\nprint(\"\n1. 营销活动总体效果分析\")\n\n# 汇总数据 - 按活动和客户群\ncampaign_summary = tracking_df.groupby(['campaign', 'cluster_name']).agg({\n    'exposures': 'sum',\n    'clicks': 'sum',\n    'conversions': 'sum',\n    'revenue': 'sum',\n    'cost': 'sum'\n}).reset_index()\n\n# 计算汇总指标\ncampaign_summary['ctr'] = campaign_summary['clicks'] / campaign_summary['exposures']\ncampaign_summary['conversion_rate'] = campaign_summary['conversions'] / campaign_summary['clicks']\ncampaign_summary['roi'] = (campaign_summary['revenue'] - campaign_summary['cost']) / campaign_summary['cost']\n\n# 显示汇总结果\nprint(\"\n营销活动总体效果:\")\nprint(campaign_summary[['campaign', 'cluster_name', 'exposures', 'conversions', 'revenue', 'cost', 'roi']])\n\n# 可视化活动ROI比较\nplt.figure(figsize=(10, 6))\nsns.barplot(x='campaign', y='roi', hue='cluster_name', data=campaign_summary)\nplt.title('各营销活动投资回报率(ROI)比较')\nplt.xlabel('营销活动')\nplt.ylabel('ROI')\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n\n# 2. 时间序列分析 - 活动效果趋势\nprint(\"\n2. 时间序列分析 - 活动效果趋势\")\n\n# 按日期和活动汇总\ndaily_trend = tracking_df.groupby(['date', 'campaign']).agg({\n    'conversions': 'sum',\n    'revenue': 'sum',\n    'cost': 'sum'\n}).reset_index()\n\ndaily_trend['roi'] = (daily_trend['revenue'] - daily_trend['cost']) / daily_trend['cost']\n\n# 可视化转化趋势\nplt.figure(figsize=(12, 6))\nfor campaign in daily_trend['campaign'].unique():\n    campaign_data = daily_trend[daily_trend['campaign'] == campaign]\n    plt.plot(campaign_data['date'], campaign_data['conversions'], label=campaign)\n\nplt.title('各营销活动每日转化趋势')\nplt.xlabel('日期')\nplt.ylabel('转化数')\nplt.legend()\nplt.grid(True, linestyle='--', alpha=0.7)\nplt.tight_layout()\nplt.show()\n\n# 3. 渠道效果比较\nprint(\"\n3. 渠道效果比较\")\n\n# 按渠道汇总\nchannel_summary = tracking_df.groupby('channel').agg({\n    'exposures': 'sum',\n    'clicks': 'sum',\n    'conversions': 'sum',\n    'revenue': 'sum',\n    'cost': 'sum'\n}).reset_index()\n\nchannel_summary['ctr'] = channel_summary['clicks'] / channel_summary['exposures']\nchannel_summary['conversion_rate'] = channel_summary['conversions'] / channel_summary['clicks']\nchannel_summary['cpa'] = channel_summary['cost'] / channel_summary['conversions']  # 每次转化成本\nchannel_summary['roi'] = (channel_summary['revenue'] - channel_summary['cost']) / channel_summary['cost']\n\n# 显示渠道效果\nprint(\"\n各渠道效果比较:\")\nprint(channel_summary[['channel', 'exposures', 'ctr', 'conversion_rate', 'cpa', 'roi']])\n\n# 可视化渠道ROI和CPA比较\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n\n# ROI比较\nsns.barplot(x='channel', y='roi', data=channel_summary.sort_values('roi', ascending=False), ax=ax1)\nax1.set_title('各渠道投资回报率(ROI)比较')\nax1.set_xlabel('渠道')\nax1.set_ylabel('ROI')\nax1.set_xticklabels(ax1.get_xticklabels(), rotation=45)\n\n# CPA比较\nsns.barplot(x='channel', y='cpa', data=channel_summary.sort_values('cpa'), ax=ax2)\nax2.set_title('各渠道获客成本(CPA)比较')\nax2.set_xlabel('渠道')\nax2.set_ylabel('CPA (元)')\nax2.set_xticklabels(ax2.get_xticklabels(), rotation=45)\n\nplt.tight_layout()\nplt.show()\n\n# 4. 客户群体响应分析\nprint(\"\n4. 客户群体响应分析\")\n\n# 按客户群体汇总\ncluster_summary = tracking_df.groupby('cluster_name').agg({\n    'exposures': 'sum',\n    'clicks': 'sum',\n    'conversions': 'sum',\n    'revenue': 'sum',\n    'cost': 'sum'\n}).reset_index()\n\ncluster_summary['ctr'] = cluster_summary['clicks'] / cluster_summary['exposures']\ncluster_summary['conversion_rate'] = cluster_summary['conversions'] / cluster_summary['clicks']\ncluster_summary['roi'] = (cluster_summary['revenue'] - cluster_summary['cost']) / cluster_summary['cost']\ncluster_summary['arpu'] = cluster_summary['revenue'] / cluster_summary['conversions']  # 每用户平均收入\n\n# 显示客户群体响应\nprint(\"\n各客户群体响应比较:\")\nprint(cluster_summary)\n\n# 可视化客户群体对比\nfig, ax = plt.subplots(figsize=(12, 6))\n\nx = range(len(cluster_summary))\nwidth = 0.2\n\n# 绘制CTR, 转化率和ROI\nax.bar([i - width for i in x], cluster_summary['ctr'] * 100, width=width, label='点击率 (%)', color='skyblue')\nax.bar([i for i in x], cluster_summary['conversion_rate'] * 100, width=width, label='转化率 (%)', color='lightgreen')\nax.bar([i + width for i in x], cluster_summary['roi'], width=width, label='ROI', color='coral')\n\nax.set_title('各客户群体营销指标对比')\nax.set_xlabel('客户群体')\nax.set_ylabel('指标值')\nax.set_xticks(x)\nax.set_xticklabels(cluster_summary['cluster_name'])\nax.legend()\nax.grid(axis='y', linestyle='--', alpha=0.7)\n\nplt.tight_layout()\nplt.show()\n\n# 5. 渠道-客户群体交叉分析\nprint(\"\n5. 渠道-客户群体交叉分析\")\n\n# 渠道-客户群体交叉分析\ncross_analysis = tracking_df.groupby(['cluster_name', 'channel']).agg({\n    'conversions': 'sum',\n    'revenue': 'sum',\n    'cost': 'sum'\n}).reset_index()\n\ncross_analysis['roi'] = (cross_analysis['revenue'] - cross_analysis['cost']) / cross_analysis['cost']\n\n# 创建渠道-客户群体ROI热力图\ncross_pivot = cross_analysis.pivot(index='cluster_name', columns='channel', values='roi')\n\nplt.figure(figsize=(12, 8))\nsns.heatmap(cross_pivot, annot=True, cmap='YlGnBu', fmt='.2f')\nplt.title('渠道-客户群体ROI热力图')\nplt.tight_layout()\nplt.show()\n\n# 6. 活动优化建议\nprint(\"\n6. 活动优化建议\")\n\n# 基于渠道分析的资源优化建议\nchannel_reallocation = channel_summary.sort_values('roi', ascending=False)\ntop_channels = channel_reallocation.head(3)['channel'].tolist()\nbottom_channels = channel_reallocation.tail(3)['channel'].tolist()\n\nprint(\"渠道资源优化建议:\")\nprint(f\"- 增加投资渠道: {', '.join(top_channels)}\")\nprint(f\"- 减少投资渠道: {', '.join(bottom_channels)}\")\n\n# 基于客户群体分析的目标优化\ncluster_reallocation = cluster_summary.sort_values('roi', ascending=False)\ntop_clusters = cluster_reallocation.head(2)['cluster_name'].tolist()\n\nprint(\"\n客户群体优化建议:\")\nprint(f\"- 增加针对客户群体的投资: {', '.join(top_clusters)}\")\n\n# 渠道-客户群体最佳匹配\nbest_matches = cross_analysis.sort_values('roi', ascending=False).head(5)\nprint(\"\n最佳渠道-客户群体匹配:\")\nfor i, row in best_matches.iterrows():\n    print(f\"- {row['cluster_name']} 通过 {row['channel']} 渠道 (ROI: {row['roi']:.2f})\")\n\n# 7. A/B测试建议\nprint(\"\n7. A/B测试建议\")\n\nprint(\"基于当前效果分析，建议进行以下A/B测试:\")\nprint(\"1. 消息内容测试:\")\nprint(\"   - 测试A: 强调产品功能和保障范围\")\nprint(\"   - 测试B: 强调价格优势和性价比\")\nprint(\"   - 适用客户群体: 价值猎手\")\n\nprint(\"\n2. 优惠策略测试:\")\nprint(\"   - 测试A: 直接价格折扣\")\nprint(\"   - 测试B: 增值服务捆绑\")\nprint(\"   - 适用客户群体: 忠诚客户\")\n\nprint(\"\n3. 渠道组合测试:\")\nprint(\"   - 测试A: 电子邮件+网站重定向\")\nprint(\"   - 测试B: 手机应用+社交媒体\")\nprint(\"   - 适用客户群体: 年轻潜力客户\")\n\nprint(\"\n4. 个性化程度测试:\")\nprint(\"   - 测试A: 基于客户历史行为的高度个性化内容\")\nprint(\"   - 测试B: 基于客户群体特征的标准化内容\")\nprint(\"   - 适用客户群体: 高收入低参与\")\n\n# 8. 长期效果预测\nprint(\"\n8. 长期效果预测\")\n\n# 基于当前趋势进行简单预测\n# 获取最近30天数据\nrecent_data = tracking_df[tracking_df['date'] >= pd.to_datetime(end_date) - pd.Timedelta(days=30)]\n\n# 按活动和日期汇总\nforecast_base = recent_data.groupby(['campaign', 'date']).agg({\n    'conversions': 'sum',\n    'revenue': 'sum'\n}).reset_index()\n\n# 计算每个活动的平均日增长率\ngrowth_rates = {}\nfor campaign in forecast_base['campaign'].unique():\n    campaign_data = forecast_base[forecast_base['campaign'] == campaign].sort_values('date')\n    if len(campaign_data) >= 2:\n        # 计算日均增长率\n        first_revenue = campaign_data['revenue'].iloc[0]\n        last_revenue = campaign_data['revenue'].iloc[-1]\n        days = (campaign_data['date'].iloc[-1] - campaign_data['date'].iloc[0]).days\n        if days > 0 and first_revenue > 0:\n            daily_growth = (last_revenue / first_revenue) ** (1/days) - 1\n            growth_rates[campaign] = daily_growth\n\nprint(\"基于当前趋势的未来90天收入预测:\")\nfor campaign, growth_rate in growth_rates.items():\n    latest_data = forecast_base[forecast_base['campaign'] == campaign].sort_values('date').iloc[-1]\n    latest_revenue = latest_data['revenue']\n    \n    # 预测90天后的收入\n    forecast_90d = latest_revenue * (1 + growth_rate) ** 90\n    print(f\"- {campaign}: 当前日收入 {latest_revenue:.2f}元, 预计90天后日收入 {forecast_90d:.2f}元\")\n\n# 9. 保存分析结果\nprint(\"\n9. 保存分析结果\")\ntry:\n    # 保存效果追踪数据\n    tracking_df.to_csv('medical_insurance_campaign_tracking.csv', index=False)\n    print(\"营销活动效果追踪数据已保存至 'medical_insurance_campaign_tracking.csv'\")\n    \n    # 保存汇总分析\n    summary = {\n        'campaign': campaign_summary,\n        'channel': channel_summary,\n        'cluster': cluster_summary\n    }\n    \n    with open('medical_insurance_campaign_analysis.json', 'w') as f:\n        json.dump({\n            'top_performing_campaigns': campaign_summary.sort_values('roi', ascending=False)['campaign'].tolist(),\n            'top_performing_channels': channel_summary.sort_values('roi', ascending=False)['channel'].tolist(),\n            'top_performing_clusters': cluster_summary.sort_values('roi', ascending=False)['cluster_name'].tolist(),\n            'optimization_suggestions': {\n                'increase_channels': top_channels,\n                'decrease_channels': bottom_channels,\n                'focus_clusters': top_clusters,\n                'best_matches': [\n                    {'cluster': row['cluster_name'], 'channel': row['channel'], 'roi': row['roi']}\n                    for _, row in best_matches.iterrows()\n                ]\n            }\n        }, indent=2)\n    print(\"营销活动分析结果已保存至 'medical_insurance_campaign_analysis.json'\")\nexcept Exception as e:\n    print(f\"保存分析结果失败: {e}\")\n\nprint(\"\n医疗保险精准营销效果评估分析完成!\")"
        }
      ]
    }
  ]
}