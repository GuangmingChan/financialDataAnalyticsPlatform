{
  "experiments": [
    {
      "id": 1,
      "title": "银行信贷风险控制",
      "description": "使用机器学习模型预测信用卡客户违约风险",
      "category": "bank",
      "difficulty_level": 3,
      "estimated_duration": 120,
      "prerequisites": "基本的数据分析和机器学习知识",
      "data_source": "台湾信用卡数据集",
      "steps": [
        {
          "id": 1,
          "title": "数据探索",
          "description": "探索信用卡数据集，了解特征和目标变量",
          "example_code": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n# 使用通用字体设置，避免中文字体问题\nimport matplotlib as mpl\nmpl.rcParams['font.family'] = 'DejaVu Sans'\nimport seaborn as sns\nfrom app.services.bank_analysis import bank_service\n\n# 加载数据\ndf = bank_service.load_credit_data()\n\n# 1. 基本统计量\nprint(\"=== Basic Statistics ===\")\nprint(df.describe())\n\n# 2. 数据类型和基本信息\nprint(\"\\n=== Data Types ===\")\nprint(df.info())\n\n# 3. 目标变量分布\ntarget_counts = df['DEFAULT'].value_counts()\nprint(\"\\n=== Target Distribution ===\")\nprint(f\"Non-Default: {target_counts[0]} ({target_counts[0]/len(df)*100:.2f}%)\")\nprint(f\"Default: {target_counts[1]} ({target_counts[1]/len(df)*100:.2f}%)\")\n\n# 4. 异常值统计\nprint(\"\\n=== Outlier Statistics ===\")\nnumeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\nfor col in numeric_cols:\n    Q1 = df[col].quantile(0.25)\n    Q3 = df[col].quantile(0.75)\n    IQR = Q3 - Q1\n    lower_bound = Q1 - 1.5 * IQR\n    upper_bound = Q3 + 1.5 * IQR\n    outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)][col]\n    if not outliers.empty:\n        print(f\"{col} has {len(outliers)} outliers ({len(outliers)/len(df)*100:.2f}%)\")\n\n# 5. 重复值统计\nduplicates = df.duplicated().sum()\nprint(f\"\\n=== Duplicate Statistics ===\")\nprint(f\"Dataset has {duplicates} duplicated rows ({duplicates/len(df)*100:.2f}%)\")\n\n# 6. 缺失值统计\nmissing_values = df.isnull().sum()\nprint(\"\\n=== Missing Value Statistics ===\")\nif missing_values.sum() == 0:\n    print(\"No missing values in the dataset\")\nelse:\n    print(missing_values[missing_values > 0])\n\n# 7. 可视化部分特征分布\nplt.figure(figsize=(15, 10))\n\n# 年龄分布\nplt.subplot(2, 3, 1)\nsns.histplot(df['AGE'], kde=True)\nplt.title('Age Distribution')\n\n# 信用额度分布\nplt.subplot(2, 3, 2)\nsns.histplot(df['LIMIT_BAL'], kde=True)\nplt.title('Credit Limit Distribution')\n\n# 违约率与年龄的关系\nplt.subplot(2, 3, 3)\nsns.boxplot(x='DEFAULT', y='AGE', data=df)\nplt.title('Default Rate vs Age')\nplt.xlabel('Default Status (0=No, 1=Yes)')\n\n# 相关性分析\nplt.figure(figsize=(12, 10))\ncorrelation_matrix = df.corr()\nsns.heatmap(correlation_matrix, annot=False, cmap='coolwarm', linewidths=0.5)\nplt.title('Feature Correlation Heatmap')\nplt.show() "
        },
        {
          "id": 2,
          "title": "数据预处理",
          "description": "处理缺失值和异常值，标准化特征",
          "example_code": "import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\n# 使用通用字体设置，避免中文字体问题\nimport matplotlib as mpl\nmpl.rcParams['font.family'] = 'DejaVu Sans'\nfrom app.services.bank_analysis import bank_service\n\n# 加载数据\ndf = bank_service.load_credit_data()\nprint(\"Original data shape:\", df.shape)\n\n# 创建原始数据的副本\ndf_cleaned = df.copy()\n\n# 1. 检查并处理重复值\nduplicates = df_cleaned.duplicated().sum()\nif duplicates > 0:\n    print(f\"Found {duplicates} duplicate rows, removing...\")\n    df_cleaned = df_cleaned.drop_duplicates()\n\n# 2. 处理缺失值\nmissing_values = df_cleaned.isnull().sum()\nif missing_values.sum() > 0:\n    print(f\"Found {missing_values.sum()} missing values, processing...\")\n    # 对数值型变量用中位数填充\n    numeric_cols = df_cleaned.select_dtypes(include=[np.number]).columns\n    for col in numeric_cols:\n        if df_cleaned[col].isnull().sum() > 0:\n            median_value = df_cleaned[col].median()\n            df_cleaned[col].fillna(median_value, inplace=True)\nelse:\n    print(\"No missing values in the dataset\")\n\n# 3. 处理异常值\nprint(\"\\nProcessing outliers...\")\nnumeric_cols = df_cleaned.select_dtypes(include=[np.number]).columns.tolist()\nnumeric_cols.remove('DEFAULT')  # 不处理目标变量\n\nfor col in numeric_cols:\n    Q1 = df_cleaned[col].quantile(0.25)\n    Q3 = df_cleaned[col].quantile(0.75)\n    IQR = Q3 - Q1\n    lower_bound = Q1 - 1.5 * IQR\n    upper_bound = Q3 + 1.5 * IQR\n    \n    # 统计异常值\n    outliers = df_cleaned[(df_cleaned[col] < lower_bound) | (df_cleaned[col] > upper_bound)][col]\n    \n    if len(outliers) > 0:\n        print(f\"  - {col} has {len(outliers)} outliers ({len(outliers)/len(df_cleaned)*100:.2f}%)\")\n        \n        # 将异常值替换为边界值\n        df_cleaned.loc[df_cleaned[col] < lower_bound, col] = lower_bound\n        df_cleaned.loc[df_cleaned[col] > upper_bound, col] = upper_bound\n\n# 4. 数据标准化\nprint(\"\\nStart data standardization...\")\n# 分离特征和标签\nX = df_cleaned.drop('DEFAULT', axis=1)\ny = df_cleaned['DEFAULT']\n\n# 对数值类型的特征进行标准化\nnumeric_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n\n# 使用StandardScaler进行标准化\nscaler = StandardScaler()\nX_scaled = X.copy()\nX_scaled[numeric_cols] = scaler.fit_transform(X[numeric_cols])\n\nprint(f\"Standardized {len(numeric_cols)} numerical features\")\n\n# 合并回DataFrame\ndf_standardized = pd.concat([X_scaled, y], axis=1)\n\n# 对比标准化前后的描述性统计\nprint(\"\\nStatistics before standardization:\")\nprint(df_cleaned[numeric_cols].describe())\nprint(\"\\nStatistics after standardization:\")\nprint(df_standardized[numeric_cols].describe())\n\nprint(f\"\\nCleaning completed! Original data: {df.shape}, After cleaning: {df_standardized.shape}\")\n\n# 可视化标准化前后\nplt.figure(figsize=(15, 6))\n\n# 标准化前\nplt.subplot(1, 2, 1)\nplt.boxplot(df_cleaned[['LIMIT_BAL', 'AGE', 'BILL_AMT1']])\nplt.title('Before Standardization')\nplt.xticks([1, 2, 3], ['LIMIT_BAL', 'AGE', 'BILL_AMT1'])\n\n# 标准化后\nplt.subplot(1, 2, 2)\nplt.boxplot(df_standardized[['LIMIT_BAL', 'AGE', 'BILL_AMT1']])\nplt.title('After Standardization')\nplt.xticks([1, 2, 3], ['LIMIT_BAL', 'AGE', 'BILL_AMT1'])\n\nplt.tight_layout()\nplt.show() "
        },
        {
          "id": 3,
          "title": "特征工程",
          "description": "创建新特征，选择重要特征",
          "example_code": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.feature_selection import SelectKBest, f_classif\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.decomposition import PCA\n# 使用通用字体设置，避免中文字体问题\nimport matplotlib as mpl\nmpl.rcParams['font.family'] = 'DejaVu Sans'\nfrom app.services.bank_analysis import bank_service\n\n# 加载数据\ndf = bank_service.load_credit_data()\n\n# 创建数据副本\ndf_fe = df.copy()\n\n# 1. 特征创建\nprint(\"Creating new features...\")\n\n# 计算平均账单金额\ndf_fe['AVG_BILL_AMT'] = (df_fe['BILL_AMT1'] + df_fe['BILL_AMT2'] + df_fe['BILL_AMT3'] + \n                         df_fe['BILL_AMT4'] + df_fe['BILL_AMT5'] + df_fe['BILL_AMT6']) / 6\n\n# 计算平均支付金额\ndf_fe['AVG_PAY_AMT'] = (df_fe['PAY_AMT1'] + df_fe['PAY_AMT2'] + df_fe['PAY_AMT3'] + \n                        df_fe['PAY_AMT4'] + df_fe['PAY_AMT5'] + df_fe['PAY_AMT6']) / 6\n\n# 计算账单与支付比率\ndf_fe['BILL_PAY_RATIO'] = df_fe['AVG_PAY_AMT'] / df_fe['AVG_BILL_AMT'].replace(0, 0.01)\n\n# 计算最大延迟\ndelay_cols = ['PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6']\ndf_fe['MAX_DELAY'] = df_fe[delay_cols].max(axis=1)\n\n# 创建延迟标志\ndf_fe['HAS_DELAY'] = (df_fe[delay_cols] > 0).any(axis=1).astype(int)\n\n# 计算信用额度使用率\ndf_fe['CREDIT_UTILIZATION'] = df_fe['BILL_AMT1'] / df_fe['LIMIT_BAL'].replace(0, 0.01)\n\n# 输出新特征的描述性统计\nprint(\"\\nNew features statistics:\")\nnew_features = ['AVG_BILL_AMT', 'AVG_PAY_AMT', 'BILL_PAY_RATIO', 'MAX_DELAY', 'HAS_DELAY', 'CREDIT_UTILIZATION']\nprint(df_fe[new_features].describe())\n\n# 2. 特征交互 (多项式特征)\nprint(\"\\nCreating polynomial features...\")\n# 选择部分数值特征\nnumeric_feat = ['LIMIT_BAL', 'AGE', 'AVG_BILL_AMT', 'AVG_PAY_AMT']\n\n# 创建二阶多项式特征\npoly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\npoly_features = poly.fit_transform(df_fe[numeric_feat])\n\n# 创建特征名称\npoly_feat_names = poly.get_feature_names_out(numeric_feat)\n\n# 添加多项式特征到数据集\npoly_df = pd.DataFrame(poly_features, columns=poly_feat_names)\npoly_df = poly_df.iloc[:, len(numeric_feat):]  # 移除原特征，只保留交互项\n\n# 重置索引并连接\ndf_fe.reset_index(drop=True, inplace=True)\npoly_df.reset_index(drop=True, inplace=True)\ndf_fe = pd.concat([df_fe, poly_df], axis=1)\n\nprint(f\"Added {poly_df.shape[1]} interaction features\")\n\n# 3. 特征选择\nprint(\"\\nPerforming feature selection...\")\n# 准备特征和目标变量\nX = df_fe.drop('DEFAULT', axis=1)\ny = df_fe['DEFAULT']\n\n# 使用ANOVA F-value进行特征选择\nselector = SelectKBest(f_classif, k=15)  # 选择前15个最重要的特征\nX_new = selector.fit_transform(X, y)\n\n# 获取特征得分\nfeature_scores = pd.DataFrame()\nfeature_scores['Feature'] = X.columns\nfeature_scores['Score'] = selector.scores_\nfeature_scores = feature_scores.sort_values(by='Score', ascending=False)\n\nprint(\"\\nTop 15 important features:\")\nprint(feature_scores.head(15))\n\n# 选择的特征\nselected_features = X.columns[selector.get_support()].tolist()\nprint(\"\\nSelected features:\", selected_features)\n\n# 使用选择的特征创建最终数据集\nX_selected = X[selected_features]\nfinal_df = pd.concat([X_selected, y], axis=1)\n\nprint(f\"\\nFinal dataset shape: {final_df.shape}\")\n\n# 4. 可视化重要特征的分布\nplt.figure(figsize=(15, 10))\n\n# 展示前6个最重要的特征\ntop_features = feature_scores['Feature'].head(6).tolist()\n\nfor i, feature in enumerate(top_features):\n    plt.subplot(2, 3, i+1)\n    sns.histplot(data=df_fe, x=feature, hue='DEFAULT', bins=20, kde=True, palette=['blue', 'red'])\n    plt.title(f'Feature: {feature}')\n    plt.xlabel(feature)\n    plt.ylabel('Frequency')\n\nplt.tight_layout()\nplt.show()\n\n# 5. 特征降维 (PCA)\nplt.figure(figsize=(12, 5))\n\n# 选择数值特征进行PCA\nnum_features = X.select_dtypes(include=['float64', 'int64']).columns.tolist()\nX_num = X[num_features]\n\n# 标准化数据\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X_num)\n\n# 应用PCA\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X_scaled)\n\n# 创建PCA数据框\npca_df = pd.DataFrame(X_pca, columns=['PC1', 'PC2'])\npca_df['DEFAULT'] = y.values\n\n# 可视化PCA结果\nplt.subplot(1, 2, 1)\nsns.scatterplot(data=pca_df, x='PC1', y='PC2', hue='DEFAULT', palette=['blue', 'red'])\nplt.title('PCA: First two principal components')\n\n# 计算特征对主成分的贡献\nloadings = pd.DataFrame(\n    pca.components_.T,\n    columns=['PC1', 'PC2'],\n    index=num_features\n)\n\n# 可视化特征负荷\nplt.subplot(1, 2, 2)\nsns.heatmap(loadings, cmap='viridis')\nplt.title('Feature contributions to PCs')\n\nplt.tight_layout()\nplt.show()\n\n# 输出解释方差比例\nprint(f\"\\nVariance explained by PC1: {pca.explained_variance_ratio_[0]:.2f}\")\nprint(f\"Variance explained by PC2: {pca.explained_variance_ratio_[1]:.2f}\")\nprint(f\"Total variance explained: {sum(pca.explained_variance_ratio_):.2f}\")"
        },
        {
          "id": 4,
          "title": "模型训练",
          "description": "训练多种机器学习模型来预测违约风险",
          "example_code": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# 使用通用字体设置，避免中文字体问题\nimport matplotlib as mpl\nmpl.rcParams['font.family'] = 'DejaVu Sans'\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.preprocessing import StandardScaler\nfrom app.services.bank_analysis import bank_service\n\n# 加载数据\ndf = bank_service.load_credit_data()\n\n# 数据预处理\nprint(\"Data preprocessing...\")\n# 准备特征和目标变量\nX = df.drop('DEFAULT', axis=1)\ny = df['DEFAULT']\n\n# 标准化特征\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# 划分训练集和测试集\nX_train, X_test, y_train, y_test = train_test_split(\n    X_scaled, y, test_size=0.25, random_state=42, stratify=y\n)\n\nprint(f\"Training set: {X_train.shape}, Test set: {X_test.shape}\")\n\n# 定义评估模型的函数\ndef evaluate_model(model, X_train, X_test, y_train, y_test):\n    # 训练模型\n    model.fit(X_train, y_train)\n    \n    # 预测\n    y_pred = model.predict(X_test)\n    \n    # 评估指标\n    accuracy = accuracy_score(y_test, y_pred)\n    report = classification_report(y_test, y_pred)\n    cm = confusion_matrix(y_test, y_pred)\n    \n    # ROC曲线\n    if hasattr(model, 'predict_proba'):\n        y_proba = model.predict_proba(X_test)[:, 1]\n        fpr, tpr, _ = roc_curve(y_test, y_proba)\n        roc_auc = auc(fpr, tpr)\n    else:\n        fpr, tpr, roc_auc = None, None, None\n    \n    return {\n        'model': model,\n        'accuracy': accuracy,\n        'report': report,\n        'confusion_matrix': cm,\n        'roc': (fpr, tpr, roc_auc)\n    }\n\n# 训练和评估多个模型\nprint(\"\\nTraining models...\")\n\n# 1. 逻辑回归\nprint(\"Training Logistic Regression...\")\nlr = LogisticRegression(max_iter=1000, class_weight='balanced')\nlr_result = evaluate_model(lr, X_train, X_test, y_train, y_test)\n\n# 2. 随机森林\nprint(\"Training Random Forest...\")\nrf = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\nrf_result = evaluate_model(rf, X_train, X_test, y_train, y_test)\n\n# 3. 梯度提升树\nprint(\"Training Gradient Boosting...\")\ngb = GradientBoostingClassifier(n_estimators=100, random_state=42)\ngb_result = evaluate_model(gb, X_train, X_test, y_train, y_test)\n\n# 4. 支持向量机\nprint(\"Training SVM...\")\nsvm = SVC(probability=True, class_weight='balanced')\nsvm_result = evaluate_model(svm, X_train, X_test, y_train, y_test)\n\n# 输出模型结果\nmodels = [lr_result, rf_result, gb_result, svm_result]\nmodel_names = ['Logistic Regression', 'Random Forest', 'Gradient Boosting', 'SVM']\n\nprint(\"\\nModel comparison:\")\nfor name, result in zip(model_names, models):\n    print(f\"\\n{name}:\")\n    print(f\"Accuracy: {result['accuracy']:.4f}\")\n    print(\"Classification Report:\")\n    print(result['report'])\n\n# 可视化比较模型性能\nprint(\"\\nVisualizing model performance...\")\n\n# 1. 准确率比较\naccuracies = [result['accuracy'] for result in models]\n\nplt.figure(figsize=(10, 6))\nplt.bar(model_names, accuracies, color=['blue', 'green', 'red', 'purple'])\nplt.title('Model Accuracy Comparison')\nplt.ylabel('Accuracy')\nplt.ylim(0.5, 1.0)  # 设置y轴范围，更好地显示差异\nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.show()\n\n# 2. 混淆矩阵\nplt.figure(figsize=(12, 10))\n\nfor i, (name, result) in enumerate(zip(model_names, models)):\n    plt.subplot(2, 2, i+1)\n    cm = result['confusion_matrix']\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n    plt.title(f'{name} Confusion Matrix')\n    plt.xlabel('Predicted')\n    plt.ylabel('Actual')\n\nplt.tight_layout()\nplt.show()\n\n# 3. ROC曲线比较\nplt.figure(figsize=(10, 8))\n\nfor name, result in zip(model_names, models):\n    fpr, tpr, roc_auc = result['roc']\n    if fpr is not None:\n        plt.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.3f})')\n\nplt.plot([0, 1], [0, 1], 'k--')  # 对角线\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curves Comparison')\nplt.legend(loc='lower right')\nplt.grid(alpha=0.3)\nplt.show()\n\n# 4. 特征重要性（仅针对随机森林模型）\nrandom_forest = rf_result['model']\n\n# 获取特征重要性\nfeature_importances = random_forest.feature_importances_\n\n# 创建DataFrame用于可视化\nimportance_df = pd.DataFrame({\n    'Feature': X.columns,\n    'Importance': feature_importances\n})\nimportance_df = importance_df.sort_values('Importance', ascending=False).head(15)\n\nplt.figure(figsize=(12, 6))\nsns.barplot(x='Importance', y='Feature', data=importance_df, palette='viridis')\nplt.title('Random Forest Feature Importance')\nplt.tight_layout()\nplt.show()\n\n# 超参数调优（以随机森林为例）\nprint(\"\\nPerforming hyperparameter tuning for Random Forest...\")\n\n# 定义参数网格\nparam_grid = {\n    'n_estimators': [50, 100],\n    'max_depth': [None, 10, 20],\n    'min_samples_split': [2, 5],\n    'min_samples_leaf': [1, 2]\n}\n\n# 使用网格搜索和交叉验证\ngrid_search = GridSearchCV(\n    RandomForestClassifier(random_state=42, class_weight='balanced'),\n    param_grid=param_grid,\n    cv=5,\n    scoring='accuracy',\n    n_jobs=-1\n)\n\ngrid_search.fit(X_train, y_train)\n\nprint(f\"Best parameters: {grid_search.best_params_}\")\nprint(f\"Best cross-validation score: {grid_search.best_score_:.4f}\")\n\n# 使用最佳参数的模型\nbest_rf = grid_search.best_estimator_\nbest_rf_result = evaluate_model(best_rf, X_train, X_test, y_train, y_test)\n\nprint(\"\\nOptimized Random Forest:\")\nprint(f\"Accuracy: {best_rf_result['accuracy']:.4f}\")\nprint(\"Classification Report:\")\nprint(best_rf_result['report'])\n\n# 保存训练好的模型\nprint(\"\\nModel training completed!\")\nprint(f\"Best model: Optimized Random Forest (Accuracy: {best_rf_result['accuracy']:.4f})\")"
        },
        {
          "id": 5,
          "title": "模型评估",
          "description": "评估模型性能，解释模型决策",
          "example_code": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# 使用通用字体设置，避免中文字体问题\nimport matplotlib as mpl\nmpl.rcParams['font.family'] = 'DejaVu Sans'\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\nfrom sklearn.metrics import confusion_matrix, roc_curve, precision_recall_curve, average_precision_score\nfrom sklearn.ensemble import RandomForestClassifier\nimport shap\nfrom app.services.bank_analysis import bank_service\n\n# 加载数据\ndf = bank_service.load_credit_data()\n\n# 数据预处理\nprint(\"Preprocessing data...\")\nX = df.drop('DEFAULT', axis=1)\ny = df['DEFAULT']\n\n# 标准化数据\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# 划分数据集\nX_train, X_test, y_train, y_test = train_test_split(\n    X_scaled, y, test_size=0.25, random_state=42, stratify=y\n)\n\n# 训练模型\nprint(\"Training model for evaluation...\")\nrf = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\nrf.fit(X_train, y_train)\n\n# 预测\ny_pred = rf.predict(X_test)\ny_proba = rf.predict_proba(X_test)[:, 1]\n\n# 计算各种评估指标\nprint(\"\\nCalculating evaluation metrics...\")\naccuracy = accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred)\nrecall = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\nroc_auc = roc_auc_score(y_test, y_proba)\navg_precision = average_precision_score(y_test, y_proba)\n\n# 打印评估指标\nprint(\"Model Performance Metrics:\")\nprint(f\"Accuracy: {accuracy:.4f}\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall: {recall:.4f}\")\nprint(f\"F1 Score: {f1:.4f}\")\nprint(f\"ROC AUC: {roc_auc:.4f}\")\nprint(f\"Average Precision: {avg_precision:.4f}\")\n\n# 混淆矩阵\ncm = confusion_matrix(y_test, y_pred)\n\n# 计算其他混淆矩阵指标\ntn, fp, fn, tp = cm.ravel()\ntotal = tn + fp + fn + tp\nprint(\"\\nConfusion Matrix Analysis:\")\nprint(f\"True Negatives: {tn} ({tn/total:.2%})\")\nprint(f\"False Positives: {fp} ({fp/total:.2%})\")\nprint(f\"False Negatives: {fn} ({fn/total:.2%})\")\nprint(f\"True Positives: {tp} ({tp/total:.2%})\")\n\n# 计算特定业务指标\nprint(\"\\nBusiness Metrics:\")\ncost_fp = 1  # 假阳性的成本（错误地识别为违约的成本）\ncost_fn = 5  # 假阴性的成本（错误地识别为非违约的成本，通常更高）\ntotal_cost = (fp * cost_fp) + (fn * cost_fn)\nprint(f\"Total Error Cost: {total_cost} (FP: {fp*cost_fp}, FN: {fn*cost_fn})\")\n\n# 可视化混淆矩阵\nplt.figure(figsize=(10, 8))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\nplt.title('Confusion Matrix')\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.show()\n\n# 交叉验证评估\nprint(\"\\nPerforming cross-validation...\")\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\ncv_scores_accuracy = cross_val_score(rf, X_scaled, y, cv=cv, scoring='accuracy')\ncv_scores_auc = cross_val_score(rf, X_scaled, y, cv=cv, scoring='roc_auc')\n\nprint(f\"CV Accuracy: {cv_scores_accuracy.mean():.4f} (+/- {cv_scores_accuracy.std()*2:.4f})\")\nprint(f\"CV ROC AUC: {cv_scores_auc.mean():.4f} (+/- {cv_scores_auc.std()*2:.4f})\")\n\n# ROC 曲线\nfpr, tpr, thresholds = roc_curve(y_test, y_proba)\n\nplt.figure(figsize=(10, 6))\nplt.plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc:.3f})')\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic (ROC) Curve')\nplt.legend(loc='lower right')\nplt.grid(alpha=0.3)\nplt.show()\n\n# Precision-Recall 曲线\nprecision_curve, recall_curve, _ = precision_recall_curve(y_test, y_proba)\n\nplt.figure(figsize=(10, 6))\nplt.plot(recall_curve, precision_curve, label=f'AP = {avg_precision:.3f}')\nplt.axhline(y=y_test.mean(), color='r', linestyle='--', label=f'Baseline: {y_test.mean():.3f}')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Precision-Recall Curve')\nplt.legend()\nplt.grid(alpha=0.3)\nplt.show()\n\n# 阈值分析\nthresholds_df = pd.DataFrame({\n    'Threshold': thresholds,\n    'TPR': tpr[:-1],\n    'FPR': fpr[:-1]\n})\n\n# 计算每个阈值的精确度和召回率\nthresholds_plot = np.arange(0.1, 1.0, 0.1)\nmetrics = []\n\nfor threshold in thresholds_plot:\n    y_pred_threshold = (y_proba >= threshold).astype(int)\n    \n    precision_t = precision_score(y_test, y_pred_threshold)\n    recall_t = recall_score(y_test, y_pred_threshold)\n    f1_t = f1_score(y_test, y_pred_threshold)\n    \n    metrics.append({\n        'Threshold': threshold,\n        'Precision': precision_t,\n        'Recall': recall_t,\n        'F1': f1_t\n    })\n\nmetrics_df = pd.DataFrame(metrics)\n\n# 绘制不同阈值下的指标\nplt.figure(figsize=(12, 6))\n\nplt.plot(metrics_df['Threshold'], metrics_df['Precision'], marker='o', label='Precision')\nplt.plot(metrics_df['Threshold'], metrics_df['Recall'], marker='s', label='Recall')\nplt.plot(metrics_df['Threshold'], metrics_df['F1'], marker='^', label='F1')\n\nplt.xlabel('Classification Threshold')\nplt.ylabel('Score')\nplt.title('Performance Metrics vs. Classification Threshold')\nplt.legend()\nplt.grid(alpha=0.3)\nplt.show()\n\n# Feature Importance and SHAP Analysis\nprint(\"\\nAnalyzing feature importance...\")\n\n# 获取特征重要性\nfeature_importances = rf.feature_importances_\n\n# 创建DataFrame并排序\nfeature_importance_df = pd.DataFrame({\n    'Feature': X.columns,\n    'Importance': feature_importances\n})\nfeature_importance_df = feature_importance_df.sort_values('Importance', ascending=False)\n\n# 绘制特征重要性\nplt.figure(figsize=(12, 6))\nsns.barplot(x='Importance', y='Feature', data=feature_importance_df.head(15))\nplt.title('Random Forest Feature Importance')\nplt.show()\n\n# SHAP值分析\nprint(\"\\nCalculating SHAP values (this may take a moment)...\")\n# 使用原始特征数据而不是标准化数据来计算SHAP值以提高可解释性\nX_sample = X.iloc[:100]  # 使用一个样本子集进行SHAP分析，以减少计算时间\nmodel_for_shap = RandomForestClassifier(n_estimators=100, random_state=42)\nmodel_for_shap.fit(X, y)  # 在完整数据上训练\n\n# 创建SHAP解释器\nexplainer = shap.TreeExplainer(model_for_shap)\nshap_values = explainer.shap_values(X_sample)\n\n# 绘制SHAP摘要图\nshap.summary_plot(shap_values[1], X_sample, feature_names=X.columns.tolist())\n\n# 绘制前3个特征的SHAP依赖图\ntop_features = feature_importance_df['Feature'].head(3).tolist()\nfor feature in top_features:\n    plt.figure()\n    shap.dependence_plot(feature, shap_values[1], X_sample, feature_names=X.columns.tolist())\n\n# 模型校准曲线\nfrom sklearn.calibration import calibration_curve\nplt.figure(figsize=(10, 6))\n\nprobability_true, probability_pred = calibration_curve(y_test, y_proba, n_bins=10)\nplt.plot(probability_pred, probability_true, marker='o', label='Random Forest')\nplt.plot([0, 1], [0, 1], 'k--', label='Perfectly Calibrated')\n\nplt.xlabel('Mean Predicted Probability')\nplt.ylabel('Fraction of Positives')\nplt.title('Calibration Curve')\nplt.legend()\nplt.grid(alpha=0.3)\nplt.show()\n\n# 输出总结和建议\nprint(\"\\nModel Evaluation Summary:\")\nprint(f\"Overall Performance: {accuracy:.2%} accuracy, {roc_auc:.2%} ROC AUC\")\n\n# 根据性能指标给出建议\nif roc_auc > 0.8:\n    print(\"The model shows good discriminative ability.\")\nelif roc_auc > 0.7:\n    print(\"The model shows acceptable discriminative ability, but could be improved.\")\nelse:\n    print(\"The model's discriminative ability is limited. Consider feature engineering or trying different algorithms.\")\n\nif precision < 0.7:\n    print(\"Consider increasing the classification threshold to improve precision, if required for business purposes.\")\n\nif recall < 0.7:\n    print(\"Consider decreasing the classification threshold to improve recall, if required for business purposes.\")"
        },
        {
          "id": 6,
          "title": "风险控制策略",
          "description": "设计和实施信用卡风险控制策略",
          "example_code": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# 使用通用字体设置，避免中文字体问题\nimport matplotlib as mpl\nmpl.rcParams['font.family'] = 'DejaVu Sans'\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom app.services.bank_analysis import bank_service\n\n# 加载数据\ndf = bank_service.load_credit_data()\n\n# 数据准备\nprint(\"Preparing data...\")\nX = df.drop('DEFAULT', axis=1)\ny = df['DEFAULT']\n\n# 标准化数据\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# 划分训练集和测试集\nX_train, X_test, y_train, y_test = train_test_split(\n    X_scaled, y, test_size=0.25, random_state=42, stratify=y\n)\n\n# 训练模型（使用随机森林为例）\nprint(\"Training model...\")\nmodel = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\nmodel.fit(X_train, y_train)\n\n# 获取预测概率\ny_proba = model.predict_proba(X_test)[:, 1]\n\n# 创建测试数据集的副本，并添加预测概率\ntest_df = pd.DataFrame(X_test, columns=X.columns)\ntest_df['ACTUAL_DEFAULT'] = y_test.values\ntest_df['DEFAULT_PROBABILITY'] = y_proba\n\n# 1. 风险分层策略\nprint(\"\\nImplementing risk stratification strategy...\")\n\n# 基于预测概率对客户进行风险分层\nrisk_bins = [0, 0.2, 0.4, 0.6, 0.8, 1.0]\nrisk_labels = ['Very Low', 'Low', 'Medium', 'High', 'Very High']\n\ntest_df['RISK_LEVEL'] = pd.cut(\n    test_df['DEFAULT_PROBABILITY'], \n    bins=risk_bins, \n    labels=risk_labels\n)\n\n# 计算每个风险等级的实际违约率\nrisk_level_stats = test_df.groupby('RISK_LEVEL')['ACTUAL_DEFAULT'].agg(\n    ['count', 'mean']\n).rename(columns={'count': 'Number_of_Customers', 'mean': 'Actual_Default_Rate'})\n\n# 添加预测的违约概率平均值\nrisk_level_stats['Avg_Predicted_Probability'] = test_df.groupby('RISK_LEVEL')['DEFAULT_PROBABILITY'].mean()\n\nprint(\"Risk Stratification Results:\")\nprint(risk_level_stats)\n\n# 可视化风险分层\nplt.figure(figsize=(12, 6))\n\n# 客户数量分布\nplt.subplot(1, 2, 1)\nsns.countplot(x='RISK_LEVEL', data=test_df, palette='YlOrRd')\nplt.title('Number of Customers by Risk Level')\nplt.xlabel('Risk Level')\nplt.ylabel('Count')\n\n# 各风险等级的实际违约率\nplt.subplot(1, 2, 2)\nsns.barplot(\n    x=risk_level_stats.index, \n    y=risk_level_stats['Actual_Default_Rate'],\n    palette='YlOrRd'\n)\nplt.title('Actual Default Rate by Risk Level')\nplt.xlabel('Risk Level')\nplt.ylabel('Default Rate')\nplt.ylim(0, 1)\n\nplt.tight_layout()\nplt.show()\n\n# 2. 设计信用审批策略\nprint(\"\\nDesigning credit approval strategy...\")\n\n# 设置不同风险等级的信用额度调整系数\ncredit_limit_adjustments = {\n    'Very Low': 1.2,   # 增加20%\n    'Low': 1.1,        # 增加10%\n    'Medium': 1.0,     # 保持不变\n    'High': 0.8,       # 减少20%\n    'Very High': 0.6   # 减少40%\n}\n\n# 假设我们有一个基准信用额度为10000\nbase_credit_limit = 10000\n\n# 计算调整后的信用额度\ntest_df['ADJUSTED_CREDIT_LIMIT'] = test_df['RISK_LEVEL'].map(credit_limit_adjustments) * base_credit_limit\n\n# 3. 拒绝策略\n# 设置拒绝阈值 - 高于此阈值的申请将被拒绝\nrejection_threshold = 0.7\n\ntest_df['APPLICATION_STATUS'] = np.where(\n    test_df['DEFAULT_PROBABILITY'] >= rejection_threshold,\n    'Rejected',\n    'Approved'\n)\n\n# 计算拒绝率和批准率\nrejection_rate = (test_df['APPLICATION_STATUS'] == 'Rejected').mean()\napproval_rate = (test_df['APPLICATION_STATUS'] == 'Approved').mean()\n\nprint(f\"Rejection Rate: {rejection_rate:.2%}\")\nprint(f\"Approval Rate: {approval_rate:.2%}\")\n\n# 计算拒绝和批准客户中的实际违约率\nrejected_default_rate = test_df[test_df['APPLICATION_STATUS'] == 'Rejected']['ACTUAL_DEFAULT'].mean()\napproved_default_rate = test_df[test_df['APPLICATION_STATUS'] == 'Approved']['ACTUAL_DEFAULT'].mean()\n\nprint(f\"Default Rate among Rejected Customers: {rejected_default_rate:.2%}\")\nprint(f\"Default Rate among Approved Customers: {approved_default_rate:.2%}\")\n\n# 可视化拒绝策略\nplt.figure(figsize=(12, 5))\n\n# 批准 vs. 拒绝的客户数量\nplt.subplot(1, 2, 1)\nsns.countplot(x='APPLICATION_STATUS', data=test_df)\nplt.title('Application Approval Status')\nplt.xlabel('Status')\nplt.ylabel('Count')\n\n# 批准 vs. 拒绝的违约率\nstatus_default_rates = test_df.groupby('APPLICATION_STATUS')['ACTUAL_DEFAULT'].mean()\nplt.subplot(1, 2, 2)\nsns.barplot(x=status_default_rates.index, y=status_default_rates.values)\nplt.title('Default Rate by Application Status')\nplt.xlabel('Status')\nplt.ylabel('Default Rate')\nplt.ylim(0, 1)\n\nplt.tight_layout()\nplt.show()\n\n# 4. 利润模拟\nprint(\"\\nSimulating profit impact...\")\n\n# 设置业务参数\nannual_interest_rate = 0.18  # 年利率18%\naverage_transaction_amount = 5000  # 平均交易金额\nfee_percentage = 0.03  # 交易费用3%\ncost_of_funds = 0.05  # 资金成本5%\ncost_per_default = 10000  # 每个违约客户的平均损失\n\n# 计算每个客户的预期收益\ntest_df['EXPECTED_PROFIT'] = np.where(\n    test_df['ACTUAL_DEFAULT'] == 1,\n    -cost_per_default,  # 违约客户造成损失\n    (annual_interest_rate - cost_of_funds) * average_transaction_amount + \n        fee_percentage * average_transaction_amount  # 非违约客户产生收益\n)\n\n# 计算不同策略下的总收益\n# 1. 接受所有客户\ntotal_profit_accept_all = test_df['EXPECTED_PROFIT'].sum()\n\n# 2. 使用我们的拒绝策略\ntotal_profit_with_strategy = test_df[test_df['APPLICATION_STATUS'] == 'Approved']['EXPECTED_PROFIT'].sum()\n\n# 3. 拒绝所有客户\ntotal_profit_reject_all = 0  # 假设拒绝所有客户没有收益也没有损失\n\nprint(f\"Profit if Accept All: ${total_profit_accept_all:.2f}\")\nprint(f\"Profit with Rejection Strategy: ${total_profit_with_strategy:.2f}\")\nprint(f\"Profit if Reject All: ${total_profit_reject_all:.2f}\")\n\nprofit_improvement = ((total_profit_with_strategy - total_profit_accept_all) / abs(total_profit_accept_all)) * 100\nprint(f\"Profit Improvement with Strategy: {profit_improvement:.2f}%\")\n\n# 5. 阈值优化\nprint(\"\\nOptimizing rejection threshold...\")\n\n# 测试不同的拒绝阈值\nthresholds = np.arange(0.1, 1.0, 0.1)\nthreshold_results = []\n\nfor threshold in thresholds:\n    # 应用拒绝策略\n    application_status = np.where(\n        test_df['DEFAULT_PROBABILITY'] >= threshold,\n        'Rejected',\n        'Approved'\n    )\n    \n    # 计算批准客户的总收益\n    approved_indices = [i for i, status in enumerate(application_status) if status == 'Approved']\n    profit = test_df.iloc[approved_indices]['EXPECTED_PROFIT'].sum()\n    \n    # 计算拒绝率\n    rejection_rate = (application_status == 'Rejected').mean()\n    \n    # 计算批准客户中的违约率\n    approved_default_rate = test_df.iloc[approved_indices]['ACTUAL_DEFAULT'].mean() if approved_indices else 0\n    \n    threshold_results.append({\n        'Threshold': threshold,\n        'Profit': profit,\n        'Rejection_Rate': rejection_rate,\n        'Approved_Default_Rate': approved_default_rate\n    })\n\n# 转换为DataFrame\nthreshold_df = pd.DataFrame(threshold_results)\n\n# 找出利润最高的阈值\noptimal_threshold = threshold_df.loc[threshold_df['Profit'].idxmax(), 'Threshold']\nmax_profit = threshold_df['Profit'].max()\n\nprint(f\"Optimal Rejection Threshold: {optimal_threshold}\")\nprint(f\"Maximum Profit: ${max_profit:.2f}\")\n\n# 可视化不同阈值下的结果\nplt.figure(figsize=(15, 6))\n\n# 利润 vs. 阈值\nplt.subplot(1, 3, 1)\nplt.plot(threshold_df['Threshold'], threshold_df['Profit'], marker='o')\nplt.axvline(x=optimal_threshold, color='r', linestyle='--', label=f'Optimal={optimal_threshold}')\nplt.title('Profit vs. Threshold')\nplt.xlabel('Rejection Threshold')\nplt.ylabel('Profit ($)')\nplt.legend()\n\n# 拒绝率 vs. 阈值\nplt.subplot(1, 3, 2)\nplt.plot(threshold_df['Threshold'], threshold_df['Rejection_Rate'], marker='o')\nplt.axvline(x=optimal_threshold, color='r', linestyle='--')\nplt.title('Rejection Rate vs. Threshold')\nplt.xlabel('Rejection Threshold')\nplt.ylabel('Rejection Rate')\n\n# 批准客户中的违约率 vs. 阈值\nplt.subplot(1, 3, 3)\nplt.plot(threshold_df['Threshold'], threshold_df['Approved_Default_Rate'], marker='o')\nplt.axvline(x=optimal_threshold, color='r', linestyle='--')\nplt.title('Approved Default Rate vs. Threshold')\nplt.xlabel('Rejection Threshold')\nplt.ylabel('Default Rate among Approved')\n\nplt.tight_layout()\nplt.show()\n\n# 6. 最终风险控制策略建议\nprint(\"\\nFinal Risk Control Strategy Recommendations:\")\n\nprint(f\"1. Implement a risk-based approval system with an optimal rejection threshold of {optimal_threshold}\")\nprint(\"2. Apply risk-based credit limit adjustments:\")\nfor risk_level, adjustment in credit_limit_adjustments.items():\n    adjustment_percentage = (adjustment - 1) * 100 if adjustment > 1 else (adjustment - 1) * 100\n    print(f\"   - {risk_level} risk: {adjustment_percentage:+.0f}% credit limit adjustment\")\n\nprint(\"3. Implement additional monitoring for high-risk approved customers\")\nprint(\"4. Consider implementing risk-based pricing for different risk segments\")\n\n# 7. 策略实施后的预期业务影响\nprint(\"\\nExpected Business Impact:\")\nprint(f\"- Expected Profit Improvement: {profit_improvement:.2f}%\")\nprint(f\"- Expected Approval Rate: {1 - threshold_df.loc[threshold_df['Threshold'] == optimal_threshold, 'Rejection_Rate'].values[0]:.2%}\")\nprint(f\"- Expected Default Rate among Approved: {threshold_df.loc[threshold_df['Threshold'] == optimal_threshold, 'Approved_Default_Rate'].values[0]:.2%}\")"
        }
      ]
    },
    {
      "id": 2,
      "title": "银行客户流失预警模型",
      "description": "使用机器学习方法预测和分析银行客户流失风险因素",
      "category": "bank",
      "difficulty_level": 4,
      "estimated_duration": 120,
      "prerequisites": "统计分析基础和数据挖掘知识",
      "data_source": "银行客户交易与行为数据",
      "steps": [
        {
          "id": 1,
          "title": "客户行为分析",
          "description": "分析客户交易频率、金额和渠道偏好等行为特征",
          "example_code": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport seaborn as sns\n# 使用通用字体设置，避免中文字体问题\nfrom app.services.docker_matplotlib_fix import configure_matplotlib_fonts\n\n# 配置字体以支持中文显示\nconfigure_matplotlib_fonts()\nfrom app.services.bank_analysis import bank_service\n\n# 加载客户数据\ndf = bank_service.load_customer_churn_data()\n\n# 1. 数据概览\nprint(\"=== Data Overview ===\")\nprint(f\"Dataset shape: {df.shape}\")\nprint(\"\\nFirst few rows:\")\nprint(df.head())\n\n# 2. 数据基本信息\nprint(\"\\n=== Basic Data Information ===\")\nprint(df.info())\n\n# 3. 基本统计描述\nprint(\"\\n=== Descriptive Statistics ===\")\nprint(df.describe())\n\n# 4. 缺失值情况\nprint(\"\\n=== Missing Values Analysis ===\")\nmissing_values = df.isnull().sum()\nif missing_values.sum() > 0:\n    print(missing_values[missing_values > 0])\n    missing_percentage = (missing_values / len(df)) * 100\n    print(\"\\nMissing percentage:\")\n    print(missing_percentage[missing_percentage > 0])\nelse:\n    print(\"No missing values found.\")\n\n# 5. 客户流失情况分析\nif 'Churn' in df.columns:\n    print(\"\\n=== Churn Distribution ===\")\n    churn_distribution = df['Churn'].value_counts(normalize=True) * 100\n    print(f\"Non-churned customers: {churn_distribution[0]:.2f}%\")\n    print(f\"Churned customers: {churn_distribution[1]:.2f}%\")\n    \n    plt.figure(figsize=(10, 5))\n    plt.subplot(1, 2, 1)\n    sns.countplot(x='Churn', data=df, palette=['lightblue', 'salmon'])\n    plt.title('Customer Churn Distribution')\n    plt.xlabel('Churn (0=No, 1=Yes)')\n    plt.ylabel('Count')\n    \n    plt.subplot(1, 2, 2)\n    plt.pie(churn_distribution, labels=['Retained', 'Churned'], autopct='%1.1f%%', colors=['lightblue', 'salmon'], startangle=90)\n    plt.title('Churn Percentage')\n    plt.axis('equal')\n    plt.tight_layout()\n    plt.show()\n\n# 6. 交易频率分析\nif 'TransactionFrequency' in df.columns:\n    print(\"\\n=== Transaction Frequency Analysis ===\")\n    print(df.groupby('Churn')['TransactionFrequency'].describe())\n    \n    plt.figure(figsize=(10, 6))\n    sns.histplot(data=df, x='TransactionFrequency', hue='Churn', bins=20, kde=True, palette=['lightblue', 'salmon'])\n    plt.title('Transaction Frequency Distribution by Churn Status')\n    plt.xlabel('Transaction Frequency (per month)')\n    plt.ylabel('Count')\n    plt.show()\n    \n    plt.figure(figsize=(8, 6))\n    sns.boxplot(x='Churn', y='TransactionFrequency', data=df, palette=['lightblue', 'salmon'])\n    plt.title('Transaction Frequency by Churn Status')\n    plt.xlabel('Churn (0=No, 1=Yes)')\n    plt.ylabel('Transaction Frequency')\n    plt.show()\n\n# 7. 交易金额分析\nif 'TransactionAmount' in df.columns:\n    print(\"\\n=== Transaction Amount Analysis ===\")\n    print(df.groupby('Churn')['TransactionAmount'].describe())\n    \n    plt.figure(figsize=(10, 6))\n    sns.histplot(data=df, x='TransactionAmount', hue='Churn', bins=20, kde=True, palette=['lightblue', 'salmon'])\n    plt.title('Transaction Amount Distribution by Churn Status')\n    plt.xlabel('Average Transaction Amount')\n    plt.ylabel('Count')\n    plt.show()\n\n# 8. 渠道偏好分析\nif 'PreferredChannel' in df.columns:\n    print(\"\\n=== Channel Preference Analysis ===\")\n    channel_counts = df.groupby(['PreferredChannel', 'Churn']).size().unstack()\n    print(channel_counts)\n    \n    # 计算每个渠道的流失率\n    channel_churn_rate = channel_counts[1] / (channel_counts[0] + channel_counts[1]) * 100\n    print(\"\\nChurn rate by channel:\")\n    print(channel_churn_rate)\n    \n    plt.figure(figsize=(12, 5))\n    plt.subplot(1, 2, 1)\n    channel_counts.plot(kind='bar', stacked=True, color=['lightblue', 'salmon'])\n    plt.title('Customer Count by Channel and Churn Status')\n    plt.xlabel('Preferred Channel')\n    plt.ylabel('Number of Customers')\n    plt.legend(['Retained', 'Churned'])\n    \n    plt.subplot(1, 2, 2)\n    channel_churn_rate.plot(kind='bar', color='salmon')\n    plt.title('Churn Rate by Channel')\n    plt.xlabel('Preferred Channel')\n    plt.ylabel('Churn Rate (%)')\n    plt.tight_layout()\n    plt.show()\n\n# 9. 客户活跃度分析\nif 'DaysSinceLastTransaction' in df.columns:\n    print(\"\\n=== Customer Activity Analysis ===\")\n    print(df.groupby('Churn')['DaysSinceLastTransaction'].describe())\n    \n    plt.figure(figsize=(10, 6))\n    sns.boxplot(x='Churn', y='DaysSinceLastTransaction', data=df, palette=['lightblue', 'salmon'])\n    plt.title('Days Since Last Transaction by Churn Status')\n    plt.xlabel('Churn (0=No, 1=Yes)')\n    plt.ylabel('Days Since Last Transaction')\n    plt.show()\n\n# 10. 客户价值分析\nif 'CustomerValue' in df.columns and 'Tenure' in df.columns:\n    print(\"\\n=== Customer Value Analysis ===\")\n    # 计算每个客户的生命周期价值\n    df['LifetimeValue'] = df['CustomerValue'] * df['Tenure']\n    print(df.groupby('Churn')['LifetimeValue'].describe())\n    \n    plt.figure(figsize=(10, 6))\n    sns.scatterplot(data=df, x='Tenure', y='CustomerValue', hue='Churn', palette=['lightblue', 'salmon'], alpha=0.7)\n    plt.title('Customer Value vs Tenure by Churn Status')\n    plt.xlabel('Tenure (months)')\n    plt.ylabel('Monthly Customer Value')\n    plt.show()\n\n# 11. 数值特征间的相关性分析\nnumeric_df = df.select_dtypes(include=[np.number])\nif len(numeric_df.columns) > 1:\n    print(\"\\n=== Correlation Analysis ===\")\n    correlation_matrix = numeric_df.corr()\n    print(correlation_matrix['Churn'].sort_values(ascending=False) if 'Churn' in numeric_df.columns else correlation_matrix)\n    \n    plt.figure(figsize=(12, 10))\n    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n    plt.title('Correlation Matrix of Numerical Features')\n    plt.tight_layout()\n    plt.show()\n\n# 12. 关键行为特征的分布比较\nkey_features = [col for col in df.columns if col not in ['CustomerID', 'Name', 'Churn']]\nif len(key_features) > 0:\n    print(\"\\n=== Key Feature Distributions ===\")\n    \n    # 显示主要特征的分布\n    num_cols = min(6, len(key_features))\n    num_rows = (len(key_features) + num_cols - 1) // num_cols\n    plt.figure(figsize=(15, num_rows * 4))\n    \n    for i, feature in enumerate(key_features[:num_cols * num_rows]):\n        if feature in df.columns and df[feature].dtype in [np.int64, np.float64]:\n            plt.subplot(num_rows, num_cols, i+1)\n            sns.histplot(data=df, x=feature, hue='Churn', kde=True, palette=['lightblue', 'salmon'], bins=20)\n            plt.title(f'Distribution of {feature}')\n    \n    plt.tight_layout()\n    plt.show()\n\n# 13. 客户分群和行为模式识别\nif 'TransactionFrequency' in df.columns and 'TransactionAmount' in df.columns:\n    print(\"\\n=== Customer Segmentation Based on Transaction Behavior ===\")\n    \n    plt.figure(figsize=(10, 8))\n    sns.scatterplot(data=df, x='TransactionFrequency', y='TransactionAmount', hue='Churn', size='Tenure', sizes=(50, 200), palette=['lightblue', 'salmon'], alpha=0.7)\n    plt.title('Customer Segmentation by Transaction Behavior')\n    plt.xlabel('Transaction Frequency')\n    plt.ylabel('Average Transaction Amount')\n    plt.grid(True, linestyle='--', alpha=0.7)\n    plt.show()\n\n# 14. 总结关键发现\nprint(\"\\n=== Key Findings Summary ===\")\nif 'Churn' in df.columns:\n    # 计算流失率\n    churn_rate = df['Churn'].mean() * 100\n    print(f\"Overall Churn Rate: {churn_rate:.2f}%\")\n    \n    # 找出与流失最相关的特征\n    if 'Churn' in numeric_df.columns:\n        churn_correlations = correlation_matrix['Churn'].drop('Churn').abs().sort_values(ascending=False)\n        print(\"\\nTop features correlated with churn:\")\n        print(churn_correlations.head(5))\n    \n    # 分析流失客户和留存客户之间的主要区别\n    print(\"\\nKey differences between churned and non-churned customers:\")\n    for feature in key_features[:5]:\n        if feature in df.columns and df[feature].dtype in [np.int64, np.float64]:\n            churned_mean = df[df['Churn'] == 1][feature].mean()\n            non_churned_mean = df[df['Churn'] == 0][feature].mean()\n            diff_pct = ((churned_mean - non_churned_mean) / non_churned_mean) * 100\n            print(f\"- {feature}: {diff_pct:.1f}% difference between churned and non-churned customers\")"
        },
        {
          "id": 2,
          "title": "流失指标定义",
          "description": "确定客户流失的定义和关键指标",
          "example_code": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport matplotlib as mpl\nfrom datetime import datetime, timedelta\nfrom app.services.docker_matplotlib_fix import configure_matplotlib_fonts\n\n# 配置字体以支持中文显示\nconfigure_matplotlib_fonts()\nfrom app.services.bank_analysis import bank_service\n\n# 加载客户数据\ndf = bank_service.load_customer_churn_data()\nprint(f\"加载数据集，共{df.shape[0]}行，{df.shape[1]}列\")\n\n# 1. 探索现有的流失标签\nif 'Churn' in df.columns:\n    print(\"\\n=== 现有的流失标签分析 ===\")\n    churn_counts = df['Churn'].value_counts()\n    print(\"流失标签分布:\")\n    print(f\"未流失客户: {churn_counts[0]} ({churn_counts[0]/len(df)*100:.2f}%)\")\n    print(f\"已流失客户: {churn_counts[1]} ({churn_counts[1]/len(df)*100:.2f}%)\")\n    \n    # 可视化现有的流失分布\n    plt.figure(figsize=(10, 5))\n    plt.subplot(1, 2, 1)\n    sns.countplot(x='Churn', data=df, palette=['lightblue', 'salmon'])\n    plt.title('客户流失分布')\n    plt.xlabel('流失状态 (0=未流失, 1=已流失)')\n    plt.ylabel('客户数量')\n    \n    plt.subplot(1, 2, 2)\n    plt.pie(churn_counts, labels=['未流失', '已流失'], autopct='%1.1f%%', colors=['lightblue', 'salmon'], startangle=90)\n    plt.title('流失比例')\n    plt.axis('equal')\n    plt.tight_layout()\n    plt.show()\n\n# 2. 定义流失的方法\nprint(\"\\n=== 流失定义方法 ===\")\nprint(\"1. 账户关闭: 客户主动关闭账户\")\nprint(\"2. 非活跃流失: 客户长时间没有活动\")\nprint(\"3. 价值流失: 客户交易额显著下降\")\nprint(\"4. 参与度流失: 客户交易频率显著下降\")\nprint(\"5. 渠道转换: 客户完全停止使用某一渠道\")\n\n# 3. 基于交易不活跃定义流失\nprint(\"\\n=== 基于交易不活跃定义流失 ===\")\ninactivity_threshold = 90  # 90天不活跃定义为流失\n\nif 'LastTransactionDate' in df.columns:\n    # 转换日期字段为日期类型\n    df['LastTransactionDate'] = pd.to_datetime(df['LastTransactionDate'])\n    \n    # 计算当前日期（使用数据集中最近的交易日期再加7天作为\"当前日期\"）\n    current_date = df['LastTransactionDate'].max() + timedelta(days=7)\n    print(f\"分析的'当前日期': {current_date.strftime('%Y-%m-%d')}\")\n    \n    # 计算自上次交易以来的天数\n    df['DaysSinceLastTransaction'] = (current_date - df['LastTransactionDate']).dt.days\n    \n    # 基于不活跃天数定义流失\n    df['InactivityChurn'] = (df['DaysSinceLastTransaction'] > inactivity_threshold).astype(int)\n    \n    inactive_churn_rate = df['InactivityChurn'].mean() * 100\n    print(f\"基于{inactivity_threshold}天不活跃定义的流失率: {inactive_churn_rate:.2f}%\")\n    \n    # 与现有流失标签对比\n    if 'Churn' in df.columns:\n        churn_match = (df['Churn'] == df['InactivityChurn']).mean() * 100\n        print(f\"不活跃定义的流失与现有流失标签匹配度: {churn_match:.2f}%\")\n        \n        # 计算现有流失标签的不同类型\n        days_inactive_churned = df[df['Churn'] == 1]['DaysSinceLastTransaction'].mean()\n        days_inactive_retained = df[df['Churn'] == 0]['DaysSinceLastTransaction'].mean()\n        print(f\"已流失客户的平均不活跃天数: {days_inactive_churned:.1f}天\")\n        print(f\"未流失客户的平均不活跃天数: {days_inactive_retained:.1f}天\")\n\n# 4. 基于交易金额下降定义流失\nprint(\"\\n=== 基于交易金额下降定义流失 ===\")\nvalue_decline_threshold = -0.5  # 交易金额下降50%定义为价值流失\n\nif 'TransactionAmountDecline' in df.columns:\n    # 基于交易金额下降定义流失\n    df['ValueChurn'] = (df['TransactionAmountDecline'] <= value_decline_threshold).astype(int)\n    \n    value_churn_rate = df['ValueChurn'].mean() * 100\n    print(f\"基于交易金额下降{abs(value_decline_threshold)*100}%定义的流失率: {value_churn_rate:.2f}%\")\n    \n    # 与现有流失标签对比\n    if 'Churn' in df.columns:\n        value_churn_match = (df['Churn'] == df['ValueChurn']).mean() * 100\n        print(f\"价值流失与现有流失标签匹配度: {value_churn_match:.2f}%\")\n\n# 5. 基于交易频率下降定义流失\nprint(\"\\n=== 基于交易频率下降定义流失 ===\")\nfrequency_decline_threshold = -0.6  # 交易频率下降60%定义为参与度流失\n\nif 'TransactionFrequencyDecline' in df.columns:\n    # 基于交易频率下降定义流失\n    df['FrequencyChurn'] = (df['TransactionFrequencyDecline'] <= frequency_decline_threshold).astype(int)\n    \n    frequency_churn_rate = df['FrequencyChurn'].mean() * 100\n    print(f\"基于交易频率下降{abs(frequency_decline_threshold)*100}%定义的流失率: {frequency_churn_rate:.2f}%\")\n    \n    # 与现有流失标签对比\n    if 'Churn' in df.columns:\n        freq_churn_match = (df['Churn'] == df['FrequencyChurn']).mean() * 100\n        print(f\"参与度流失与现有流失标签匹配度: {freq_churn_match:.2f}%\")\n\n# 6. 综合指标流失定义\nprint(\"\\n=== 综合指标流失定义 ===\")\n# 创建综合流失指标\nchurn_indicators = []\n\nif 'InactivityChurn' in df.columns:\n    churn_indicators.append('InactivityChurn')\nif 'ValueChurn' in df.columns:\n    churn_indicators.append('ValueChurn')\nif 'FrequencyChurn' in df.columns:\n    churn_indicators.append('FrequencyChurn')\n\nif churn_indicators:\n    # 如果任一指标显示流失，则认为客户已流失\n    df['CompositeChurn'] = df[churn_indicators].max(axis=1)\n    \n    composite_churn_rate = df['CompositeChurn'].mean() * 100\n    print(f\"基于综合指标定义的流失率: {composite_churn_rate:.2f}%\")\n    \n    # 各种流失类型的重叠情况\n    if len(churn_indicators) > 1:\n        print(\"\\n不同流失类型的占比:\")\n        for indicator in churn_indicators:\n            indicator_rate = df[indicator].mean() * 100\n            print(f\"- {indicator}: {indicator_rate:.2f}%\")\n        \n        # 计算纯流失类型的客户\n        for i, indicator1 in enumerate(churn_indicators):\n            only_this_type = df[indicator1] == 1\n            for j, indicator2 in enumerate(churn_indicators):\n                if i != j:\n                    only_this_type &= df[indicator2] == 0\n            \n            only_this_type_pct = only_this_type.mean() * 100\n            print(f\"- 仅{indicator1}类型流失: {only_this_type_pct:.2f}%\")\n    \n    # 与现有流失标签对比\n    if 'Churn' in df.columns:\n        composite_match = (df['Churn'] == df['CompositeChurn']).mean() * 100\n        print(f\"\\n综合流失定义与现有流失标签匹配度: {composite_match:.2f}%\")\n\n# 7. 流失预警信号定义\nprint(\"\\n=== 流失预警信号定义 ===\")\n# 定义预警等级\ndf['ChurnRiskScore'] = 0\n\n# 根据不同指标分配风险分数\nif 'DaysSinceLastTransaction' in df.columns:\n    df.loc[df['DaysSinceLastTransaction'] > 60, 'ChurnRiskScore'] += 1\n    df.loc[df['DaysSinceLastTransaction'] > 75, 'ChurnRiskScore'] += 1\n\nif 'TransactionAmountDecline' in df.columns:\n    df.loc[df['TransactionAmountDecline'] < -0.3, 'ChurnRiskScore'] += 1\n    df.loc[df['TransactionAmountDecline'] < -0.4, 'ChurnRiskScore'] += 1\n\nif 'TransactionFrequencyDecline' in df.columns:\n    df.loc[df['TransactionFrequencyDecline'] < -0.4, 'ChurnRiskScore'] += 1\n    df.loc[df['TransactionFrequencyDecline'] < -0.5, 'ChurnRiskScore'] += 1\n\n# 其他可能的风险信号\nif 'SupportCallsCount' in df.columns:\n    df.loc[df['SupportCallsCount'] > 3, 'ChurnRiskScore'] += 1\n\nif 'ComplaintsCount' in df.columns:\n    df.loc[df['ComplaintsCount'] > 0, 'ChurnRiskScore'] += 1\n\n# 定义风险等级\ndf['ChurnRiskLevel'] = pd.cut(df['ChurnRiskScore'], \n                          bins=[-1, 0, 2, 4, 100], \n                          labels=['无风险', '低风险', '中风险', '高风险'])\n\n# 显示风险等级分布\nrisk_distribution = df['ChurnRiskLevel'].value_counts(normalize=True) * 100\nprint(\"流失风险等级分布:\")\nfor level, percentage in risk_distribution.items():\n    print(f\"- {level}: {percentage:.2f}%\")\n\n# 可视化风险分数分布\nplt.figure(figsize=(15, 5))\nplt.subplot(1, 2, 1)\nsns.countplot(x='ChurnRiskScore', data=df, palette='YlOrRd')\nplt.title('流失风险分数分布')\nplt.xlabel('风险分数')\nplt.ylabel('客户数量')\n\nplt.subplot(1, 2, 2)\nsns.countplot(x='ChurnRiskLevel', data=df, palette='YlOrRd')\nplt.title('流失风险等级分布')\nplt.xlabel('风险等级')\nplt.ylabel('客户数量')\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n\n# 8. 流失指标的业务解释\nprint(\"\\n=== 流失指标的业务解释 ===\")\nprint(\"根据分析，我们建议使用以下流失定义:\")\nprint(\"1. 活跃度流失: 客户连续90天未进行任何交易\")\nprint(\"2. 价值流失: 客户近期交易金额较过往平均下降50%以上\")\nprint(\"3. 频率流失: 客户近期交易频率较过往平均下降60%以上\")\nprint(\"4. 综合流失指标: 满足上述任一条件即视为流失风险客户\")\n\nprint(\"\\n预警信号层级:\")\nprint(\"- 低风险: 出现1-2个轻微预警信号，建议进行常规客户维护\")\nprint(\"- 中风险: 出现2-4个预警信号，建议进行针对性的客户挽留措施\")\nprint(\"- 高风险: 出现4个以上预警信号，建议立即实施紧急挽留计划\")\n\n# 9. 流失率的时间趋势（假设有历史数据）\nprint(\"\\n=== 流失率时间趋势模拟 ===\")\n# 创建假设的月度流失率数据\nmonths = pd.date_range(start='2023-01-01', periods=12, freq='M')\nmonthly_churn_rates = [5.2, 5.3, 5.1, 5.4, 5.6, 5.8, 6.1, 6.3, 6.0, 5.7, 5.5, 5.3]\n\n# 可视化流失率趋势\nplt.figure(figsize=(12, 6))\nplt.plot(months, monthly_churn_rates, marker='o', linestyle='-', color='salmon')\nplt.title('月度客户流失率趋势')\nplt.xlabel('月份')\nplt.ylabel('流失率 (%)')\nplt.grid(True, linestyle='--', alpha=0.7)\nplt.tight_layout()\nplt.show()\n\n# 10. 流失客户的财务影响\nprint(\"\\n=== 流失客户的财务影响分析 ===\")\n# 假设的客户价值数据\navg_customer_value = 5000  # 平均客户年价值\ncustomer_acquisition_cost = 500  # 获取新客户成本\ntotal_customers = len(df)\n\n# 计算不同流失率下的财务影响\nchurn_rates = [0.05, 0.10, 0.15, 0.20]\nfinancial_impact = []\n\nfor rate in churn_rates:\n    churned_customers = total_customers * rate\n    lost_revenue = churned_customers * avg_customer_value\n    replacement_cost = churned_customers * customer_acquisition_cost\n    total_impact = lost_revenue + replacement_cost\n    \n    financial_impact.append({\n        'Churn_Rate': rate * 100,\n        'Churned_Customers': churned_customers,\n        'Lost_Revenue': lost_revenue,\n        'Replacement_Cost': replacement_cost,\n        'Total_Impact': total_impact\n    })\n\nimpact_df = pd.DataFrame(financial_impact)\nprint(impact_df)\n\n# 可视化财务影响\nplt.figure(figsize=(12, 6))\nx = impact_df['Churn_Rate']\n\nplt.bar(x, impact_df['Lost_Revenue'], label='损失收入', color='salmon', alpha=0.7)\nplt.bar(x, impact_df['Replacement_Cost'], bottom=impact_df['Lost_Revenue'], label='替换成本', color='lightblue', alpha=0.7)\n\nplt.title('不同流失率下的财务影响')\nplt.xlabel('流失率 (%)')\nplt.ylabel('财务影响 (元)')\nplt.legend()\nplt.grid(True, axis='y', linestyle='--', alpha=0.7)\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n通过明确定义流失指标和预警信号，银行可以更早地识别潜在流失客户，并采取相应措施降低流失率，从而减少财务损失。\")"
        },
        {
          "id": 3,
          "title": "预警变量筛选",
          "description": "识别和筛选有预测价值的客户特征变量",
          "example_code": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif, RFE\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom scipy.stats import pointbiserialr\nimport matplotlib as mpl\nfrom app.services.docker_matplotlib_fix import configure_matplotlib_fonts\n\n# 配置字体以支持中文显示\nconfigure_matplotlib_fonts()\nfrom app.services.bank_analysis import bank_service\n\n# 加载客户数据\ndf = bank_service.load_customer_churn_data()\nprint(f\"加载数据集，共{df.shape[0]}行，{df.shape[1]}列\")\n\n# 1. 数据预处理\nprint(\"\\n=== 数据预处理 ===\")\n\n# 检查目标变量\nif 'Churn' not in df.columns:\n    print(\"错误: 数据集中缺少目标变量'Churn'\")\n    exit()\n\n# 移除不需要的列\nnon_feature_cols = ['CustomerID', 'Name', 'Email', 'Address']\nfeature_cols = [col for col in df.columns if col != 'Churn' and col not in non_feature_cols]\n\n# 检查并处理缺失值\nmissing_values = df[feature_cols].isnull().sum()\nif missing_values.sum() > 0:\n    print(\"\\n存在缺失值的特征:\")\n    print(missing_values[missing_values > 0])\n    \n    # 使用中位数填充数值型缺失值\n    numeric_cols = df[feature_cols].select_dtypes(include=['int64', 'float64']).columns\n    for col in numeric_cols:\n        if df[col].isnull().sum() > 0:\n            median_val = df[col].median()\n            df[col].fillna(median_val, inplace=True)\n    \n    # 使用众数填充类别型缺失值\n    categorical_cols = df[feature_cols].select_dtypes(include=['object', 'category']).columns\n    for col in categorical_cols:\n        if df[col].isnull().sum() > 0:\n            mode_val = df[col].mode()[0]\n            df[col].fillna(mode_val, inplace=True)\n\n# 处理类别特征\ncategorical_cols = df[feature_cols].select_dtypes(include=['object', 'category']).columns.tolist()\nif categorical_cols:\n    print(f\"\\n对{len(categorical_cols)}个类别特征进行独热编码\")\n    df_encoded = pd.get_dummies(df, columns=categorical_cols, drop_first=True)\n    \n    # 更新特征列列表\n    feature_cols = [col for col in df_encoded.columns if col != 'Churn' and col not in non_feature_cols]\nelse:\n    df_encoded = df.copy()\n\n# 获取X和y\nX = df_encoded[feature_cols]\ny = df_encoded['Churn']\n\n# 分割训练集和测试集\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n\n# 标准化数值特征\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nprint(f\"\\n预处理后的特征数量: {len(feature_cols)}\")\nprint(f\"训练集: {X_train.shape[0]}行, 测试集: {X_test.shape[0]}行\")\n\n# 2. 单变量特征筛选\nprint(\"\\n=== 单变量特征重要性分析 ===\")\n\n# 2.1 点二列相关系数(连续特征)\nprint(\"\\n连续特征的点二列相关系数:\")\nnumeric_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n\nif numeric_cols:\n    corr_results = []\n    \n    for col in numeric_cols:\n        corr, p_value = pointbiserialr(y, X[col])\n        corr_results.append({\n            'Feature': col,\n            'Correlation': abs(corr),\n            'P_Value': p_value\n        })\n    \n    corr_df = pd.DataFrame(corr_results)\n    corr_df = corr_df.sort_values('Correlation', ascending=False)\n    \n    # 显示相关系数最高的特征\n    print(corr_df.head(10))\n    \n    # 可视化相关系数\n    plt.figure(figsize=(12, 8))\n    top_features = corr_df.head(15)['Feature'].tolist()\n    sns.barplot(x='Correlation', y='Feature', data=corr_df[corr_df['Feature'].isin(top_features)])\n    plt.title('特征与流失的点二列相关系数（绝对值）')\n    plt.xlabel('绝对相关系数')\n    plt.tight_layout()\n    plt.show()\n\n# 2.2 ANOVA F-value\nprint(\"\\nANOVA F-value特征筛选:\")\nanova_selector = SelectKBest(f_classif, k='all')\nanova_selector.fit(X_train_scaled, y_train)\n\n# 计算ANOVA F-value和P-value\nanova_scores = pd.DataFrame()\nanova_scores['Feature'] = X.columns\nanova_scores['F_Value'] = anova_selector.scores_\nanova_scores['P_Value'] = anova_selector.pvalues_\nanova_scores = anova_scores.sort_values('F_Value', ascending=False)\n\n# 显示得分最高的特征\nprint(anova_scores.head(10))\n\n# 可视化得分最高的特征\nplt.figure(figsize=(12, 8))\ntop_features = anova_scores.head(15)['Feature'].tolist()\nsns.barplot(x='F_Value', y='Feature', data=anova_scores[anova_scores['Feature'].isin(top_features)])\nplt.title('ANOVA F-value特征重要性')\nplt.xlabel('F-value')\nplt.tight_layout()\nplt.show()\n\n# 2.3 互信息筛选\nprint(\"\\n互信息特征筛选:\")\nmi_selector = SelectKBest(mutual_info_classif, k='all')\nmi_selector.fit(X_train_scaled, y_train)\n\n# 计算互信息分数\nmi_scores = pd.DataFrame()\nmi_scores['Feature'] = X.columns\nmi_scores['MI_Score'] = mi_selector.scores_\nmi_scores = mi_scores.sort_values('MI_Score', ascending=False)\n\n# 显示得分最高的特征\nprint(mi_scores.head(10))\n\n# 可视化得分最高的特征\nplt.figure(figsize=(12, 8))\ntop_features = mi_scores.head(15)['Feature'].tolist()\nsns.barplot(x='MI_Score', y='Feature', data=mi_scores[mi_scores['Feature'].isin(top_features)])\nplt.title('互信息特征重要性')\nplt.xlabel('互信息分数')\nplt.tight_layout()\nplt.show()\n\n# 3. 基于模型的特征筛选\nprint(\"\\n=== 基于模型的特征重要性分析 ===\")\n\n# 3.1 随机森林特征重要性\nprint(\"\\n随机森林特征重要性:\")\nrf_model = RandomForestClassifier(n_estimators=100, random_state=42)\nrf_model.fit(X_train_scaled, y_train)\n\n# 计算特征重要性\nrf_importances = pd.DataFrame()\nrf_importances['Feature'] = X.columns\nrf_importances['Importance'] = rf_model.feature_importances_\nrf_importances = rf_importances.sort_values('Importance', ascending=False)\n\n# 显示最重要的特征\nprint(rf_importances.head(10))\n\n# 可视化特征重要性\nplt.figure(figsize=(12, 8))\ntop_features = rf_importances.head(15)['Feature'].tolist()\nsns.barplot(x='Importance', y='Feature', data=rf_importances[rf_importances['Feature'].isin(top_features)])\nplt.title('随机森林特征重要性')\nplt.xlabel('重要性')\nplt.tight_layout()\nplt.show()\n\n# 3.2 递归特征消除(RFE)\nprint(\"\\n递归特征消除(RFE):\")\nlogit_model = LogisticRegression(max_iter=1000, random_state=42)\nrfe = RFE(estimator=logit_model, n_features_to_select=15, step=1)\nrfe.fit(X_train_scaled, y_train)\n\n# 获取RFE结果\nrfe_results = pd.DataFrame()\nrfe_results['Feature'] = X.columns\nrfe_results['Selected'] = rfe.support_\nrfe_results['Rank'] = rfe.ranking_\nrfe_results = rfe_results.sort_values('Rank')\n\n# 显示被选中的特征\nprint(rfe_results[rfe_results['Selected'] == True])\n\n# 4. 特征重要性综合分析\nprint(\"\\n=== 特征重要性综合分析 ===\")\n\n# 创建一个综合得分\nfeature_importance = pd.DataFrame({'Feature': X.columns})\n\n# 添加各种特征选择方法的排名\nif numeric_cols:\n    corr_df['Corr_Rank'] = corr_df['Correlation'].rank(ascending=False)\n    feature_importance = feature_importance.merge(corr_df[['Feature', 'Corr_Rank']], on='Feature', how='left')\n\nanova_scores['Anova_Rank'] = anova_scores['F_Value'].rank(ascending=False)\nfeature_importance = feature_importance.merge(anova_scores[['Feature', 'Anova_Rank']], on='Feature', how='left')\n\nmi_scores['MI_Rank'] = mi_scores['MI_Score'].rank(ascending=False)\nfeature_importance = feature_importance.merge(mi_scores[['Feature', 'MI_Rank']], on='Feature', how='left')\n\nrf_importances['RF_Rank'] = rf_importances['Importance'].rank(ascending=False)\nfeature_importance = feature_importance.merge(rf_importances[['Feature', 'RF_Rank']], on='Feature', how='left')\n\nfeature_importance = feature_importance.merge(rfe_results[['Feature', 'Rank']], on='Feature', how='left')\nfeature_importance.rename(columns={'Rank': 'RFE_Rank'}, inplace=True)\n\n# 计算平均排名\nrank_columns = [col for col in feature_importance.columns if col.endswith('_Rank')]\nfeature_importance['Avg_Rank'] = feature_importance[rank_columns].mean(axis=1)\nfeature_importance = feature_importance.sort_values('Avg_Rank')\n\n# 显示综合排名前20的特征\nprint(\"综合特征重要性排名(前20名):\")\nprint(feature_importance.head(20))\n\n# 可视化综合排名靠前的特征\nplt.figure(figsize=(12, 8))\ntop_features = feature_importance.head(15)['Feature'].tolist()\nsns.barplot(y='Feature', x='Avg_Rank', data=feature_importance[feature_importance['Feature'].isin(top_features)])\nplt.title('特征重要性综合排名')\nplt.xlabel('平均排名(数值越小越重要)')\nplt.tight_layout()\nplt.show()\n\n# 5. 最终特征选择\nprint(\"\\n=== 最终特征选择 ===\")\n# 选择排名前15的特征\ntop_n_features = 15\nselected_features = feature_importance.head(top_n_features)['Feature'].tolist()\n\nprint(f\"选择的{top_n_features}个最重要特征:\")\nfor i, feature in enumerate(selected_features, 1):\n    print(f\"{i}. {feature}\")\n\n# 检查所选特征间的相关性\nif len(numeric_cols) > 0:\n    correlation_matrix = X[selected_features].corr()\n    \n    # 可视化相关性矩阵\n    plt.figure(figsize=(12, 10))\n    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n    plt.title('所选特征间的相关性矩阵')\n    plt.tight_layout()\n    plt.show()\n    \n    # 检测高度相关的特征对\n    print(\"\\n高度相关的特征对(相关性绝对值>0.8):\")\n    high_corr_found = False\n    \n    for i in range(len(selected_features)):\n        for j in range(i+1, len(selected_features)):\n            corr = correlation_matrix.iloc[i, j]\n            if abs(corr) > 0.8:\n                print(f\"- {selected_features[i]} 与 {selected_features[j]}: {corr:.3f}\")\n                high_corr_found = True\n    \n    if not high_corr_found:\n        print(\"未发现高度相关的特征对\")\n\n# 6. 特征与流失的可视化分析\nprint(\"\\n=== 特征与流失的关系可视化 ===\")\n\n# 选择前几个最重要的特征进行可视化\nn_features_to_plot = min(6, len(selected_features))  # 最多展示6个特征\n\n# 为分类特征生成箱型图，为连续特征生成直方图\nplt.figure(figsize=(15, n_features_to_plot * 4))\n\nfor i, feature in enumerate(selected_features[:n_features_to_plot]):\n    if feature in df.columns:  # 确保特征在原始数据框中\n        feature_data = df[[feature, 'Churn']]\n        \n        # 移除可能的缺失值\n        feature_data = feature_data.dropna()\n        \n        # 根据特征类型选择不同的可视化方法\n        if feature_data[feature].dtype in ['int64', 'float64'] and feature_data[feature].nunique() > 10:\n            # 连续数值特征 - 使用直方图和密度图\n            plt.subplot(n_features_to_plot, 2, 2*i+1)\n            sns.histplot(data=feature_data, x=feature, hue='Churn', kde=True, bins=30, palette=['lightblue', 'salmon'])\n            plt.title(f'{feature}的分布与流失关系')\n            \n            # 箱形图\n            plt.subplot(n_features_to_plot, 2, 2*i+2)\n            sns.boxplot(x='Churn', y=feature, data=feature_data, palette=['lightblue', 'salmon'])\n            plt.title(f'{feature}在不同流失状态下的分布')\n            \n        else:\n            # 分类特征或低基数特征 - 使用计数图和百分比堆叠图\n            plt.subplot(n_features_to_plot, 2, 2*i+1)\n            sns.countplot(x=feature, hue='Churn', data=feature_data, palette=['lightblue', 'salmon'])\n            plt.title(f'{feature}类别与流失关系')\n            plt.xticks(rotation=45)\n            \n            # 按类别的流失率\n            plt.subplot(n_features_to_plot, 2, 2*i+2)\n            category_churn = feature_data.groupby(feature)['Churn'].mean() * 100\n            sns.barplot(x=category_churn.index, y=category_churn.values, palette='YlOrRd')\n            plt.title(f'按{feature}分类的流失率(%)')\n            plt.xticks(rotation=45)\n            plt.ylabel('流失率 (%)')\n\nplt.tight_layout()\nplt.show()\n\n# 7. 结论与建议\nprint(\"\\n=== 结论与建议 ===\")\nprint(\"基于预警变量筛选的结论:\")\nprint(f\"1. 我们从{len(feature_cols)}个初始特征中筛选出{len(selected_features)}个最具预测价值的特征\")\nprint(\"2. 主要的预警指标包括:\")\n\n# 展示前5个重要特征及其解释\ntop_5_features = feature_importance.head(5)['Feature'].tolist()\nfor i, feature in enumerate(top_5_features, 1):\n    # 这里可以添加对每个特征的业务解释\n    print(f\"   {i}. {feature}\")\n\nprint(\"\\n3. 预警模型构建建议:\")\nprint(\"   - 使用筛选的特征集构建预警模型\")\nprint(\"   - 重点关注交易行为的变化趋势特征\")\nprint(\"   - 考虑客户生命周期的不同阶段特征\")\nprint(\"   - 对相关性高的特征考虑进行特征工程或只选择其中一个\")\n\nprint(\"\\n4. 业务应用建议:\")\nprint(\"   - 建立以上述关键指标为基础的客户流失风险监控仪表板\")\nprint(\"   - 设置差异化的预警阈值，并定期对阈值进行校准\")\nprint(\"   - 将预警结果与客户价值结合，优先关注高价值客户\")"
        },
        {
          "id": 4,
          "title": "预警模型构建",
          "description": "使用随机森林、逻辑回归等算法构建流失预警模型",
          "example_code": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\nfrom sklearn.metrics import confusion_matrix, classification_report, roc_curve, precision_recall_curve\nfrom sklearn.pipeline import Pipeline\nfrom imblearn.over_sampling import SMOTE\nimport matplotlib as mpl\nfrom app.services.docker_matplotlib_fix import configure_matplotlib_fonts\n\n# 配置字体以支持中文显示\nconfigure_matplotlib_fonts()\nfrom app.services.bank_analysis import bank_service\n\n# 加载客户数据\ndf = bank_service.load_customer_churn_data()\nprint(f\"加载数据集，共{df.shape[0]}行，{df.shape[1]}列\")\n\n# 1. 数据准备\nprint(\"\\n=== 数据准备 ===\")\n\n# 检查目标变量\nif 'Churn' not in df.columns:\n    print(\"错误: 数据集中缺少目标变量'Churn'\")\n    exit()\n\n# 检查流失率\nchurn_rate = df['Churn'].mean() * 100\nprint(f\"客户流失率: {churn_rate:.2f}%\")\n\n# 使用之前步骤中筛选出的重要特征\n# 注意：这里假设我们已经从上一步中得到了重要特征列表\n# 在实际应用中，可以从上一步中加载或者重新筛选特征\nimportant_features = [\n    'DaysSinceLastTransaction', 'TransactionFrequencyDecline', 'TransactionAmountDecline',\n    'Tenure', 'CustomerValue', 'TransactionFrequency', 'TransactionAmount',\n    'SupportCallsCount', 'ProductCount', 'Age', 'ComplaintsCount',\n    'IsDigitalActive', 'IsMobileActive', 'HasCreditCard', 'HasLoan'\n]\n\n# 检查所有重要特征是否在数据集中\nmissing_features = [feature for feature in important_features if feature not in df.columns]\nif missing_features:\n    print(f\"警告: 以下特征不在数据集中: {missing_features}\")\n    important_features = [feature for feature in important_features if feature in df.columns]\n\n# 如果没有足够的特征，使用所有可用的数值特征\nif len(important_features) < 5:\n    print(\"特征太少，将使用所有数值特征\")\n    important_features = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n    if 'Churn' in important_features:\n        important_features.remove('Churn')\n\nprint(f\"使用{len(important_features)}个特征构建预警模型:\")\nfor i, feature in enumerate(important_features, 1):\n    print(f\"{i}. {feature}\")\n\n# 准备特征和目标变量\nX = df[important_features]\ny = df['Churn']\n\n# 划分训练集和测试集\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=42, stratify=y\n)\nprint(f\"训练集: {X_train.shape[0]}行, 测试集: {X_test.shape[0]}行\")\n\n# 检查类别不平衡\nneg_count = (y_train == 0).sum()\npos_count = (y_train == 1).sum()\nratio = neg_count / pos_count\nprint(f\"训练集中未流失客户: {neg_count}, 流失客户: {pos_count}, 比例: {ratio:.2f}:1\")\n\n# 2. 创建评估模型的函数\ndef evaluate_model(model, X_train, X_test, y_train, y_test, model_name, use_smote=False):\n    # 复制数据，避免修改原始数据\n    X_train_copy = X_train.copy()\n    y_train_copy = y_train.copy()\n    \n    # 如果使用SMOTE处理类别不平衡\n    if use_smote:\n        print(f\"\\n对{model_name}使用SMOTE处理类别不平衡...\")\n        smote = SMOTE(random_state=42)\n        X_train_copy, y_train_copy = smote.fit_resample(X_train_copy, y_train_copy)\n        print(f\"SMOTE后，训练集形状: {X_train_copy.shape}\")\n        print(f\"流失占比: {y_train_copy.mean():.2f}\")\n    \n    # 训练模型\n    print(f\"\\n训练{model_name}...\")\n    model.fit(X_train_copy, y_train_copy)\n    \n    # 预测\n    y_pred = model.predict(X_test)\n    \n    # 获取预测概率（如果模型支持）\n    if hasattr(model, 'predict_proba'):\n        y_prob = model.predict_proba(X_test)[:, 1]\n    else:\n        y_prob = model.decision_function(X_test) if hasattr(model, 'decision_function') else y_pred\n    \n    # 计算评估指标\n    accuracy = accuracy_score(y_test, y_pred)\n    precision = precision_score(y_test, y_pred)\n    recall = recall_score(y_test, y_pred)\n    f1 = f1_score(y_test, y_pred)\n    auc = roc_auc_score(y_test, y_prob)\n    \n    # 输出评估结果\n    print(f\"\\n{model_name}评估结果:\")\n    print(f\"准确率(Accuracy): {accuracy:.4f}\")\n    print(f\"精确率(Precision): {precision:.4f}\")\n    print(f\"召回率(Recall): {recall:.4f}\")\n    print(f\"F1分数: {f1:.4f}\")\n    print(f\"ROC AUC: {auc:.4f}\")\n    \n    # 混淆矩阵\n    cm = confusion_matrix(y_test, y_pred)\n    tn, fp, fn, tp = cm.ravel()\n    print(f\"\\n混淆矩阵:\")\n    print(f\"真负例(TN): {tn}\")\n    print(f\"假正例(FP): {fp}\")\n    print(f\"假负例(FN): {fn}\")\n    print(f\"真正例(TP): {tp}\")\n    \n    # 计算特定业务指标\n    print(\"\\n业务指标:\")\n    # 假设流失客户的平均损失是5000元，误判非流失的成本是1000元\n    cost_fn = 5000  # 假负例成本 (错过流失客户的损失)\n    cost_fp = 1000  # 假正例成本 (对非流失客户采取不必要措施的成本)\n    total_cost = (fn * cost_fn) + (fp * cost_fp)\n    print(f\"总错误成本: {total_cost}元 (FN: {fn*cost_fn}元, FP: {fp*cost_fp}元)\")\n    \n    return {\n        'model': model,\n        'name': model_name,\n        'accuracy': accuracy,\n        'precision': precision,\n        'recall': recall,\n        'f1': f1,\n        'auc': auc,\n        'confusion_matrix': cm,\n        'y_pred': y_pred,\n        'y_prob': y_prob,\n        'cost': total_cost\n    }\n\n# 3. 训练和评估多个模型\nprint(\"\\n=== 训练和评估模型 ===\")\n\n# 3.1 逻辑回归模型\npipeline_lr = Pipeline([\n    ('scaler', StandardScaler()),\n    ('classifier', LogisticRegression(max_iter=1000, random_state=42))\n])\nresults_lr = evaluate_model(\n    pipeline_lr, X_train, X_test, y_train, y_test, \"逻辑回归\", use_smote=True\n)\n\n# 3.2 随机森林模型\npipeline_rf = Pipeline([\n    ('scaler', StandardScaler()),\n    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))\n])\nresults_rf = evaluate_model(\n    pipeline_rf, X_train, X_test, y_train, y_test, \"随机森林\", use_smote=True\n)\n\n# 3.3 梯度提升树模型\npipeline_gbt = Pipeline([\n    ('scaler', StandardScaler()),\n    ('classifier', GradientBoostingClassifier(n_estimators=100, random_state=42))\n])\nresults_gbt = evaluate_model(\n    pipeline_gbt, X_train, X_test, y_train, y_test, \"梯度提升树\", use_smote=True\n)\n\n# 3.4 支持向量机模型\npipeline_svm = Pipeline([\n    ('scaler', StandardScaler()),\n    ('classifier', SVC(probability=True, random_state=42))\n])\nresults_svm = evaluate_model(\n    pipeline_svm, X_train, X_test, y_train, y_test, \"支持向量机\", use_smote=True\n)\n\n# 4. 比较模型性能\nprint(\"\\n=== 模型性能比较 ===\")\nresults = [results_lr, results_rf, results_gbt, results_svm]\nmodel_names = [result['name'] for result in results]\naccuracies = [result['accuracy'] for result in results]\nprecisions = [result['precision'] for result in results]\nrecalls = [result['recall'] for result in results]\nf1_scores = [result['f1'] for result in results]\naucs = [result['auc'] for result in results]\ncosts = [result['cost'] for result in results]\n\n# 创建比较表格\ncomparison_df = pd.DataFrame({\n    '模型': model_names,\n    '准确率': accuracies,\n    '精确率': precisions,\n    '召回率': recalls,\n    'F1分数': f1_scores,\n    'ROC AUC': aucs,\n    '错误成本': costs\n})\nprint(comparison_df)\n\n# 可视化比较结果\nplt.figure(figsize=(15, 10))\n\n# 准确率比较\nplt.subplot(2, 2, 1)\nsns.barplot(x='模型', y='准确率', data=comparison_df)\nplt.title('模型准确率比较')\nplt.ylim(0.7, 1.0)\n\n# 精确率和召回率比较\nplt.subplot(2, 2, 2)\ncomparison_plot = pd.melt(comparison_df, id_vars=['模型'], value_vars=['精确率', '召回率'], var_name='指标', value_name='值')\nsns.barplot(x='模型', y='值', hue='指标', data=comparison_plot)\nplt.title('模型精确率和召回率比较')\nplt.ylim(0.0, 1.0)\n\n# F1和AUC比较\nplt.subplot(2, 2, 3)\ncomparison_plot = pd.melt(comparison_df, id_vars=['模型'], value_vars=['F1分数', 'ROC AUC'], var_name='指标', value_name='值')\nsns.barplot(x='模型', y='值', hue='指标', data=comparison_plot)\nplt.title('模型F1分数和ROC AUC比较')\nplt.ylim(0.0, 1.0)\n\n# 成本比较\nplt.subplot(2, 2, 4)\nsns.barplot(x='模型', y='错误成本', data=comparison_df, palette='YlOrRd_r')\nplt.title('模型错误成本比较')\nplt.ylabel('错误成本(元)')\n\nplt.tight_layout()\nplt.show()\n\n# 5. 模型调优 (以最佳模型为例)\n# 假设随机森林是初步评估中的最佳模型\nprint(\"\\n=== 随机森林模型调优 ===\")\n\n# 设定参数网格\nparam_grid = {\n    'classifier__n_estimators': [50, 100, 200],\n    'classifier__max_depth': [None, 10, 20, 30],\n    'classifier__min_samples_split': [2, 5, 10],\n    'classifier__min_samples_leaf': [1, 2, 4],\n    'classifier__class_weight': [None, 'balanced', 'balanced_subsample']\n}\n\n# 使用网格搜索和交叉验证\nprint(\"执行网格搜索交叉验证来优化随机森林参数 (这可能需要一些时间)...\")\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\ngrid_search = GridSearchCV(\n    pipeline_rf,\n    param_grid=param_grid,\n    scoring='f1',  # 使用F1分数作为评估指标\n    cv=cv,\n    n_jobs=-1,  # 使用所有可用CPU\n    verbose=1\n)\n\n# 使用SMOTE处理过的数据进行训练\nsmote = SMOTE(random_state=42)\nX_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\ngrid_search.fit(X_train_smote, y_train_smote)\n\n# 输出最佳参数\nprint(f\"\\n最佳参数: {grid_search.best_params_}\")\nprint(f\"最佳交叉验证F1分数: {grid_search.best_score_:.4f}\")\n\n# 使用最佳参数的模型进行评估\nbest_rf = grid_search.best_estimator_\nresults_best_rf = evaluate_model(\n    best_rf, X_train, X_test, y_train, y_test, \"调优后的随机森林\", use_smote=True\n)\n\n# 6. ROC曲线和PR曲线分析\nprint(\"\\n=== ROC曲线和PR曲线分析 ===\")\nplt.figure(figsize=(15, 6))\n\n# ROC曲线\nplt.subplot(1, 2, 1)\nfor result in results + [results_best_rf]:\n    fpr, tpr, _ = roc_curve(y_test, result['y_prob'])\n    plt.plot(fpr, tpr, label=f'{result[\"name\"]} (AUC = {result[\"auc\"]:.3f})')\n\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlabel('假正例率(False Positive Rate)')\nplt.ylabel('真正例率(True Positive Rate)')\nplt.title('ROC曲线比较')\nplt.legend()\nplt.grid(alpha=0.3)\n\n# PR曲线\nplt.subplot(1, 2, 2)\nfor result in results + [results_best_rf]:\n    precision_curve, recall_curve, _ = precision_recall_curve(y_test, result['y_prob'])\n    plt.plot(recall_curve, precision_curve, label=f'{result[\"name\"]} (F1 = {result[\"f1\"]:.3f})')\n\nplt.xlabel('召回率(Recall)')\nplt.ylabel('精确率(Precision)')\nplt.title('精确率-召回率曲线比较')\nplt.legend()\nplt.grid(alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# 7. 阈值分析 (以最佳模型为例)\nprint(\"\\n=== 预测阈值分析 ===\")\n# 获取最佳模型的预测概率\nbest_model = results_best_rf\ny_prob = best_model['y_prob']\n\n# 测试不同的阈值\nthresholds = np.arange(0.1, 1.0, 0.1)\nthreshold_metrics = []\n\nfor threshold in thresholds:\n    # 根据阈值生成预测\n    y_pred_threshold = (y_prob >= threshold).astype(int)\n    \n    # 计算指标\n    accuracy = accuracy_score(y_test, y_pred_threshold)\n    precision = precision_score(y_test, y_pred_threshold)\n    recall = recall_score(y_test, y_pred_threshold)\n    f1 = f1_score(y_test, y_pred_threshold)\n    \n    # 计算成本\n    cm = confusion_matrix(y_test, y_pred_threshold)\n    tn, fp, fn, tp = cm.ravel()\n    cost_fn = 5000  # 假负例成本\n    cost_fp = 1000  # 假正例成本\n    total_cost = (fn * cost_fn) + (fp * cost_fp)\n    \n    threshold_metrics.append({\n        '阈值': threshold,\n        '准确率': accuracy,\n        '精确率': precision,\n        '召回率': recall,\n        'F1分数': f1,\n        '假正例(FP)': fp,\n        '假负例(FN)': fn,\n        '错误成本': total_cost\n    })\n\n# 创建阈值分析表格\nthreshold_df = pd.DataFrame(threshold_metrics)\nprint(threshold_df)\n\n# 可视化阈值分析\nplt.figure(figsize=(15, 10))\n\n# 绘制准确率、精确率、召回率和F1随阈值的变化\nplt.subplot(2, 2, 1)\nplt.plot(threshold_df['阈值'], threshold_df['准确率'], marker='o', label='准确率')\nplt.plot(threshold_df['阈值'], threshold_df['精确率'], marker='s', label='精确率')\nplt.plot(threshold_df['阈值'], threshold_df['召回率'], marker='^', label='召回率')\nplt.plot(threshold_df['阈值'], threshold_df['F1分数'], marker='*', label='F1分数')\nplt.xlabel('预测阈值')\nplt.ylabel('指标值')\nplt.title('性能指标与预测阈值的关系')\nplt.legend()\nplt.grid(alpha=0.3)\n\n# 绘制FP和FN随阈值的变化\nplt.subplot(2, 2, 2)\nplt.plot(threshold_df['阈值'], threshold_df['假正例(FP)'], marker='o', label='假正例(FP)')\nplt.plot(threshold_df['阈值'], threshold_df['假负例(FN)'], marker='s', label='假负例(FN)')\nplt.xlabel('预测阈值')\nplt.ylabel('错误计数')\nplt.title('错误类型与预测阈值的关系')\nplt.legend()\nplt.grid(alpha=0.3)\n\n# 绘制错误成本随阈值的变化\nplt.subplot(2, 2, 3)\nplt.plot(threshold_df['阈值'], threshold_df['错误成本'], marker='o', color='red')\nplt.xlabel('预测阈值')\nplt.ylabel('错误成本(元)')\nplt.title('错误成本与预测阈值的关系')\nplt.grid(alpha=0.3)\n\n# 找出成本最低的阈值\nmin_cost_threshold = threshold_df.loc[threshold_df['错误成本'].idxmin(), '阈值']\nmin_cost = threshold_df['错误成本'].min()\nplt.axvline(min_cost_threshold, color='green', linestyle='--', label=f'最优阈值={min_cost_threshold}')\nplt.legend()\n\n# 显示不同阈值下的混淆矩阵热图\nplt.subplot(2, 2, 4)\nopt_index = threshold_df['阈值'].tolist().index(min_cost_threshold)\ny_pred_opt = (y_prob >= min_cost_threshold).astype(int)\ncm_opt = confusion_matrix(y_test, y_pred_opt)\nsns.heatmap(cm_opt, annot=True, fmt='d', cmap='Blues')\nplt.title(f'最优阈值({min_cost_threshold})下的混淆矩阵')\nplt.xlabel('预测标签')\nplt.ylabel('真实标签')\n\nplt.tight_layout()\nplt.show()\n\n# 8. 特征重要性分析（针对随机森林模型）\nprint(\"\\n=== 特征重要性分析 ===\")\n\n# 获取最佳随机森林模型的特征重要性\nbest_rf_classifier = best_rf.named_steps['classifier']\nfeature_importances = best_rf_classifier.feature_importances_\n\n# 创建特征重要性数据框\nimportance_df = pd.DataFrame({\n    '特征': important_features,\n    '重要性': feature_importances\n})\nimportance_df = importance_df.sort_values('重要性', ascending=False)\n\n# 显示特征重要性\nprint(\"随机森林特征重要性:\")\nprint(importance_df)\n\n# 可视化特征重要性\nplt.figure(figsize=(12, 8))\nsns.barplot(x='重要性', y='特征', data=importance_df)\nplt.title('随机森林特征重要性')\nplt.xlabel('重要性')\nplt.tight_layout()\nplt.show()\n\n# 9. 最终模型保存和使用示例\nprint(\"\\n=== 最终模型 ===\")\n\n# 使用最优阈值\noptimal_threshold = min_cost_threshold\nprint(f\"最优预测阈值: {optimal_threshold}\")\n\n# 应用最终模型进行预测的示例\ndef predict_churn_risk(model, threshold, customer_data):\n    \"\"\"使用训练好的模型和优化的阈值预测客户流失风险\"\"\"\n    # 确保数据格式正确\n    if isinstance(customer_data, pd.DataFrame):\n        # 确保包含所有需要的特征\n        missing_cols = set(important_features) - set(customer_data.columns)\n        if missing_cols:\n            raise ValueError(f\"客户数据缺少以下特征: {missing_cols}\")\n        \n        # 仅选择模型所需特征\n        X = customer_data[important_features]\n    else:\n        raise ValueError(\"客户数据必须是pandas DataFrame格式\")\n    \n    # 预测概率\n    churn_proba = model.predict_proba(X)[:, 1]\n    \n    # 根据阈值确定预测结果\n    predictions = (churn_proba >= threshold).astype(int)\n    \n    # 返回预测结果和概率\n    results = pd.DataFrame({\n        'customer_id': customer_data.index if customer_data.index.name else range(len(X)),\n        'churn_probability': churn_proba,\n        'churn_prediction': predictions\n    })\n    \n    # 添加风险等级\n    results['risk_level'] = pd.cut(\n        results['churn_probability'],\n        bins=[0, 0.3, 0.6, 0.8, 1.0],\n        labels=['低风险', '中风险', '高风险', '极高风险']\n    )\n    \n    return results\n\n# 演示最终模型预测\nprint(\"\\n预测示例:\")\n# 使用测试集的前10行作为示例\nsample_customers = X_test.head(10).copy()\n\n# 预测这些客户的流失风险\nprediction_results = predict_churn_risk(best_rf, optimal_threshold, sample_customers)\n\n# 添加实际标签用于比较\nprediction_results['actual_churn'] = y_test.head(10).values\n\n# 显示预测结果\nprint(prediction_results[['churn_probability', 'churn_prediction', 'risk_level', 'actual_churn']])\n\n# 10. 模型总结与业务建议\nprint(\"\\n=== 模型总结与业务建议 ===\")\nprint(\"客户流失预警模型总结:\")\nprint(f\"1. 最佳模型: {best_model['name']}\")\nprint(f\"2. 模型性能: F1分数 = {best_model['f1']:.4f}, ROC AUC = {best_model['auc']:.4f}\")\nprint(f\"3. 最优预测阈值: {optimal_threshold} (基于错误成本最小化)\")\nprint(\"4. 关键预警特征:\")\nfor i, row in importance_df.head(5).iterrows():\n    print(f\"   - {row['特征']}: {row['重要性']:.4f}\")\n\nprint(\"\\n客户挽留策略建议:\")\nprint(\"1. 对于高风险和极高风险客户，实施主动挽留措施\")\nprint(\"2. 针对不同风险等级的客户制定差异化的挽留策略\")\nprint(\"3. 定期更新和评估模型，调整预测阈值以适应业务变化\")\nprint(\"4. 结合客户价值，优先挽留高价值且高流失风险的客户\")\nprint(\"5. 将流失风险预测集成到客户关系管理系统中，支持业务决策\")"
        },
        {
          "id": 5,
          "title": "模型验证",
          "description": "使用ROC曲线、混淆矩阵等方法评估模型效果"
        },
        {
          "id": 6,
          "title": "客户挽留策略",
          "description": "基于模型结果设计针对性的客户挽留方案"
        }
      ]
    },
    {
      "id": 3,
      "title": "银行信用欺诈数据分析",
      "description": "分析并检测银行信用卡交易中的欺诈行为",
      "category": "bank",
      "difficulty_level": 3,
      "estimated_duration": 130,
      "prerequisites": "数据挖掘和异常检测基础知识",
      "data_source": "银行信用卡交易数据",
      "steps": [
        {
          "id": 1,
          "title": "欺诈类型分析",
          "description": "了解不同类型的银行信用欺诈手段和特征"
        },
        {
          "id": 2,
          "title": "交易数据处理",
          "description": "清洗和准备交易数据，处理时间特征"
        },
        {
          "id": 3,
          "title": "异常特征提取",
          "description": "提取能够识别欺诈交易的关键特征"
        },
        {
          "id": 4,
          "title": "欺诈检测建模",
          "description": "构建欺诈检测模型，解决不平衡数据问题"
        },
        {
          "id": 5,
          "title": "模型调优与评估",
          "description": "使用精确率-召回率和AUC评估模型性能"
        },
        {
          "id": 6,
          "title": "欺诈预警系统",
          "description": "设计实时欺诈交易监控与预警系统架构"
        }
      ]
    },
    {
      "id": 4,
      "title": "股票技术指标分析",
      "description": "计算和分析股票技术指标，包括移动平均线、MACD、RSI等",
      "category": "security",
      "difficulty_level": 2,
      "estimated_duration": 90,
      "prerequisites": "基本的金融和股票知识",
      "data_source": "股票历史价格数据",
      "steps": [
        {
          "id": 1,
          "title": "数据获取",
          "description": "获取股票历史价格数据"
        },
        {
          "id": 2,
          "title": "趋势指标计算",
          "description": "计算移动平均线、MACD等趋势指标"
        },
        {
          "id": 3,
          "title": "摆动指标计算",
          "description": "计算RSI、KDJ等摆动指标"
        },
        {
          "id": 4,
          "title": "指标可视化",
          "description": "绘制技术指标和价格图表"
        },
        {
          "id": 5,
          "title": "交易信号分析",
          "description": "分析指标产生的交易信号"
        },
        {
          "id": 6,
          "title": "回测策略",
          "description": "回测基于技术指标的交易策略"
        }
      ]
    },
    {
      "id": 5,
      "title": "投资组合优化",
      "description": "使用马科维茨模型和夏普比率优化股票投资组合",
      "category": "security",
      "difficulty_level": 4,
      "estimated_duration": 150,
      "prerequisites": "投资理论和基础统计知识",
      "data_source": "多只股票历史收益率数据",
      "steps": [
        {
          "id": 1,
          "title": "数据准备",
          "description": "获取多只股票的历史价格和收益率"
        },
        {
          "id": 2,
          "title": "风险和收益计算",
          "description": "计算各股票的预期收益和风险"
        },
        {
          "id": 3,
          "title": "相关性分析",
          "description": "分析股票间的相关性"
        },
        {
          "id": 4,
          "title": "效率前沿构建",
          "description": "构建投资组合的效率前沿"
        },
        {
          "id": 5,
          "title": "最优组合确定",
          "description": "确定最优夏普比率的投资组合"
        },
        {
          "id": 6,
          "title": "投资组合评估",
          "description": "评估最优投资组合的表现"
        }
      ]
    },
    {
      "id": 6,
      "title": "车险索赔率影响因素分析",
      "description": "分析影响车险索赔率的因素，建立预测模型",
      "category": "insurance",
      "difficulty_level": 3,
      "estimated_duration": 120,
      "prerequisites": "基本的统计分析和机器学习知识",
      "data_source": "车险客户和索赔数据",
      "steps": [
        {
          "id": 1,
          "title": "数据探索",
          "description": "探索车险客户和索赔数据"
        },
        {
          "id": 2,
          "title": "索赔率计算",
          "description": "计算不同因素下的索赔率"
        },
        {
          "id": 3,
          "title": "特征重要性分析",
          "description": "分析影响索赔率的重要因素"
        },
        {
          "id": 4,
          "title": "预测模型构建",
          "description": "构建索赔金额预测模型"
        },
        {
          "id": 5,
          "title": "风险分群",
          "description": "对客户进行风险分群"
        },
        {
          "id": 6,
          "title": "保险定价策略",
          "description": "基于分析结果制定保险定价策略"
        }
      ]
    },
    {
      "id": 7,
      "title": "医疗保险精准营销分析",
      "description": "使用客户分群和预测模型进行医疗保险精准营销",
      "category": "insurance",
      "difficulty_level": 3,
      "estimated_duration": 120,
      "prerequisites": "基本的数据分析和机器学习知识",
      "data_source": "医疗保险客户数据",
      "steps": [
        {
          "id": 1,
          "title": "数据探索",
          "description": "探索医疗保险客户数据"
        },
        {
          "id": 2,
          "title": "客户分群",
          "description": "使用K-means对客户进行分群"
        },
        {
          "id": 3,
          "title": "购买意向预测",
          "description": "构建购买意向预测模型"
        },
        {
          "id": 4,
          "title": "分群特征分析",
          "description": "分析各群体的特征和行为"
        },
        {
          "id": 5,
          "title": "营销建议生成",
          "description": "为各客户群体生成营销建议"
        },
        {
          "id": 6,
          "title": "营销效果评估",
          "description": "评估针对不同群体的营销策略效果"
        }
      ]
    }
  ]
}