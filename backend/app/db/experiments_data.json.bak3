{
  "experiments": [
    {
      "id": 1,
      "title": "银行信贷风险控制",
      "description": "使用机器学习模型预测信用卡客户违约风险",
      "category": "bank",
      "difficulty_level": 3,
      "estimated_duration": 120,
      "prerequisites": "基本的数据分析和机器学习知识",
      "data_source": "台湾信用卡数据集",
      "steps": [
        {
          "id": 1,
          "title": "数据探索",
          "description": "探索信用卡数据集，了解特征和目标变量",
          "example_code": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n# 使用通用字体设置，避免中文字体问题\nimport matplotlib as mpl\nmpl.rcParams['font.family'] = 'DejaVu Sans'\nimport seaborn as sns\nfrom app.services.bank_analysis import bank_service\n\n# 加载数据\ndf = bank_service.load_credit_data()\n\n# 1. 基本统计量\nprint(\"=== Basic Statistics ===\")\nprint(df.describe())\n\n# 2. 数据类型和基本信息\nprint(\"\\n=== Data Types ===\")\nprint(df.info())\n\n# 3. 目标变量分布\ntarget_counts = df['DEFAULT'].value_counts()\nprint(\"\\n=== Target Distribution ===\")\nprint(f\"Non-Default: {target_counts[0]} ({target_counts[0]/len(df)*100:.2f}%)\")\nprint(f\"Default: {target_counts[1]} ({target_counts[1]/len(df)*100:.2f}%)\")\n\n# 4. 异常值统计\nprint(\"\\n=== Outlier Statistics ===\")\nnumeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\nfor col in numeric_cols:\n    Q1 = df[col].quantile(0.25)\n    Q3 = df[col].quantile(0.75)\n    IQR = Q3 - Q1\n    lower_bound = Q1 - 1.5 * IQR\n    upper_bound = Q3 + 1.5 * IQR\n    outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)][col]\n    if not outliers.empty:\n        print(f\"{col} has {len(outliers)} outliers ({len(outliers)/len(df)*100:.2f}%)\")\n\n# 5. 重复值统计\nduplicates = df.duplicated().sum()\nprint(f\"\\n=== Duplicate Statistics ===\")\nprint(f\"Dataset has {duplicates} duplicated rows ({duplicates/len(df)*100:.2f}%)\")\n\n# 6. 缺失值统计\nmissing_values = df.isnull().sum()\nprint(\"\\n=== Missing Value Statistics ===\")\nif missing_values.sum() == 0:\n    print(\"No missing values in the dataset\")\nelse:\n    print(missing_values[missing_values > 0])\n\n# 7. 可视化部分特征分布\nplt.figure(figsize=(15, 10))\n\n# 年龄分布\nplt.subplot(2, 3, 1)\nsns.histplot(df['AGE'], kde=True)\nplt.title('Age Distribution')\n\n# 信用额度分布\nplt.subplot(2, 3, 2)\nsns.histplot(df['LIMIT_BAL'], kde=True)\nplt.title('Credit Limit Distribution')\n\n# 违约率与年龄的关系\nplt.subplot(2, 3, 3)\nsns.boxplot(x='DEFAULT', y='AGE', data=df)\nplt.title('Default Rate vs Age')\nplt.xlabel('Default Status (0=No, 1=Yes)')\n\n# 相关性分析\nplt.figure(figsize=(12, 10))\ncorrelation_matrix = df.corr()\nsns.heatmap(correlation_matrix, annot=False, cmap='coolwarm', linewidths=0.5)\nplt.title('Feature Correlation Heatmap')\nplt.show() "
        },
        {
          "id": 2,
          "title": "数据预处理",
          "description": "处理缺失值和异常值，标准化特征",
          "example_code": "import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\n# 使用通用字体设置，避免中文字体问题\nimport matplotlib as mpl\nmpl.rcParams['font.family'] = 'DejaVu Sans'\nfrom app.services.bank_analysis import bank_service\n\n# 加载数据\ndf = bank_service.load_credit_data()\nprint(\"Original data shape:\", df.shape)\n\n# 创建原始数据的副本\ndf_cleaned = df.copy()\n\n# 1. 检查并处理重复值\nduplicates = df_cleaned.duplicated().sum()\nif duplicates > 0:\n    print(f\"Found {duplicates} duplicate rows, removing...\")\n    df_cleaned = df_cleaned.drop_duplicates()\n\n# 2. 处理缺失值\nmissing_values = df_cleaned.isnull().sum()\nif missing_values.sum() > 0:\n    print(f\"Found {missing_values.sum()} missing values, processing...\")\n    # 对数值型变量用中位数填充\n    numeric_cols = df_cleaned.select_dtypes(include=[np.number]).columns\n    for col in numeric_cols:\n        if df_cleaned[col].isnull().sum() > 0:\n            median_value = df_cleaned[col].median()\n            df_cleaned[col].fillna(median_value, inplace=True)\nelse:\n    print(\"No missing values in the dataset\")\n\n# 3. 处理异常值\nprint(\"\\nProcessing outliers...\")\nnumeric_cols = df_cleaned.select_dtypes(include=[np.number]).columns.tolist()\nnumeric_cols.remove('DEFAULT')  # 不处理目标变量\n\nfor col in numeric_cols:\n    Q1 = df_cleaned[col].quantile(0.25)\n    Q3 = df_cleaned[col].quantile(0.75)\n    IQR = Q3 - Q1\n    lower_bound = Q1 - 1.5 * IQR\n    upper_bound = Q3 + 1.5 * IQR\n    \n    # 统计异常值\n    outliers = df_cleaned[(df_cleaned[col] < lower_bound) | (df_cleaned[col] > upper_bound)][col]\n    \n    if len(outliers) > 0:\n        print(f\"  - {col} has {len(outliers)} outliers ({len(outliers)/len(df_cleaned)*100:.2f}%)\")\n        \n        # 将异常值替换为边界值\n        df_cleaned.loc[df_cleaned[col] < lower_bound, col] = lower_bound\n        df_cleaned.loc[df_cleaned[col] > upper_bound, col] = upper_bound\n\n# 4. 数据标准化\nprint(\"\\nStart data standardization...\")\n# 分离特征和标签\nX = df_cleaned.drop('DEFAULT', axis=1)\ny = df_cleaned['DEFAULT']\n\n# 对数值类型的特征进行标准化\nnumeric_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n\n# 使用StandardScaler进行标准化\nscaler = StandardScaler()\nX_scaled = X.copy()\nX_scaled[numeric_cols] = scaler.fit_transform(X[numeric_cols])\n\nprint(f\"Standardized {len(numeric_cols)} numerical features\")\n\n# 合并回DataFrame\ndf_standardized = pd.concat([X_scaled, y], axis=1)\n\n# 对比标准化前后的描述性统计\nprint(\"\\nStatistics before standardization:\")\nprint(df_cleaned[numeric_cols].describe())\nprint(\"\\nStatistics after standardization:\")\nprint(df_standardized[numeric_cols].describe())\n\nprint(f\"\\nCleaning completed! Original data: {df.shape}, After cleaning: {df_standardized.shape}\")\n\n# 可视化标准化前后\nplt.figure(figsize=(15, 6))\n\n# 标准化前\nplt.subplot(1, 2, 1)\nplt.boxplot(df_cleaned[['LIMIT_BAL', 'AGE', 'BILL_AMT1']])\nplt.title('Before Standardization')\nplt.xticks([1, 2, 3], ['LIMIT_BAL', 'AGE', 'BILL_AMT1'])\n\n# 标准化后\nplt.subplot(1, 2, 2)\nplt.boxplot(df_standardized[['LIMIT_BAL', 'AGE', 'BILL_AMT1']])\nplt.title('After Standardization')\nplt.xticks([1, 2, 3], ['LIMIT_BAL', 'AGE', 'BILL_AMT1'])\n\nplt.tight_layout()\nplt.show() "
        },
        {
          "id": 3,
          "title": "特征工程",
          "description": "创建新特征，选择重要特征",
          "example_code": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.feature_selection import SelectKBest, f_classif\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.decomposition import PCA\n# 使用通用字体设置，避免中文字体问题\nimport matplotlib as mpl\nmpl.rcParams['font.family'] = 'DejaVu Sans'\nfrom app.services.bank_analysis import bank_service\n\n# 加载数据\ndf = bank_service.load_credit_data()\n\n# 创建数据副本\ndf_fe = df.copy()\n\n# 1. 特征创建\nprint(\"Creating new features...\")\n\n# 计算平均账单金额\ndf_fe['AVG_BILL_AMT'] = (df_fe['BILL_AMT1'] + df_fe['BILL_AMT2'] + df_fe['BILL_AMT3'] + \n                         df_fe['BILL_AMT4'] + df_fe['BILL_AMT5'] + df_fe['BILL_AMT6']) / 6\n\n# 计算平均支付金额\ndf_fe['AVG_PAY_AMT'] = (df_fe['PAY_AMT1'] + df_fe['PAY_AMT2'] + df_fe['PAY_AMT3'] + \n                        df_fe['PAY_AMT4'] + df_fe['PAY_AMT5'] + df_fe['PAY_AMT6']) / 6\n\n# 计算账单与支付比率\ndf_fe['BILL_PAY_RATIO'] = df_fe['AVG_PAY_AMT'] / df_fe['AVG_BILL_AMT'].replace(0, 0.01)\n\n# 计算最大延迟\ndelay_cols = ['PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6']\ndf_fe['MAX_DELAY'] = df_fe[delay_cols].max(axis=1)\n\n# 创建延迟标志\ndf_fe['HAS_DELAY'] = (df_fe[delay_cols] > 0).any(axis=1).astype(int)\n\n# 计算信用额度使用率\ndf_fe['CREDIT_UTILIZATION'] = df_fe['BILL_AMT1'] / df_fe['LIMIT_BAL'].replace(0, 0.01)\n\n# 输出新特征的描述性统计\nprint(\"\\nNew features statistics:\")\nnew_features = ['AVG_BILL_AMT', 'AVG_PAY_AMT', 'BILL_PAY_RATIO', 'MAX_DELAY', 'HAS_DELAY', 'CREDIT_UTILIZATION']\nprint(df_fe[new_features].describe())\n\n# 2. 特征交互 (多项式特征)\nprint(\"\\nCreating polynomial features...\")\n# 选择部分数值特征\nnumeric_feat = ['LIMIT_BAL', 'AGE', 'AVG_BILL_AMT', 'AVG_PAY_AMT']\n\n# 创建二阶多项式特征\npoly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\npoly_features = poly.fit_transform(df_fe[numeric_feat])\n\n# 创建特征名称\npoly_feat_names = poly.get_feature_names_out(numeric_feat)\n\n# 添加多项式特征到数据集\npoly_df = pd.DataFrame(poly_features, columns=poly_feat_names)\npoly_df = poly_df.iloc[:, len(numeric_feat):]  # 移除原特征，只保留交互项\n\n# 重置索引并连接\ndf_fe.reset_index(drop=True, inplace=True)\npoly_df.reset_index(drop=True, inplace=True)\ndf_fe = pd.concat([df_fe, poly_df], axis=1)\n\nprint(f\"Added {poly_df.shape[1]} interaction features\")\n\n# 3. 特征选择\nprint(\"\\nPerforming feature selection...\")\n# 准备特征和目标变量\nX = df_fe.drop('DEFAULT', axis=1)\ny = df_fe['DEFAULT']\n\n# 使用ANOVA F-value进行特征选择\nselector = SelectKBest(f_classif, k=15)  # 选择前15个最重要的特征\nX_new = selector.fit_transform(X, y)\n\n# 获取特征得分\nfeature_scores = pd.DataFrame()\nfeature_scores['Feature'] = X.columns\nfeature_scores['Score'] = selector.scores_\nfeature_scores = feature_scores.sort_values(by='Score', ascending=False)\n\nprint(\"\\nTop 15 important features:\")\nprint(feature_scores.head(15))\n\n# 选择的特征\nselected_features = X.columns[selector.get_support()].tolist()\nprint(\"\\nSelected features:\", selected_features)\n\n# 使用选择的特征创建最终数据集\nX_selected = X[selected_features]\nfinal_df = pd.concat([X_selected, y], axis=1)\n\nprint(f\"\\nFinal dataset shape: {final_df.shape}\")\n\n# 4. 可视化重要特征的分布\nplt.figure(figsize=(15, 10))\n\n# 展示前6个最重要的特征\ntop_features = feature_scores['Feature'].head(6).tolist()\n\nfor i, feature in enumerate(top_features):\n    plt.subplot(2, 3, i+1)\n    sns.histplot(data=df_fe, x=feature, hue='DEFAULT', bins=20, kde=True, palette=['blue', 'red'])\n    plt.title(f'Feature: {feature}')\n    plt.xlabel(feature)\n    plt.ylabel('Frequency')\n\nplt.tight_layout()\nplt.show()\n\n# 5. 特征降维 (PCA)\nplt.figure(figsize=(12, 5))\n\n# 选择数值特征进行PCA\nnum_features = X.select_dtypes(include=['float64', 'int64']).columns.tolist()\nX_num = X[num_features]\n\n# 标准化数据\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X_num)\n\n# 应用PCA\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X_scaled)\n\n# 创建PCA数据框\npca_df = pd.DataFrame(X_pca, columns=['PC1', 'PC2'])\npca_df['DEFAULT'] = y.values\n\n# 可视化PCA结果\nplt.subplot(1, 2, 1)\nsns.scatterplot(data=pca_df, x='PC1', y='PC2', hue='DEFAULT', palette=['blue', 'red'])\nplt.title('PCA: First two principal components')\n\n# 计算特征对主成分的贡献\nloadings = pd.DataFrame(\n    pca.components_.T,\n    columns=['PC1', 'PC2'],\n    index=num_features\n)\n\n# 可视化特征负荷\nplt.subplot(1, 2, 2)\nsns.heatmap(loadings, cmap='viridis')\nplt.title('Feature contributions to PCs')\n\nplt.tight_layout()\nplt.show()\n\n# 输出解释方差比例\nprint(f\"\\nVariance explained by PC1: {pca.explained_variance_ratio_[0]:.2f}\")\nprint(f\"Variance explained by PC2: {pca.explained_variance_ratio_[1]:.2f}\")\nprint(f\"Total variance explained: {sum(pca.explained_variance_ratio_):.2f}\")"
        },
        {
          "id": 4,
          "title": "模型训练",
          "description": "训练多种机器学习模型来预测违约风险",
          "example_code": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# 使用通用字体设置，避免中文字体问题\nimport matplotlib as mpl\nmpl.rcParams['font.family'] = 'DejaVu Sans'\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.preprocessing import StandardScaler\nfrom app.services.bank_analysis import bank_service\n\n# 加载数据\ndf = bank_service.load_credit_data()\n\n# 数据预处理\nprint(\"Data preprocessing...\")\n# 准备特征和目标变量\nX = df.drop('DEFAULT', axis=1)\ny = df['DEFAULT']\n\n# 标准化特征\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# 划分训练集和测试集\nX_train, X_test, y_train, y_test = train_test_split(\n    X_scaled, y, test_size=0.25, random_state=42, stratify=y\n)\n\nprint(f\"Training set: {X_train.shape}, Test set: {X_test.shape}\")\n\n# 定义评估模型的函数\ndef evaluate_model(model, X_train, X_test, y_train, y_test):\n    # 训练模型\n    model.fit(X_train, y_train)\n    \n    # 预测\n    y_pred = model.predict(X_test)\n    \n    # 评估指标\n    accuracy = accuracy_score(y_test, y_pred)\n    report = classification_report(y_test, y_pred)\n    cm = confusion_matrix(y_test, y_pred)\n    \n    # ROC曲线\n    if hasattr(model, 'predict_proba'):\n        y_proba = model.predict_proba(X_test)[:, 1]\n        fpr, tpr, _ = roc_curve(y_test, y_proba)\n        roc_auc = auc(fpr, tpr)\n    else:\n        fpr, tpr, roc_auc = None, None, None\n    \n    return {\n        'model': model,\n        'accuracy': accuracy,\n        'report': report,\n        'confusion_matrix': cm,\n        'roc': (fpr, tpr, roc_auc)\n    }\n\n# 训练和评估多个模型\nprint(\"\\nTraining models...\")\n\n# 1. 逻辑回归\nprint(\"Training Logistic Regression...\")\nlr = LogisticRegression(max_iter=1000, class_weight='balanced')\nlr_result = evaluate_model(lr, X_train, X_test, y_train, y_test)\n\n# 2. 随机森林\nprint(\"Training Random Forest...\")\nrf = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\nrf_result = evaluate_model(rf, X_train, X_test, y_train, y_test)\n\n# 3. 梯度提升树\nprint(\"Training Gradient Boosting...\")\ngb = GradientBoostingClassifier(n_estimators=100, random_state=42)\ngb_result = evaluate_model(gb, X_train, X_test, y_train, y_test)\n\n# 4. 支持向量机\nprint(\"Training SVM...\")\nsvm = SVC(probability=True, class_weight='balanced')\nsvm_result = evaluate_model(svm, X_train, X_test, y_train, y_test)\n\n# 输出模型结果\nmodels = [lr_result, rf_result, gb_result, svm_result]\nmodel_names = ['Logistic Regression', 'Random Forest', 'Gradient Boosting', 'SVM']\n\nprint(\"\\nModel comparison:\")\nfor name, result in zip(model_names, models):\n    print(f\"\\n{name}:\")\n    print(f\"Accuracy: {result['accuracy']:.4f}\")\n    print(\"Classification Report:\")\n    print(result['report'])\n\n# 可视化比较模型性能\nprint(\"\\nVisualizing model performance...\")\n\n# 1. 准确率比较\naccuracies = [result['accuracy'] for result in models]\n\nplt.figure(figsize=(10, 6))\nplt.bar(model_names, accuracies, color=['blue', 'green', 'red', 'purple'])\nplt.title('Model Accuracy Comparison')\nplt.ylabel('Accuracy')\nplt.ylim(0.5, 1.0)  # 设置y轴范围，更好地显示差异\nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.show()\n\n# 2. 混淆矩阵\nplt.figure(figsize=(12, 10))\n\nfor i, (name, result) in enumerate(zip(model_names, models)):\n    plt.subplot(2, 2, i+1)\n    cm = result['confusion_matrix']\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n    plt.title(f'{name} Confusion Matrix')\n    plt.xlabel('Predicted')\n    plt.ylabel('Actual')\n\nplt.tight_layout()\nplt.show()\n\n# 3. ROC曲线比较\nplt.figure(figsize=(10, 8))\n\nfor name, result in zip(model_names, models):\n    fpr, tpr, roc_auc = result['roc']\n    if fpr is not None:\n        plt.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.3f})')\n\nplt.plot([0, 1], [0, 1], 'k--')  # 对角线\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curves Comparison')\nplt.legend(loc='lower right')\nplt.grid(alpha=0.3)\nplt.show()\n\n# 4. 特征重要性（仅针对随机森林模型）\nrandom_forest = rf_result['model']\n\n# 获取特征重要性\nfeature_importances = random_forest.feature_importances_\n\n# 创建DataFrame用于可视化\nimportance_df = pd.DataFrame({\n    'Feature': X.columns,\n    'Importance': feature_importances\n})\nimportance_df = importance_df.sort_values('Importance', ascending=False).head(15)\n\nplt.figure(figsize=(12, 6))\nsns.barplot(x='Importance', y='Feature', data=importance_df, palette='viridis')\nplt.title('Random Forest Feature Importance')\nplt.tight_layout()\nplt.show()\n\n# 超参数调优（以随机森林为例）\nprint(\"\\nPerforming hyperparameter tuning for Random Forest...\")\n\n# 定义参数网格\nparam_grid = {\n    'n_estimators': [50, 100],\n    'max_depth': [None, 10, 20],\n    'min_samples_split': [2, 5],\n    'min_samples_leaf': [1, 2]\n}\n\n# 使用网格搜索和交叉验证\ngrid_search = GridSearchCV(\n    RandomForestClassifier(random_state=42, class_weight='balanced'),\n    param_grid=param_grid,\n    cv=5,\n    scoring='accuracy',\n    n_jobs=-1\n)\n\ngrid_search.fit(X_train, y_train)\n\nprint(f\"Best parameters: {grid_search.best_params_}\")\nprint(f\"Best cross-validation score: {grid_search.best_score_:.4f}\")\n\n# 使用最佳参数的模型\nbest_rf = grid_search.best_estimator_\nbest_rf_result = evaluate_model(best_rf, X_train, X_test, y_train, y_test)\n\nprint(\"\\nOptimized Random Forest:\")\nprint(f\"Accuracy: {best_rf_result['accuracy']:.4f}\")\nprint(\"Classification Report:\")\nprint(best_rf_result['report'])\n\n# 保存训练好的模型\nprint(\"\\nModel training completed!\")\nprint(f\"Best model: Optimized Random Forest (Accuracy: {best_rf_result['accuracy']:.4f})\")"
        },
        {
          "id": 5,
          "title": "模型评估",
          "description": "评估模型性能，解释模型决策",
          "example_code": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# 使用通用字体设置，避免中文字体问题\nimport matplotlib as mpl\nmpl.rcParams['font.family'] = 'DejaVu Sans'\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\nfrom sklearn.metrics import confusion_matrix, roc_curve, precision_recall_curve, average_precision_score\nfrom sklearn.ensemble import RandomForestClassifier\nimport shap\nfrom app.services.bank_analysis import bank_service\n\n# 加载数据\ndf = bank_service.load_credit_data()\n\n# 数据预处理\nprint(\"Preprocessing data...\")\nX = df.drop('DEFAULT', axis=1)\ny = df['DEFAULT']\n\n# 标准化数据\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# 划分数据集\nX_train, X_test, y_train, y_test = train_test_split(\n    X_scaled, y, test_size=0.25, random_state=42, stratify=y\n)\n\n# 训练模型\nprint(\"Training model for evaluation...\")\nrf = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\nrf.fit(X_train, y_train)\n\n# 预测\ny_pred = rf.predict(X_test)\ny_proba = rf.predict_proba(X_test)[:, 1]\n\n# 计算各种评估指标\nprint(\"\\nCalculating evaluation metrics...\")\naccuracy = accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred)\nrecall = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\nroc_auc = roc_auc_score(y_test, y_proba)\navg_precision = average_precision_score(y_test, y_proba)\n\n# 打印评估指标\nprint(\"Model Performance Metrics:\")\nprint(f\"Accuracy: {accuracy:.4f}\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall: {recall:.4f}\")\nprint(f\"F1 Score: {f1:.4f}\")\nprint(f\"ROC AUC: {roc_auc:.4f}\")\nprint(f\"Average Precision: {avg_precision:.4f}\")\n\n# 混淆矩阵\ncm = confusion_matrix(y_test, y_pred)\n\n# 计算其他混淆矩阵指标\ntn, fp, fn, tp = cm.ravel()\ntotal = tn + fp + fn + tp\nprint(\"\\nConfusion Matrix Analysis:\")\nprint(f\"True Negatives: {tn} ({tn/total:.2%})\")\nprint(f\"False Positives: {fp} ({fp/total:.2%})\")\nprint(f\"False Negatives: {fn} ({fn/total:.2%})\")\nprint(f\"True Positives: {tp} ({tp/total:.2%})\")\n\n# 计算特定业务指标\nprint(\"\\nBusiness Metrics:\")\ncost_fp = 1  # 假阳性的成本（错误地识别为违约的成本）\ncost_fn = 5  # 假阴性的成本（错误地识别为非违约的成本，通常更高）\ntotal_cost = (fp * cost_fp) + (fn * cost_fn)\nprint(f\"Total Error Cost: {total_cost} (FP: {fp*cost_fp}, FN: {fn*cost_fn})\")\n\n# 可视化混淆矩阵\nplt.figure(figsize=(10, 8))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\nplt.title('Confusion Matrix')\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.show()\n\n# 交叉验证评估\nprint(\"\\nPerforming cross-validation...\")\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\ncv_scores_accuracy = cross_val_score(rf, X_scaled, y, cv=cv, scoring='accuracy')\ncv_scores_auc = cross_val_score(rf, X_scaled, y, cv=cv, scoring='roc_auc')\n\nprint(f\"CV Accuracy: {cv_scores_accuracy.mean():.4f} (+/- {cv_scores_accuracy.std()*2:.4f})\")\nprint(f\"CV ROC AUC: {cv_scores_auc.mean():.4f} (+/- {cv_scores_auc.std()*2:.4f})\")\n\n# ROC 曲线\nfpr, tpr, thresholds = roc_curve(y_test, y_proba)\n\nplt.figure(figsize=(10, 6))\nplt.plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc:.3f})')\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic (ROC) Curve')\nplt.legend(loc='lower right')\nplt.grid(alpha=0.3)\nplt.show()\n\n# Precision-Recall 曲线\nprecision_curve, recall_curve, _ = precision_recall_curve(y_test, y_proba)\n\nplt.figure(figsize=(10, 6))\nplt.plot(recall_curve, precision_curve, label=f'AP = {avg_precision:.3f}')\nplt.axhline(y=y_test.mean(), color='r', linestyle='--', label=f'Baseline: {y_test.mean():.3f}')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Precision-Recall Curve')\nplt.legend()\nplt.grid(alpha=0.3)\nplt.show()\n\n# 阈值分析\nthresholds_df = pd.DataFrame({\n    'Threshold': thresholds,\n    'TPR': tpr[:-1],\n    'FPR': fpr[:-1]\n})\n\n# 计算每个阈值的精确度和召回率\nthresholds_plot = np.arange(0.1, 1.0, 0.1)\nmetrics = []\n\nfor threshold in thresholds_plot:\n    y_pred_threshold = (y_proba >= threshold).astype(int)\n    \n    precision_t = precision_score(y_test, y_pred_threshold)\n    recall_t = recall_score(y_test, y_pred_threshold)\n    f1_t = f1_score(y_test, y_pred_threshold)\n    \n    metrics.append({\n        'Threshold': threshold,\n        'Precision': precision_t,\n        'Recall': recall_t,\n        'F1': f1_t\n    })\n\nmetrics_df = pd.DataFrame(metrics)\n\n# 绘制不同阈值下的指标\nplt.figure(figsize=(12, 6))\n\nplt.plot(metrics_df['Threshold'], metrics_df['Precision'], marker='o', label='Precision')\nplt.plot(metrics_df['Threshold'], metrics_df['Recall'], marker='s', label='Recall')\nplt.plot(metrics_df['Threshold'], metrics_df['F1'], marker='^', label='F1')\n\nplt.xlabel('Classification Threshold')\nplt.ylabel('Score')\nplt.title('Performance Metrics vs. Classification Threshold')\nplt.legend()\nplt.grid(alpha=0.3)\nplt.show()\n\n# Feature Importance and SHAP Analysis\nprint(\"\\nAnalyzing feature importance...\")\n\n# 获取特征重要性\nfeature_importances = rf.feature_importances_\n\n# 创建DataFrame并排序\nfeature_importance_df = pd.DataFrame({\n    'Feature': X.columns,\n    'Importance': feature_importances\n})\nfeature_importance_df = feature_importance_df.sort_values('Importance', ascending=False)\n\n# 绘制特征重要性\nplt.figure(figsize=(12, 6))\nsns.barplot(x='Importance', y='Feature', data=feature_importance_df.head(15))\nplt.title('Random Forest Feature Importance')\nplt.show()\n\n# SHAP值分析\nprint(\"\\nCalculating SHAP values (this may take a moment)...\")\n# 使用原始特征数据而不是标准化数据来计算SHAP值以提高可解释性\nX_sample = X.iloc[:100]  # 使用一个样本子集进行SHAP分析，以减少计算时间\nmodel_for_shap = RandomForestClassifier(n_estimators=100, random_state=42)\nmodel_for_shap.fit(X, y)  # 在完整数据上训练\n\n# 创建SHAP解释器\nexplainer = shap.TreeExplainer(model_for_shap)\nshap_values = explainer.shap_values(X_sample)\n\n# 绘制SHAP摘要图\nshap.summary_plot(shap_values[1], X_sample, feature_names=X.columns.tolist())\n\n# 绘制前3个特征的SHAP依赖图\ntop_features = feature_importance_df['Feature'].head(3).tolist()\nfor feature in top_features:\n    plt.figure()\n    shap.dependence_plot(feature, shap_values[1], X_sample, feature_names=X.columns.tolist())\n\n# 模型校准曲线\nfrom sklearn.calibration import calibration_curve\nplt.figure(figsize=(10, 6))\n\nprobability_true, probability_pred = calibration_curve(y_test, y_proba, n_bins=10)\nplt.plot(probability_pred, probability_true, marker='o', label='Random Forest')\nplt.plot([0, 1], [0, 1], 'k--', label='Perfectly Calibrated')\n\nplt.xlabel('Mean Predicted Probability')\nplt.ylabel('Fraction of Positives')\nplt.title('Calibration Curve')\nplt.legend()\nplt.grid(alpha=0.3)\nplt.show()\n\n# 输出总结和建议\nprint(\"\\nModel Evaluation Summary:\")\nprint(f\"Overall Performance: {accuracy:.2%} accuracy, {roc_auc:.2%} ROC AUC\")\n\n# 根据性能指标给出建议\nif roc_auc > 0.8:\n    print(\"The model shows good discriminative ability.\")\nelif roc_auc > 0.7:\n    print(\"The model shows acceptable discriminative ability, but could be improved.\")\nelse:\n    print(\"The model's discriminative ability is limited. Consider feature engineering or trying different algorithms.\")\n\nif precision < 0.7:\n    print(\"Consider increasing the classification threshold to improve precision, if required for business purposes.\")\n\nif recall < 0.7:\n    print(\"Consider decreasing the classification threshold to improve recall, if required for business purposes.\")"
        },
        {
          "id": 6,
          "title": "风险控制策略",
          "description": "设计和实施信用卡风险控制策略",
          "example_code": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# 使用通用字体设置，避免中文字体问题\nimport matplotlib as mpl\nmpl.rcParams['font.family'] = 'DejaVu Sans'\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom app.services.bank_analysis import bank_service\n\n# 加载数据\ndf = bank_service.load_credit_data()\n\n# 数据准备\nprint(\"Preparing data...\")\nX = df.drop('DEFAULT', axis=1)\ny = df['DEFAULT']\n\n# 标准化数据\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# 划分训练集和测试集\nX_train, X_test, y_train, y_test = train_test_split(\n    X_scaled, y, test_size=0.25, random_state=42, stratify=y\n)\n\n# 训练模型（使用随机森林为例）\nprint(\"Training model...\")\nmodel = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\nmodel.fit(X_train, y_train)\n\n# 获取预测概率\ny_proba = model.predict_proba(X_test)[:, 1]\n\n# 创建测试数据集的副本，并添加预测概率\ntest_df = pd.DataFrame(X_test, columns=X.columns)\ntest_df['ACTUAL_DEFAULT'] = y_test.values\ntest_df['DEFAULT_PROBABILITY'] = y_proba\n\n# 1. 风险分层策略\nprint(\"\\nImplementing risk stratification strategy...\")\n\n# 基于预测概率对客户进行风险分层\nrisk_bins = [0, 0.2, 0.4, 0.6, 0.8, 1.0]\nrisk_labels = ['Very Low', 'Low', 'Medium', 'High', 'Very High']\n\ntest_df['RISK_LEVEL'] = pd.cut(\n    test_df['DEFAULT_PROBABILITY'], \n    bins=risk_bins, \n    labels=risk_labels\n)\n\n# 计算每个风险等级的实际违约率\nrisk_level_stats = test_df.groupby('RISK_LEVEL')['ACTUAL_DEFAULT'].agg(\n    ['count', 'mean']\n).rename(columns={'count': 'Number_of_Customers', 'mean': 'Actual_Default_Rate'})\n\n# 添加预测的违约概率平均值\nrisk_level_stats['Avg_Predicted_Probability'] = test_df.groupby('RISK_LEVEL')['DEFAULT_PROBABILITY'].mean()\n\nprint(\"Risk Stratification Results:\")\nprint(risk_level_stats)\n\n# 可视化风险分层\nplt.figure(figsize=(12, 6))\n\n# 客户数量分布\nplt.subplot(1, 2, 1)\nsns.countplot(x='RISK_LEVEL', data=test_df, palette='YlOrRd')\nplt.title('Number of Customers by Risk Level')\nplt.xlabel('Risk Level')\nplt.ylabel('Count')\n\n# 各风险等级的实际违约率\nplt.subplot(1, 2, 2)\nsns.barplot(\n    x=risk_level_stats.index, \n    y=risk_level_stats['Actual_Default_Rate'],\n    palette='YlOrRd'\n)\nplt.title('Actual Default Rate by Risk Level')\nplt.xlabel('Risk Level')\nplt.ylabel('Default Rate')\nplt.ylim(0, 1)\n\nplt.tight_layout()\nplt.show()\n\n# 2. 设计信用审批策略\nprint(\"\\nDesigning credit approval strategy...\")\n\n# 设置不同风险等级的信用额度调整系数\ncredit_limit_adjustments = {\n    'Very Low': 1.2,   # 增加20%\n    'Low': 1.1,        # 增加10%\n    'Medium': 1.0,     # 保持不变\n    'High': 0.8,       # 减少20%\n    'Very High': 0.6   # 减少40%\n}\n\n# 假设我们有一个基准信用额度为10000\nbase_credit_limit = 10000\n\n# 计算调整后的信用额度\ntest_df['ADJUSTED_CREDIT_LIMIT'] = test_df['RISK_LEVEL'].map(credit_limit_adjustments) * base_credit_limit\n\n# 3. 拒绝策略\n# 设置拒绝阈值 - 高于此阈值的申请将被拒绝\nrejection_threshold = 0.7\n\ntest_df['APPLICATION_STATUS'] = np.where(\n    test_df['DEFAULT_PROBABILITY'] >= rejection_threshold,\n    'Rejected',\n    'Approved'\n)\n\n# 计算拒绝率和批准率\nrejection_rate = (test_df['APPLICATION_STATUS'] == 'Rejected').mean()\napproval_rate = (test_df['APPLICATION_STATUS'] == 'Approved').mean()\n\nprint(f\"Rejection Rate: {rejection_rate:.2%}\")\nprint(f\"Approval Rate: {approval_rate:.2%}\")\n\n# 计算拒绝和批准客户中的实际违约率\nrejected_default_rate = test_df[test_df['APPLICATION_STATUS'] == 'Rejected']['ACTUAL_DEFAULT'].mean()\napproved_default_rate = test_df[test_df['APPLICATION_STATUS'] == 'Approved']['ACTUAL_DEFAULT'].mean()\n\nprint(f\"Default Rate among Rejected Customers: {rejected_default_rate:.2%}\")\nprint(f\"Default Rate among Approved Customers: {approved_default_rate:.2%}\")\n\n# 可视化拒绝策略\nplt.figure(figsize=(12, 5))\n\n# 批准 vs. 拒绝的客户数量\nplt.subplot(1, 2, 1)\nsns.countplot(x='APPLICATION_STATUS', data=test_df)\nplt.title('Application Approval Status')\nplt.xlabel('Status')\nplt.ylabel('Count')\n\n# 批准 vs. 拒绝的违约率\nstatus_default_rates = test_df.groupby('APPLICATION_STATUS')['ACTUAL_DEFAULT'].mean()\nplt.subplot(1, 2, 2)\nsns.barplot(x=status_default_rates.index, y=status_default_rates.values)\nplt.title('Default Rate by Application Status')\nplt.xlabel('Status')\nplt.ylabel('Default Rate')\nplt.ylim(0, 1)\n\nplt.tight_layout()\nplt.show()\n\n# 4. 利润模拟\nprint(\"\\nSimulating profit impact...\")\n\n# 设置业务参数\nannual_interest_rate = 0.18  # 年利率18%\naverage_transaction_amount = 5000  # 平均交易金额\nfee_percentage = 0.03  # 交易费用3%\ncost_of_funds = 0.05  # 资金成本5%\ncost_per_default = 10000  # 每个违约客户的平均损失\n\n# 计算每个客户的预期收益\ntest_df['EXPECTED_PROFIT'] = np.where(\n    test_df['ACTUAL_DEFAULT'] == 1,\n    -cost_per_default,  # 违约客户造成损失\n    (annual_interest_rate - cost_of_funds) * average_transaction_amount + \n        fee_percentage * average_transaction_amount  # 非违约客户产生收益\n)\n\n# 计算不同策略下的总收益\n# 1. 接受所有客户\ntotal_profit_accept_all = test_df['EXPECTED_PROFIT'].sum()\n\n# 2. 使用我们的拒绝策略\ntotal_profit_with_strategy = test_df[test_df['APPLICATION_STATUS'] == 'Approved']['EXPECTED_PROFIT'].sum()\n\n# 3. 拒绝所有客户\ntotal_profit_reject_all = 0  # 假设拒绝所有客户没有收益也没有损失\n\nprint(f\"Profit if Accept All: ${total_profit_accept_all:.2f}\")\nprint(f\"Profit with Rejection Strategy: ${total_profit_with_strategy:.2f}\")\nprint(f\"Profit if Reject All: ${total_profit_reject_all:.2f}\")\n\nprofit_improvement = ((total_profit_with_strategy - total_profit_accept_all) / abs(total_profit_accept_all)) * 100\nprint(f\"Profit Improvement with Strategy: {profit_improvement:.2f}%\")\n\n# 5. 阈值优化\nprint(\"\\nOptimizing rejection threshold...\")\n\n# 测试不同的拒绝阈值\nthresholds = np.arange(0.1, 1.0, 0.1)\nthreshold_results = []\n\nfor threshold in thresholds:\n    # 应用拒绝策略\n    application_status = np.where(\n        test_df['DEFAULT_PROBABILITY'] >= threshold,\n        'Rejected',\n        'Approved'\n    )\n    \n    # 计算批准客户的总收益\n    approved_indices = [i for i, status in enumerate(application_status) if status == 'Approved']\n    profit = test_df.iloc[approved_indices]['EXPECTED_PROFIT'].sum()\n    \n    # 计算拒绝率\n    rejection_rate = (application_status == 'Rejected').mean()\n    \n    # 计算批准客户中的违约率\n    approved_default_rate = test_df.iloc[approved_indices]['ACTUAL_DEFAULT'].mean() if approved_indices else 0\n    \n    threshold_results.append({\n        'Threshold': threshold,\n        'Profit': profit,\n        'Rejection_Rate': rejection_rate,\n        'Approved_Default_Rate': approved_default_rate\n    })\n\n# 转换为DataFrame\nthreshold_df = pd.DataFrame(threshold_results)\n\n# 找出利润最高的阈值\noptimal_threshold = threshold_df.loc[threshold_df['Profit'].idxmax(), 'Threshold']\nmax_profit = threshold_df['Profit'].max()\n\nprint(f\"Optimal Rejection Threshold: {optimal_threshold}\")\nprint(f\"Maximum Profit: ${max_profit:.2f}\")\n\n# 可视化不同阈值下的结果\nplt.figure(figsize=(15, 6))\n\n# 利润 vs. 阈值\nplt.subplot(1, 3, 1)\nplt.plot(threshold_df['Threshold'], threshold_df['Profit'], marker='o')\nplt.axvline(x=optimal_threshold, color='r', linestyle='--', label=f'Optimal={optimal_threshold}')\nplt.title('Profit vs. Threshold')\nplt.xlabel('Rejection Threshold')\nplt.ylabel('Profit ($)')\nplt.legend()\n\n# 拒绝率 vs. 阈值\nplt.subplot(1, 3, 2)\nplt.plot(threshold_df['Threshold'], threshold_df['Rejection_Rate'], marker='o')\nplt.axvline(x=optimal_threshold, color='r', linestyle='--')\nplt.title('Rejection Rate vs. Threshold')\nplt.xlabel('Rejection Threshold')\nplt.ylabel('Rejection Rate')\n\n# 批准客户中的违约率 vs. 阈值\nplt.subplot(1, 3, 3)\nplt.plot(threshold_df['Threshold'], threshold_df['Approved_Default_Rate'], marker='o')\nplt.axvline(x=optimal_threshold, color='r', linestyle='--')\nplt.title('Approved Default Rate vs. Threshold')\nplt.xlabel('Rejection Threshold')\nplt.ylabel('Default Rate among Approved')\n\nplt.tight_layout()\nplt.show()\n\n# 6. 最终风险控制策略建议\nprint(\"\\nFinal Risk Control Strategy Recommendations:\")\n\nprint(f\"1. Implement a risk-based approval system with an optimal rejection threshold of {optimal_threshold}\")\nprint(\"2. Apply risk-based credit limit adjustments:\")\nfor risk_level, adjustment in credit_limit_adjustments.items():\n    adjustment_percentage = (adjustment - 1) * 100 if adjustment > 1 else (adjustment - 1) * 100\n    print(f\"   - {risk_level} risk: {adjustment_percentage:+.0f}% credit limit adjustment\")\n\nprint(\"3. Implement additional monitoring for high-risk approved customers\")\nprint(\"4. Consider implementing risk-based pricing for different risk segments\")\n\n# 7. 策略实施后的预期业务影响\nprint(\"\\nExpected Business Impact:\")\nprint(f\"- Expected Profit Improvement: {profit_improvement:.2f}%\")\nprint(f\"- Expected Approval Rate: {1 - threshold_df.loc[threshold_df['Threshold'] == optimal_threshold, 'Rejection_Rate'].values[0]:.2%}\")\nprint(f\"- Expected Default Rate among Approved: {threshold_df.loc[threshold_df['Threshold'] == optimal_threshold, 'Approved_Default_Rate'].values[0]:.2%}\")"
        }
      ]
    },
    {
      "id": 2,
      "title": "银行客户流失预警模型",
      "description": "使用机器学习方法预测和分析银行客户流失风险因素",
      "category": "bank",
      "difficulty_level": 4,
      "estimated_duration": 120,
      "prerequisites": "统计分析基础和数据挖掘知识",
      "data_source": "银行客户交易与行为数据",
      "steps": [
        {
          "id": 1,
          "title": "客户行为分析",
          "description": "分析客户交易频率、金额和渠道偏好等行为特征",
          "example_code": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport seaborn as sns\n# 使用通用字体设置，避免中文字体问题\nfrom app.services.docker_matplotlib_fix import configure_matplotlib_fonts\n\n# 配置字体以支持中文显示\nconfigure_matplotlib_fonts()\nfrom app.services.bank_analysis import bank_service\n\n# 加载客户数据\ndf = bank_service.load_customer_churn_data()\n\n# 1. 数据概览\nprint(\"=== Data Overview ===\")\nprint(f\"Dataset shape: {df.shape}\")\nprint(\"\\nFirst few rows:\")\nprint(df.head())\n\n# 2. 数据基本信息\nprint(\"\\n=== Basic Data Information ===\")\nprint(df.info())\n\n# 3. 基本统计描述\nprint(\"\\n=== Descriptive Statistics ===\")\nprint(df.describe())\n\n# 4. 缺失值情况\nprint(\"\\n=== Missing Values Analysis ===\")\nmissing_values = df.isnull().sum()\nif missing_values.sum() > 0:\n    print(missing_values[missing_values > 0])\n    missing_percentage = (missing_values / len(df)) * 100\n    print(\"\\nMissing percentage:\")\n    print(missing_percentage[missing_percentage > 0])\nelse:\n    print(\"No missing values found.\")\n\n# 5. 客户流失情况分析\nif 'Churn' in df.columns:\n    print(\"\\n=== Churn Distribution ===\")\n    churn_distribution = df['Churn'].value_counts(normalize=True) * 100\n    print(f\"Non-churned customers: {churn_distribution[0]:.2f}%\")\n    print(f\"Churned customers: {churn_distribution[1]:.2f}%\")\n    \n    plt.figure(figsize=(10, 5))\n    plt.subplot(1, 2, 1)\n    sns.countplot(x='Churn', data=df, palette=['lightblue', 'salmon'])\n    plt.title('Customer Churn Distribution')\n    plt.xlabel('Churn (0=No, 1=Yes)')\n    plt.ylabel('Count')\n    \n    plt.subplot(1, 2, 2)\n    plt.pie(churn_distribution, labels=['Retained', 'Churned'], autopct='%1.1f%%', colors=['lightblue', 'salmon'], startangle=90)\n    plt.title('Churn Percentage')\n    plt.axis('equal')\n    plt.tight_layout()\n    plt.show()\n\n# 6. 交易频率分析\nif 'TransactionFrequency' in df.columns:\n    print(\"\\n=== Transaction Frequency Analysis ===\")\n    print(df.groupby('Churn')['TransactionFrequency'].describe())\n    \n    plt.figure(figsize=(10, 6))\n    sns.histplot(data=df, x='TransactionFrequency', hue='Churn', bins=20, kde=True, palette=['lightblue', 'salmon'])\n    plt.title('Transaction Frequency Distribution by Churn Status')\n    plt.xlabel('Transaction Frequency (per month)')\n    plt.ylabel('Count')\n    plt.show()\n    \n    plt.figure(figsize=(8, 6))\n    sns.boxplot(x='Churn', y='TransactionFrequency', data=df, palette=['lightblue', 'salmon'])\n    plt.title('Transaction Frequency by Churn Status')\n    plt.xlabel('Churn (0=No, 1=Yes)')\n    plt.ylabel('Transaction Frequency')\n    plt.show()\n\n# 7. 交易金额分析\nif 'TransactionAmount' in df.columns:\n    print(\"\\n=== Transaction Amount Analysis ===\")\n    print(df.groupby('Churn')['TransactionAmount'].describe())\n    \n    plt.figure(figsize=(10, 6))\n    sns.histplot(data=df, x='TransactionAmount', hue='Churn', bins=20, kde=True, palette=['lightblue', 'salmon'])\n    plt.title('Transaction Amount Distribution by Churn Status')\n    plt.xlabel('Average Transaction Amount')\n    plt.ylabel('Count')\n    plt.show()\n\n# 8. 渠道偏好分析\nif 'PreferredChannel' in df.columns:\n    print(\"\\n=== Channel Preference Analysis ===\")\n    channel_counts = df.groupby(['PreferredChannel', 'Churn']).size().unstack()\n    print(channel_counts)\n    \n    # 计算每个渠道的流失率\n    channel_churn_rate = channel_counts[1] / (channel_counts[0] + channel_counts[1]) * 100\n    print(\"\\nChurn rate by channel:\")\n    print(channel_churn_rate)\n    \n    plt.figure(figsize=(12, 5))\n    plt.subplot(1, 2, 1)\n    channel_counts.plot(kind='bar', stacked=True, color=['lightblue', 'salmon'])\n    plt.title('Customer Count by Channel and Churn Status')\n    plt.xlabel('Preferred Channel')\n    plt.ylabel('Number of Customers')\n    plt.legend(['Retained', 'Churned'])\n    \n    plt.subplot(1, 2, 2)\n    channel_churn_rate.plot(kind='bar', color='salmon')\n    plt.title('Churn Rate by Channel')\n    plt.xlabel('Preferred Channel')\n    plt.ylabel('Churn Rate (%)')\n    plt.tight_layout()\n    plt.show()\n\n# 9. 客户活跃度分析\nif 'DaysSinceLastTransaction' in df.columns:\n    print(\"\\n=== Customer Activity Analysis ===\")\n    print(df.groupby('Churn')['DaysSinceLastTransaction'].describe())\n    \n    plt.figure(figsize=(10, 6))\n    sns.boxplot(x='Churn', y='DaysSinceLastTransaction', data=df, palette=['lightblue', 'salmon'])\n    plt.title('Days Since Last Transaction by Churn Status')\n    plt.xlabel('Churn (0=No, 1=Yes)')\n    plt.ylabel('Days Since Last Transaction')\n    plt.show()\n\n# 10. 客户价值分析\nif 'CustomerValue' in df.columns and 'Tenure' in df.columns:\n    print(\"\\n=== Customer Value Analysis ===\")\n    # 计算每个客户的生命周期价值\n    df['LifetimeValue'] = df['CustomerValue'] * df['Tenure']\n    print(df.groupby('Churn')['LifetimeValue'].describe())\n    \n    plt.figure(figsize=(10, 6))\n    sns.scatterplot(data=df, x='Tenure', y='CustomerValue', hue='Churn', palette=['lightblue', 'salmon'], alpha=0.7)\n    plt.title('Customer Value vs Tenure by Churn Status')\n    plt.xlabel('Tenure (months)')\n    plt.ylabel('Monthly Customer Value')\n    plt.show()\n\n# 11. 数值特征间的相关性分析\nnumeric_df = df.select_dtypes(include=[np.number])\nif len(numeric_df.columns) > 1:\n    print(\"\\n=== Correlation Analysis ===\")\n    correlation_matrix = numeric_df.corr()\n    print(correlation_matrix['Churn'].sort_values(ascending=False) if 'Churn' in numeric_df.columns else correlation_matrix)\n    \n    plt.figure(figsize=(12, 10))\n    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n    plt.title('Correlation Matrix of Numerical Features')\n    plt.tight_layout()\n    plt.show()\n\n# 12. 关键行为特征的分布比较\nkey_features = [col for col in df.columns if col not in ['CustomerID', 'Name', 'Churn']]\nif len(key_features) > 0:\n    print(\"\\n=== Key Feature Distributions ===\")\n    \n    # 显示主要特征的分布\n    num_cols = min(6, len(key_features))\n    num_rows = (len(key_features) + num_cols - 1) // num_cols\n    plt.figure(figsize=(15, num_rows * 4))\n    \n    for i, feature in enumerate(key_features[:num_cols * num_rows]):\n        if feature in df.columns and df[feature].dtype in [np.int64, np.float64]:\n            plt.subplot(num_rows, num_cols, i+1)\n            sns.histplot(data=df, x=feature, hue='Churn', kde=True, palette=['lightblue', 'salmon'], bins=20)\n            plt.title(f'Distribution of {feature}')\n    \n    plt.tight_layout()\n    plt.show()\n\n# 13. 客户分群和行为模式识别\nif 'TransactionFrequency' in df.columns and 'TransactionAmount' in df.columns:\n    print(\"\\n=== Customer Segmentation Based on Transaction Behavior ===\")\n    \n    plt.figure(figsize=(10, 8))\n    sns.scatterplot(data=df, x='TransactionFrequency', y='TransactionAmount', hue='Churn', size='Tenure', sizes=(50, 200), palette=['lightblue', 'salmon'], alpha=0.7)\n    plt.title('Customer Segmentation by Transaction Behavior')\n    plt.xlabel('Transaction Frequency')\n    plt.ylabel('Average Transaction Amount')\n    plt.grid(True, linestyle='--', alpha=0.7)\n    plt.show()\n\n# 14. 总结关键发现\nprint(\"\\n=== Key Findings Summary ===\")\nif 'Churn' in df.columns:\n    # 计算流失率\n    churn_rate = df['Churn'].mean() * 100\n    print(f\"Overall Churn Rate: {churn_rate:.2f}%\")\n    \n    # 找出与流失最相关的特征\n    if 'Churn' in numeric_df.columns:\n        churn_correlations = correlation_matrix['Churn'].drop('Churn').abs().sort_values(ascending=False)\n        print(\"\\nTop features correlated with churn:\")\n        print(churn_correlations.head(5))\n    \n    # 分析流失客户和留存客户之间的主要区别\n    print(\"\\nKey differences between churned and non-churned customers:\")\n    for feature in key_features[:5]:\n        if feature in df.columns and df[feature].dtype in [np.int64, np.float64]:\n            churned_mean = df[df['Churn'] == 1][feature].mean()\n            non_churned_mean = df[df['Churn'] == 0][feature].mean()\n            diff_pct = ((churned_mean - non_churned_mean) / non_churned_mean) * 100\n            print(f\"- {feature}: {diff_pct:.1f}% difference between churned and non-churned customers\")"
        },
        {
          "id": 2,
          "title": "流失指标定义",
          "description": "确定客户流失的定义和关键指标",
          "example_code": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport matplotlib as mpl\nfrom datetime import datetime, timedelta\nfrom app.services.docker_matplotlib_fix import configure_matplotlib_fonts\n\n# 配置字体以支持中文显示\nconfigure_matplotlib_fonts()\nfrom app.services.bank_analysis import bank_service\n\n# 加载客户数据\ndf = bank_service.load_customer_churn_data()\nprint(f\"加载数据集，共{df.shape[0]}行，{df.shape[1]}列\")\n\n# 1. 探索现有的流失标签\nif 'Churn' in df.columns:\n    print(\"\\n=== 现有的流失标签分析 ===\")\n    churn_counts = df['Churn'].value_counts()\n    print(\"流失标签分布:\")\n    print(f\"未流失客户: {churn_counts[0]} ({churn_counts[0]/len(df)*100:.2f}%)\")\n    print(f\"已流失客户: {churn_counts[1]} ({churn_counts[1]/len(df)*100:.2f}%)\")\n    \n    # 可视化现有的流失分布\n    plt.figure(figsize=(10, 5))\n    plt.subplot(1, 2, 1)\n    sns.countplot(x='Churn', data=df, palette=['lightblue', 'salmon'])\n    plt.title('客户流失分布')\n    plt.xlabel('流失状态 (0=未流失, 1=已流失)')\n    plt.ylabel('客户数量')\n    \n    plt.subplot(1, 2, 2)\n    plt.pie(churn_counts, labels=['未流失', '已流失'], autopct='%1.1f%%', colors=['lightblue', 'salmon'], startangle=90)\n    plt.title('流失比例')\n    plt.axis('equal')\n    plt.tight_layout()\n    plt.show()\n\n# 2. 定义流失的方法\nprint(\"\\n=== 流失定义方法 ===\")\nprint(\"1. 账户关闭: 客户主动关闭账户\")\nprint(\"2. 非活跃流失: 客户长时间没有活动\")\nprint(\"3. 价值流失: 客户交易额显著下降\")\nprint(\"4. 参与度流失: 客户交易频率显著下降\")\nprint(\"5. 渠道转换: 客户完全停止使用某一渠道\")\n\n# 3. 基于交易不活跃定义流失\nprint(\"\\n=== 基于交易不活跃定义流失 ===\")\ninactivity_threshold = 90  # 90天不活跃定义为流失\n\nif 'LastTransactionDate' in df.columns:\n    # 转换日期字段为日期类型\n    df['LastTransactionDate'] = pd.to_datetime(df['LastTransactionDate'])\n    \n    # 计算当前日期（使用数据集中最近的交易日期再加7天作为\"当前日期\"）\n    current_date = df['LastTransactionDate'].max() + timedelta(days=7)\n    print(f\"分析的'当前日期': {current_date.strftime('%Y-%m-%d')}\")\n    \n    # 计算自上次交易以来的天数\n    df['DaysSinceLastTransaction'] = (current_date - df['LastTransactionDate']).dt.days\n    \n    # 基于不活跃天数定义流失\n    df['InactivityChurn'] = (df['DaysSinceLastTransaction'] > inactivity_threshold).astype(int)\n    \n    inactive_churn_rate = df['InactivityChurn'].mean() * 100\n    print(f\"基于{inactivity_threshold}天不活跃定义的流失率: {inactive_churn_rate:.2f}%\")\n    \n    # 与现有流失标签对比\n    if 'Churn' in df.columns:\n        churn_match = (df['Churn'] == df['InactivityChurn']).mean() * 100\n        print(f\"不活跃定义的流失与现有流失标签匹配度: {churn_match:.2f}%\")\n        \n        # 计算现有流失标签的不同类型\n        days_inactive_churned = df[df['Churn'] == 1]['DaysSinceLastTransaction'].mean()\n        days_inactive_retained = df[df['Churn'] == 0]['DaysSinceLastTransaction'].mean()\n        print(f\"已流失客户的平均不活跃天数: {days_inactive_churned:.1f}天\")\n        print(f\"未流失客户的平均不活跃天数: {days_inactive_retained:.1f}天\")\n\n# 4. 基于交易金额下降定义流失\nprint(\"\\n=== 基于交易金额下降定义流失 ===\")\nvalue_decline_threshold = -0.5  # 交易金额下降50%定义为价值流失\n\nif 'TransactionAmountDecline' in df.columns:\n    # 基于交易金额下降定义流失\n    df['ValueChurn'] = (df['TransactionAmountDecline'] <= value_decline_threshold).astype(int)\n    \n    value_churn_rate = df['ValueChurn'].mean() * 100\n    print(f\"基于交易金额下降{abs(value_decline_threshold)*100}%定义的流失率: {value_churn_rate:.2f}%\")\n    \n    # 与现有流失标签对比\n    if 'Churn' in df.columns:\n        value_churn_match = (df['Churn'] == df['ValueChurn']).mean() * 100\n        print(f\"价值流失与现有流失标签匹配度: {value_churn_match:.2f}%\")\n\n# 5. 基于交易频率下降定义流失\nprint(\"\\n=== 基于交易频率下降定义流失 ===\")\nfrequency_decline_threshold = -0.6  # 交易频率下降60%定义为参与度流失\n\nif 'TransactionFrequencyDecline' in df.columns:\n    # 基于交易频率下降定义流失\n    df['FrequencyChurn'] = (df['TransactionFrequencyDecline'] <= frequency_decline_threshold).astype(int)\n    \n    frequency_churn_rate = df['FrequencyChurn'].mean() * 100\n    print(f\"基于交易频率下降{abs(frequency_decline_threshold)*100}%定义的流失率: {frequency_churn_rate:.2f}%\")\n    \n    # 与现有流失标签对比\n    if 'Churn' in df.columns:\n        freq_churn_match = (df['Churn'] == df['FrequencyChurn']).mean() * 100\n        print(f\"参与度流失与现有流失标签匹配度: {freq_churn_match:.2f}%\")\n\n# 6. 综合指标流失定义\nprint(\"\\n=== 综合指标流失定义 ===\")\n# 创建综合流失指标\nchurn_indicators = []\n\nif 'InactivityChurn' in df.columns:\n    churn_indicators.append('InactivityChurn')\nif 'ValueChurn' in df.columns:\n    churn_indicators.append('ValueChurn')\nif 'FrequencyChurn' in df.columns:\n    churn_indicators.append('FrequencyChurn')\n\nif churn_indicators:\n    # 如果任一指标显示流失，则认为客户已流失\n    df['CompositeChurn'] = df[churn_indicators].max(axis=1)\n    \n    composite_churn_rate = df['CompositeChurn'].mean() * 100\n    print(f\"基于综合指标定义的流失率: {composite_churn_rate:.2f}%\")\n    \n    # 各种流失类型的重叠情况\n    if len(churn_indicators) > 1:\n        print(\"\\n不同流失类型的占比:\")\n        for indicator in churn_indicators:\n            indicator_rate = df[indicator].mean() * 100\n            print(f\"- {indicator}: {indicator_rate:.2f}%\")\n        \n        # 计算纯流失类型的客户\n        for i, indicator1 in enumerate(churn_indicators):\n            only_this_type = df[indicator1] == 1\n            for j, indicator2 in enumerate(churn_indicators):\n                if i != j:\n                    only_this_type &= df[indicator2] == 0\n            \n            only_this_type_pct = only_this_type.mean() * 100\n            print(f\"- 仅{indicator1}类型流失: {only_this_type_pct:.2f}%\")\n    \n    # 与现有流失标签对比\n    if 'Churn' in df.columns:\n        composite_match = (df['Churn'] == df['CompositeChurn']).mean() * 100\n        print(f\"\\n综合流失定义与现有流失标签匹配度: {composite_match:.2f}%\")\n\n# 7. 流失预警信号定义\nprint(\"\\n=== 流失预警信号定义 ===\")\n# 定义预警等级\ndf['ChurnRiskScore'] = 0\n\n# 根据不同指标分配风险分数\nif 'DaysSinceLastTransaction' in df.columns:\n    df.loc[df['DaysSinceLastTransaction'] > 60, 'ChurnRiskScore'] += 1\n    df.loc[df['DaysSinceLastTransaction'] > 75, 'ChurnRiskScore'] += 1\n\nif 'TransactionAmountDecline' in df.columns:\n    df.loc[df['TransactionAmountDecline'] < -0.3, 'ChurnRiskScore'] += 1\n    df.loc[df['TransactionAmountDecline'] < -0.4, 'ChurnRiskScore'] += 1\n\nif 'TransactionFrequencyDecline' in df.columns:\n    df.loc[df['TransactionFrequencyDecline'] < -0.4, 'ChurnRiskScore'] += 1\n    df.loc[df['TransactionFrequencyDecline'] < -0.5, 'ChurnRiskScore'] += 1\n\n# 其他可能的风险信号\nif 'SupportCallsCount' in df.columns:\n    df.loc[df['SupportCallsCount'] > 3, 'ChurnRiskScore'] += 1\n\nif 'ComplaintsCount' in df.columns:\n    df.loc[df['ComplaintsCount'] > 0, 'ChurnRiskScore'] += 1\n\n# 定义风险等级\ndf['ChurnRiskLevel'] = pd.cut(df['ChurnRiskScore'], \n                          bins=[-1, 0, 2, 4, 100], \n                          labels=['无风险', '低风险', '中风险', '高风险'])\n\n# 显示风险等级分布\nrisk_distribution = df['ChurnRiskLevel'].value_counts(normalize=True) * 100\nprint(\"流失风险等级分布:\")\nfor level, percentage in risk_distribution.items():\n    print(f\"- {level}: {percentage:.2f}%\")\n\n# 可视化风险分数分布\nplt.figure(figsize=(15, 5))\nplt.subplot(1, 2, 1)\nsns.countplot(x='ChurnRiskScore', data=df, palette='YlOrRd')\nplt.title('流失风险分数分布')\nplt.xlabel('风险分数')\nplt.ylabel('客户数量')\n\nplt.subplot(1, 2, 2)\nsns.countplot(x='ChurnRiskLevel', data=df, palette='YlOrRd')\nplt.title('流失风险等级分布')\nplt.xlabel('风险等级')\nplt.ylabel('客户数量')\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n\n# 8. 流失指标的业务解释\nprint(\"\\n=== 流失指标的业务解释 ===\")\nprint(\"根据分析，我们建议使用以下流失定义:\")\nprint(\"1. 活跃度流失: 客户连续90天未进行任何交易\")\nprint(\"2. 价值流失: 客户近期交易金额较过往平均下降50%以上\")\nprint(\"3. 频率流失: 客户近期交易频率较过往平均下降60%以上\")\nprint(\"4. 综合流失指标: 满足上述任一条件即视为流失风险客户\")\n\nprint(\"\\n预警信号层级:\")\nprint(\"- 低风险: 出现1-2个轻微预警信号，建议进行常规客户维护\")\nprint(\"- 中风险: 出现2-4个预警信号，建议进行针对性的客户挽留措施\")\nprint(\"- 高风险: 出现4个以上预警信号，建议立即实施紧急挽留计划\")\n\n# 9. 流失率的时间趋势（假设有历史数据）\nprint(\"\\n=== 流失率时间趋势模拟 ===\")\n# 创建假设的月度流失率数据\nmonths = pd.date_range(start='2023-01-01', periods=12, freq='M')\nmonthly_churn_rates = [5.2, 5.3, 5.1, 5.4, 5.6, 5.8, 6.1, 6.3, 6.0, 5.7, 5.5, 5.3]\n\n# 可视化流失率趋势\nplt.figure(figsize=(12, 6))\nplt.plot(months, monthly_churn_rates, marker='o', linestyle='-', color='salmon')\nplt.title('月度客户流失率趋势')\nplt.xlabel('月份')\nplt.ylabel('流失率 (%)')\nplt.grid(True, linestyle='--', alpha=0.7)\nplt.tight_layout()\nplt.show()\n\n# 10. 流失客户的财务影响\nprint(\"\\n=== 流失客户的财务影响分析 ===\")\n# 假设的客户价值数据\navg_customer_value = 5000  # 平均客户年价值\ncustomer_acquisition_cost = 500  # 获取新客户成本\ntotal_customers = len(df)\n\n# 计算不同流失率下的财务影响\nchurn_rates = [0.05, 0.10, 0.15, 0.20]\nfinancial_impact = []\n\nfor rate in churn_rates:\n    churned_customers = total_customers * rate\n    lost_revenue = churned_customers * avg_customer_value\n    replacement_cost = churned_customers * customer_acquisition_cost\n    total_impact = lost_revenue + replacement_cost\n    \n    financial_impact.append({\n        'Churn_Rate': rate * 100,\n        'Churned_Customers': churned_customers,\n        'Lost_Revenue': lost_revenue,\n        'Replacement_Cost': replacement_cost,\n        'Total_Impact': total_impact\n    })\n\nimpact_df = pd.DataFrame(financial_impact)\nprint(impact_df)\n\n# 可视化财务影响\nplt.figure(figsize=(12, 6))\nx = impact_df['Churn_Rate']\n\nplt.bar(x, impact_df['Lost_Revenue'], label='损失收入', color='salmon', alpha=0.7)\nplt.bar(x, impact_df['Replacement_Cost'], bottom=impact_df['Lost_Revenue'], label='替换成本', color='lightblue', alpha=0.7)\n\nplt.title('不同流失率下的财务影响')\nplt.xlabel('流失率 (%)')\nplt.ylabel('财务影响 (元)')\nplt.legend()\nplt.grid(True, axis='y', linestyle='--', alpha=0.7)\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n通过明确定义流失指标和预警信号，银行可以更早地识别潜在流失客户，并采取相应措施降低流失率，从而减少财务损失。\")"
        },
        {
          "id": 3,
          "title": "预警变量筛选",
          "description": "识别和筛选有预测价值的客户特征变量",
          "example_code": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif, RFE\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom scipy.stats import pointbiserialr\nimport matplotlib as mpl\nfrom app.services.docker_matplotlib_fix import configure_matplotlib_fonts\n\n# 配置字体以支持中文显示\nconfigure_matplotlib_fonts()\nfrom app.services.bank_analysis import bank_service\n\n# 加载客户数据\ndf = bank_service.load_customer_churn_data()\nprint(f\"加载数据集，共{df.shape[0]}行，{df.shape[1]}列\")\n\n# 1. 数据预处理\nprint(\"\\n=== 数据预处理 ===\")\n\n# 检查目标变量\nif 'Churn' not in df.columns:\n    print(\"错误: 数据集中缺少目标变量'Churn'\")\n    exit()\n\n# 移除不需要的列\nnon_feature_cols = ['CustomerID', 'Name', 'Email', 'Address']\nfeature_cols = [col for col in df.columns if col != 'Churn' and col not in non_feature_cols]\n\n# 检查并处理缺失值\nmissing_values = df[feature_cols].isnull().sum()\nif missing_values.sum() > 0:\n    print(\"\\n存在缺失值的特征:\")\n    print(missing_values[missing_values > 0])\n    \n    # 使用中位数填充数值型缺失值\n    numeric_cols = df[feature_cols].select_dtypes(include=['int64', 'float64']).columns\n    for col in numeric_cols:\n        if df[col].isnull().sum() > 0:\n            median_val = df[col].median()\n            df[col].fillna(median_val, inplace=True)\n    \n    # 使用众数填充类别型缺失值\n    categorical_cols = df[feature_cols].select_dtypes(include=['object', 'category']).columns\n    for col in categorical_cols:\n        if df[col].isnull().sum() > 0:\n            mode_val = df[col].mode()[0]\n            df[col].fillna(mode_val, inplace=True)\n\n# 处理类别特征\ncategorical_cols = df[feature_cols].select_dtypes(include=['object', 'category']).columns.tolist()\nif categorical_cols:\n    print(f\"\\n对{len(categorical_cols)}个类别特征进行独热编码\")\n    df_encoded = pd.get_dummies(df, columns=categorical_cols, drop_first=True)\n    \n    # 更新特征列列表\n    feature_cols = [col for col in df_encoded.columns if col != 'Churn' and col not in non_feature_cols]\nelse:\n    df_encoded = df.copy()\n\n# 获取X和y\nX = df_encoded[feature_cols]\ny = df_encoded['Churn']\n\n# 分割训练集和测试集\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n\n# 标准化数值特征\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nprint(f\"\\n预处理后的特征数量: {len(feature_cols)}\")\nprint(f\"训练集: {X_train.shape[0]}行, 测试集: {X_test.shape[0]}行\")\n\n# 2. 单变量特征筛选\nprint(\"\\n=== 单变量特征重要性分析 ===\")\n\n# 2.1 点二列相关系数(连续特征)\nprint(\"\\n连续特征的点二列相关系数:\")\nnumeric_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n\nif numeric_cols:\n    corr_results = []\n    \n    for col in numeric_cols:\n        corr, p_value = pointbiserialr(y, X[col])\n        corr_results.append({\n            'Feature': col,\n            'Correlation': abs(corr),\n            'P_Value': p_value\n        })\n    \n    corr_df = pd.DataFrame(corr_results)\n    corr_df = corr_df.sort_values('Correlation', ascending=False)\n    \n    # 显示相关系数最高的特征\n    print(corr_df.head(10))\n    \n    # 可视化相关系数\n    plt.figure(figsize=(12, 8))\n    top_features = corr_df.head(15)['Feature'].tolist()\n    sns.barplot(x='Correlation', y='Feature', data=corr_df[corr_df['Feature'].isin(top_features)])\n    plt.title('特征与流失的点二列相关系数（绝对值）')\n    plt.xlabel('绝对相关系数')\n    plt.tight_layout()\n    plt.show()\n\n# 2.2 ANOVA F-value\nprint(\"\\nANOVA F-value特征筛选:\")\nanova_selector = SelectKBest(f_classif, k='all')\nanova_selector.fit(X_train_scaled, y_train)\n\n# 计算ANOVA F-value和P-value\nanova_scores = pd.DataFrame()\nanova_scores['Feature'] = X.columns\nanova_scores['F_Value'] = anova_selector.scores_\nanova_scores['P_Value'] = anova_selector.pvalues_\nanova_scores = anova_scores.sort_values('F_Value', ascending=False)\n\n# 显示得分最高的特征\nprint(anova_scores.head(10))\n\n# 可视化得分最高的特征\nplt.figure(figsize=(12, 8))\ntop_features = anova_scores.head(15)['Feature'].tolist()\nsns.barplot(x='F_Value', y='Feature', data=anova_scores[anova_scores['Feature'].isin(top_features)])\nplt.title('ANOVA F-value特征重要性')\nplt.xlabel('F-value')\nplt.tight_layout()\nplt.show()\n\n# 2.3 互信息筛选\nprint(\"\\n互信息特征筛选:\")\nmi_selector = SelectKBest(mutual_info_classif, k='all')\nmi_selector.fit(X_train_scaled, y_train)\n\n# 计算互信息分数\nmi_scores = pd.DataFrame()\nmi_scores['Feature'] = X.columns\nmi_scores['MI_Score'] = mi_selector.scores_\nmi_scores = mi_scores.sort_values('MI_Score', ascending=False)\n\n# 显示得分最高的特征\nprint(mi_scores.head(10))\n\n# 可视化得分最高的特征\nplt.figure(figsize=(12, 8))\ntop_features = mi_scores.head(15)['Feature'].tolist()\nsns.barplot(x='MI_Score', y='Feature', data=mi_scores[mi_scores['Feature'].isin(top_features)])\nplt.title('互信息特征重要性')\nplt.xlabel('互信息分数')\nplt.tight_layout()\nplt.show()\n\n# 3. 基于模型的特征筛选\nprint(\"\\n=== 基于模型的特征重要性分析 ===\")\n\n# 3.1 随机森林特征重要性\nprint(\"\\n随机森林特征重要性:\")\nrf_model = RandomForestClassifier(n_estimators=100, random_state=42)\nrf_model.fit(X_train_scaled, y_train)\n\n# 计算特征重要性\nrf_importances = pd.DataFrame()\nrf_importances['Feature'] = X.columns\nrf_importances['Importance'] = rf_model.feature_importances_\nrf_importances = rf_importances.sort_values('Importance', ascending=False)\n\n# 显示最重要的特征\nprint(rf_importances.head(10))\n\n# 可视化特征重要性\nplt.figure(figsize=(12, 8))\ntop_features = rf_importances.head(15)['Feature'].tolist()\nsns.barplot(x='Importance', y='Feature', data=rf_importances[rf_importances['Feature'].isin(top_features)])\nplt.title('随机森林特征重要性')\nplt.xlabel('重要性')\nplt.tight_layout()\nplt.show()\n\n# 3.2 递归特征消除(RFE)\nprint(\"\\n递归特征消除(RFE):\")\nlogit_model = LogisticRegression(max_iter=1000, random_state=42)\nrfe = RFE(estimator=logit_model, n_features_to_select=15, step=1)\nrfe.fit(X_train_scaled, y_train)\n\n# 获取RFE结果\nrfe_results = pd.DataFrame()\nrfe_results['Feature'] = X.columns\nrfe_results['Selected'] = rfe.support_\nrfe_results['Rank'] = rfe.ranking_\nrfe_results = rfe_results.sort_values('Rank')\n\n# 显示被选中的特征\nprint(rfe_results[rfe_results['Selected'] == True])\n\n# 4. 特征重要性综合分析\nprint(\"\\n=== 特征重要性综合分析 ===\")\n\n# 创建一个综合得分\nfeature_importance = pd.DataFrame({'Feature': X.columns})\n\n# 添加各种特征选择方法的排名\nif numeric_cols:\n    corr_df['Corr_Rank'] = corr_df['Correlation'].rank(ascending=False)\n    feature_importance = feature_importance.merge(corr_df[['Feature', 'Corr_Rank']], on='Feature', how='left')\n\nanova_scores['Anova_Rank'] = anova_scores['F_Value'].rank(ascending=False)\nfeature_importance = feature_importance.merge(anova_scores[['Feature', 'Anova_Rank']], on='Feature', how='left')\n\nmi_scores['MI_Rank'] = mi_scores['MI_Score'].rank(ascending=False)\nfeature_importance = feature_importance.merge(mi_scores[['Feature', 'MI_Rank']], on='Feature', how='left')\n\nrf_importances['RF_Rank'] = rf_importances['Importance'].rank(ascending=False)\nfeature_importance = feature_importance.merge(rf_importances[['Feature', 'RF_Rank']], on='Feature', how='left')\n\nfeature_importance = feature_importance.merge(rfe_results[['Feature', 'Rank']], on='Feature', how='left')\nfeature_importance.rename(columns={'Rank': 'RFE_Rank'}, inplace=True)\n\n# 计算平均排名\nrank_columns = [col for col in feature_importance.columns if col.endswith('_Rank')]\nfeature_importance['Avg_Rank'] = feature_importance[rank_columns].mean(axis=1)\nfeature_importance = feature_importance.sort_values('Avg_Rank')\n\n# 显示综合排名前20的特征\nprint(\"综合特征重要性排名(前20名):\")\nprint(feature_importance.head(20))\n\n# 可视化综合排名靠前的特征\nplt.figure(figsize=(12, 8))\ntop_features = feature_importance.head(15)['Feature'].tolist()\nsns.barplot(y='Feature', x='Avg_Rank', data=feature_importance[feature_importance['Feature'].isin(top_features)])\nplt.title('特征重要性综合排名')\nplt.xlabel('平均排名(数值越小越重要)')\nplt.tight_layout()\nplt.show()\n\n# 5. 最终特征选择\nprint(\"\\n=== 最终特征选择 ===\")\n# 选择排名前15的特征\ntop_n_features = 15\nselected_features = feature_importance.head(top_n_features)['Feature'].tolist()\n\nprint(f\"选择的{top_n_features}个最重要特征:\")\nfor i, feature in enumerate(selected_features, 1):\n    print(f\"{i}. {feature}\")\n\n# 检查所选特征间的相关性\nif len(numeric_cols) > 0:\n    correlation_matrix = X[selected_features].corr()\n    \n    # 可视化相关性矩阵\n    plt.figure(figsize=(12, 10))\n    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n    plt.title('所选特征间的相关性矩阵')\n    plt.tight_layout()\n    plt.show()\n    \n    # 检测高度相关的特征对\n    print(\"\\n高度相关的特征对(相关性绝对值>0.8):\")\n    high_corr_found = False\n    \n    for i in range(len(selected_features)):\n        for j in range(i+1, len(selected_features)):\n            corr = correlation_matrix.iloc[i, j]\n            if abs(corr) > 0.8:\n                print(f\"- {selected_features[i]} 与 {selected_features[j]}: {corr:.3f}\")\n                high_corr_found = True\n    \n    if not high_corr_found:\n        print(\"未发现高度相关的特征对\")\n\n# 6. 特征与流失的可视化分析\nprint(\"\\n=== 特征与流失的关系可视化 ===\")\n\n# 选择前几个最重要的特征进行可视化\nn_features_to_plot = min(6, len(selected_features))  # 最多展示6个特征\n\n# 为分类特征生成箱型图，为连续特征生成直方图\nplt.figure(figsize=(15, n_features_to_plot * 4))\n\nfor i, feature in enumerate(selected_features[:n_features_to_plot]):\n    if feature in df.columns:  # 确保特征在原始数据框中\n        feature_data = df[[feature, 'Churn']]\n        \n        # 移除可能的缺失值\n        feature_data = feature_data.dropna()\n        \n        # 根据特征类型选择不同的可视化方法\n        if feature_data[feature].dtype in ['int64', 'float64'] and feature_data[feature].nunique() > 10:\n            # 连续数值特征 - 使用直方图和密度图\n            plt.subplot(n_features_to_plot, 2, 2*i+1)\n            sns.histplot(data=feature_data, x=feature, hue='Churn', kde=True, bins=30, palette=['lightblue', 'salmon'])\n            plt.title(f'{feature}的分布与流失关系')\n            \n            # 箱形图\n            plt.subplot(n_features_to_plot, 2, 2*i+2)\n            sns.boxplot(x='Churn', y=feature, data=feature_data, palette=['lightblue', 'salmon'])\n            plt.title(f'{feature}在不同流失状态下的分布')\n            \n        else:\n            # 分类特征或低基数特征 - 使用计数图和百分比堆叠图\n            plt.subplot(n_features_to_plot, 2, 2*i+1)\n            sns.countplot(x=feature, hue='Churn', data=feature_data, palette=['lightblue', 'salmon'])\n            plt.title(f'{feature}类别与流失关系')\n            plt.xticks(rotation=45)\n            \n            # 按类别的流失率\n            plt.subplot(n_features_to_plot, 2, 2*i+2)\n            category_churn = feature_data.groupby(feature)['Churn'].mean() * 100\n            sns.barplot(x=category_churn.index, y=category_churn.values, palette='YlOrRd')\n            plt.title(f'按{feature}分类的流失率(%)')\n            plt.xticks(rotation=45)\n            plt.ylabel('流失率 (%)')\n\nplt.tight_layout()\nplt.show()\n\n# 7. 结论与建议\nprint(\"\\n=== 结论与建议 ===\")\nprint(\"基于预警变量筛选的结论:\")\nprint(f\"1. 我们从{len(feature_cols)}个初始特征中筛选出{len(selected_features)}个最具预测价值的特征\")\nprint(\"2. 主要的预警指标包括:\")\n\n# 展示前5个重要特征及其解释\ntop_5_features = feature_importance.head(5)['Feature'].tolist()\nfor i, feature in enumerate(top_5_features, 1):\n    # 这里可以添加对每个特征的业务解释\n    print(f\"   {i}. {feature}\")\n\nprint(\"\\n3. 预警模型构建建议:\")\nprint(\"   - 使用筛选的特征集构建预警模型\")\nprint(\"   - 重点关注交易行为的变化趋势特征\")\nprint(\"   - 考虑客户生命周期的不同阶段特征\")\nprint(\"   - 对相关性高的特征考虑进行特征工程或只选择其中一个\")\n\nprint(\"\\n4. 业务应用建议:\")\nprint(\"   - 建立以上述关键指标为基础的客户流失风险监控仪表板\")\nprint(\"   - 设置差异化的预警阈值，并定期对阈值进行校准\")\nprint(\"   - 将预警结果与客户价值结合，优先关注高价值客户\")"
        },
        {
          "id": 4,
          "title": "预警模型构建",
          "description": "使用随机森林、逻辑回归等算法构建流失预警模型",
          "example_code": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\nfrom sklearn.metrics import confusion_matrix, classification_report, roc_curve, precision_recall_curve\nfrom sklearn.pipeline import Pipeline\nfrom imblearn.over_sampling import SMOTE\nimport matplotlib as mpl\nfrom app.services.docker_matplotlib_fix import configure_matplotlib_fonts\n\n# 配置字体以支持中文显示\nconfigure_matplotlib_fonts()\nfrom app.services.bank_analysis import bank_service\n\n# 加载客户数据\ndf = bank_service.load_customer_churn_data()\nprint(f\"加载数据集，共{df.shape[0]}行，{df.shape[1]}列\")\n\n# 1. 数据准备\nprint(\"\\n=== 数据准备 ===\")\n\n# 检查目标变量\nif 'Churn' not in df.columns:\n    print(\"错误: 数据集中缺少目标变量'Churn'\")\n    exit()\n\n# 检查流失率\nchurn_rate = df['Churn'].mean() * 100\nprint(f\"客户流失率: {churn_rate:.2f}%\")\n\n# 使用之前步骤中筛选出的重要特征\n# 注意：这里假设我们已经从上一步中得到了重要特征列表\n# 在实际应用中，可以从上一步中加载或者重新筛选特征\nimportant_features = [\n    'DaysSinceLastTransaction', 'TransactionFrequencyDecline', 'TransactionAmountDecline',\n    'Tenure', 'CustomerValue', 'TransactionFrequency', 'TransactionAmount',\n    'SupportCallsCount', 'ProductCount', 'Age', 'ComplaintsCount',\n    'IsDigitalActive', 'IsMobileActive', 'HasCreditCard', 'HasLoan'\n]\n\n# 检查所有重要特征是否在数据集中\nmissing_features = [feature for feature in important_features if feature not in df.columns]\nif missing_features:\n    print(f\"警告: 以下特征不在数据集中: {missing_features}\")\n    important_features = [feature for feature in important_features if feature in df.columns]\n\n# 如果没有足够的特征，使用所有可用的数值特征\nif len(important_features) < 5:\n    print(\"特征太少，将使用所有数值特征\")\n    important_features = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n    if 'Churn' in important_features:\n        important_features.remove('Churn')\n\nprint(f\"使用{len(important_features)}个特征构建预警模型:\")\nfor i, feature in enumerate(important_features, 1):\n    print(f\"{i}. {feature}\")\n\n# 准备特征和目标变量\nX = df[important_features]\ny = df['Churn']\n\n# 划分训练集和测试集\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=42, stratify=y\n)\nprint(f\"训练集: {X_train.shape[0]}行, 测试集: {X_test.shape[0]}行\")\n\n# 检查类别不平衡\nneg_count = (y_train == 0).sum()\npos_count = (y_train == 1).sum()\nratio = neg_count / pos_count\nprint(f\"训练集中未流失客户: {neg_count}, 流失客户: {pos_count}, 比例: {ratio:.2f}:1\")\n\n# 2. 创建评估模型的函数\ndef evaluate_model(model, X_train, X_test, y_train, y_test, model_name, use_smote=False):\n    # 复制数据，避免修改原始数据\n    X_train_copy = X_train.copy()\n    y_train_copy = y_train.copy()\n    \n    # 如果使用SMOTE处理类别不平衡\n    if use_smote:\n        print(f\"\\n对{model_name}使用SMOTE处理类别不平衡...\")\n        smote = SMOTE(random_state=42)\n        X_train_copy, y_train_copy = smote.fit_resample(X_train_copy, y_train_copy)\n        print(f\"SMOTE后，训练集形状: {X_train_copy.shape}\")\n        print(f\"流失占比: {y_train_copy.mean():.2f}\")\n    \n    # 训练模型\n    print(f\"\\n训练{model_name}...\")\n    model.fit(X_train_copy, y_train_copy)\n    \n    # 预测\n    y_pred = model.predict(X_test)\n    \n    # 获取预测概率（如果模型支持）\n    if hasattr(model, 'predict_proba'):\n        y_prob = model.predict_proba(X_test)[:, 1]\n    else:\n        y_prob = model.decision_function(X_test) if hasattr(model, 'decision_function') else y_pred\n    \n    # 计算评估指标\n    accuracy = accuracy_score(y_test, y_pred)\n    precision = precision_score(y_test, y_pred)\n    recall = recall_score(y_test, y_pred)\n    f1 = f1_score(y_test, y_pred)\n    auc = roc_auc_score(y_test, y_prob)\n    \n    # 输出评估结果\n    print(f\"\\n{model_name}评估结果:\")\n    print(f\"准确率(Accuracy): {accuracy:.4f}\")\n    print(f\"精确率(Precision): {precision:.4f}\")\n    print(f\"召回率(Recall): {recall:.4f}\")\n    print(f\"F1分数: {f1:.4f}\")\n    print(f\"ROC AUC: {auc:.4f}\")\n    \n    # 混淆矩阵\n    cm = confusion_matrix(y_test, y_pred)\n    tn, fp, fn, tp = cm.ravel()\n    print(f\"\\n混淆矩阵:\")\n    print(f\"真负例(TN): {tn}\")\n    print(f\"假正例(FP): {fp}\")\n    print(f\"假负例(FN): {fn}\")\n    print(f\"真正例(TP): {tp}\")\n    \n    # 计算特定业务指标\n    print(\"\\n业务指标:\")\n    # 假设流失客户的平均损失是5000元，误判非流失的成本是1000元\n    cost_fn = 5000  # 假负例成本 (错过流失客户的损失)\n    cost_fp = 1000  # 假正例成本 (对非流失客户采取不必要措施的成本)\n    total_cost = (fn * cost_fn) + (fp * cost_fp)\n    print(f\"总错误成本: {total_cost}元 (FN: {fn*cost_fn}元, FP: {fp*cost_fp}元)\")\n    \n    return {\n        'model': model,\n        'name': model_name,\n        'accuracy': accuracy,\n        'precision': precision,\n        'recall': recall,\n        'f1': f1,\n        'auc': auc,\n        'confusion_matrix': cm,\n        'y_pred': y_pred,\n        'y_prob': y_prob,\n        'cost': total_cost\n    }\n\n# 3. 训练和评估多个模型\nprint(\"\\n=== 训练和评估模型 ===\")\n\n# 3.1 逻辑回归模型\npipeline_lr = Pipeline([\n    ('scaler', StandardScaler()),\n    ('classifier', LogisticRegression(max_iter=1000, random_state=42))\n])\nresults_lr = evaluate_model(\n    pipeline_lr, X_train, X_test, y_train, y_test, \"逻辑回归\", use_smote=True\n)\n\n# 3.2 随机森林模型\npipeline_rf = Pipeline([\n    ('scaler', StandardScaler()),\n    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))\n])\nresults_rf = evaluate_model(\n    pipeline_rf, X_train, X_test, y_train, y_test, \"随机森林\", use_smote=True\n)\n\n# 3.3 梯度提升树模型\npipeline_gbt = Pipeline([\n    ('scaler', StandardScaler()),\n    ('classifier', GradientBoostingClassifier(n_estimators=100, random_state=42))\n])\nresults_gbt = evaluate_model(\n    pipeline_gbt, X_train, X_test, y_train, y_test, \"梯度提升树\", use_smote=True\n)\n\n# 3.4 支持向量机模型\npipeline_svm = Pipeline([\n    ('scaler', StandardScaler()),\n    ('classifier', SVC(probability=True, random_state=42))\n])\nresults_svm = evaluate_model(\n    pipeline_svm, X_train, X_test, y_train, y_test, \"支持向量机\", use_smote=True\n)\n\n# 4. 比较模型性能\nprint(\"\\n=== 模型性能比较 ===\")\nresults = [results_lr, results_rf, results_gbt, results_svm]\nmodel_names = [result['name'] for result in results]\naccuracies = [result['accuracy'] for result in results]\nprecisions = [result['precision'] for result in results]\nrecalls = [result['recall'] for result in results]\nf1_scores = [result['f1'] for result in results]\naucs = [result['auc'] for result in results]\ncosts = [result['cost'] for result in results]\n\n# 创建比较表格\ncomparison_df = pd.DataFrame({\n    '模型': model_names,\n    '准确率': accuracies,\n    '精确率': precisions,\n    '召回率': recalls,\n    'F1分数': f1_scores,\n    'ROC AUC': aucs,\n    '错误成本': costs\n})\nprint(comparison_df)\n\n# 可视化比较结果\nplt.figure(figsize=(15, 10))\n\n# 准确率比较\nplt.subplot(2, 2, 1)\nsns.barplot(x='模型', y='准确率', data=comparison_df)\nplt.title('模型准确率比较')\nplt.ylim(0.7, 1.0)\n\n# 精确率和召回率比较\nplt.subplot(2, 2, 2)\ncomparison_plot = pd.melt(comparison_df, id_vars=['模型'], value_vars=['精确率', '召回率'], var_name='指标', value_name='值')\nsns.barplot(x='模型', y='值', hue='指标', data=comparison_plot)\nplt.title('模型精确率和召回率比较')\nplt.ylim(0.0, 1.0)\n\n# F1和AUC比较\nplt.subplot(2, 2, 3)\ncomparison_plot = pd.melt(comparison_df, id_vars=['模型'], value_vars=['F1分数', 'ROC AUC'], var_name='指标', value_name='值')\nsns.barplot(x='模型', y='值', hue='指标', data=comparison_plot)\nplt.title('模型F1分数和ROC AUC比较')\nplt.ylim(0.0, 1.0)\n\n# 成本比较\nplt.subplot(2, 2, 4)\nsns.barplot(x='模型', y='错误成本', data=comparison_df, palette='YlOrRd_r')\nplt.title('模型错误成本比较')\nplt.ylabel('错误成本(元)')\n\nplt.tight_layout()\nplt.show()\n\n# 5. 模型调优 (以最佳模型为例)\n# 假设随机森林是初步评估中的最佳模型\nprint(\"\\n=== 随机森林模型调优 ===\")\n\n# 设定参数网格\nparam_grid = {\n    'classifier__n_estimators': [50, 100, 200],\n    'classifier__max_depth': [None, 10, 20, 30],\n    'classifier__min_samples_split': [2, 5, 10],\n    'classifier__min_samples_leaf': [1, 2, 4],\n    'classifier__class_weight': [None, 'balanced', 'balanced_subsample']\n}\n\n# 使用网格搜索和交叉验证\nprint(\"执行网格搜索交叉验证来优化随机森林参数 (这可能需要一些时间)...\")\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\ngrid_search = GridSearchCV(\n    pipeline_rf,\n    param_grid=param_grid,\n    scoring='f1',  # 使用F1分数作为评估指标\n    cv=cv,\n    n_jobs=-1,  # 使用所有可用CPU\n    verbose=1\n)\n\n# 使用SMOTE处理过的数据进行训练\nsmote = SMOTE(random_state=42)\nX_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\ngrid_search.fit(X_train_smote, y_train_smote)\n\n# 输出最佳参数\nprint(f\"\\n最佳参数: {grid_search.best_params_}\")\nprint(f\"最佳交叉验证F1分数: {grid_search.best_score_:.4f}\")\n\n# 使用最佳参数的模型进行评估\nbest_rf = grid_search.best_estimator_\nresults_best_rf = evaluate_model(\n    best_rf, X_train, X_test, y_train, y_test, \"调优后的随机森林\", use_smote=True\n)\n\n# 6. ROC曲线和PR曲线分析\nprint(\"\\n=== ROC曲线和PR曲线分析 ===\")\nplt.figure(figsize=(15, 6))\n\n# ROC曲线\nplt.subplot(1, 2, 1)\nfor result in results + [results_best_rf]:\n    fpr, tpr, _ = roc_curve(y_test, result['y_prob'])\n    plt.plot(fpr, tpr, label=f'{result[\"name\"]} (AUC = {result[\"auc\"]:.3f})')\n\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlabel('假正例率(False Positive Rate)')\nplt.ylabel('真正例率(True Positive Rate)')\nplt.title('ROC曲线比较')\nplt.legend()\nplt.grid(alpha=0.3)\n\n# PR曲线\nplt.subplot(1, 2, 2)\nfor result in results + [results_best_rf]:\n    precision_curve, recall_curve, _ = precision_recall_curve(y_test, result['y_prob'])\n    plt.plot(recall_curve, precision_curve, label=f'{result[\"name\"]} (F1 = {result[\"f1\"]:.3f})')\n\nplt.xlabel('召回率(Recall)')\nplt.ylabel('精确率(Precision)')\nplt.title('精确率-召回率曲线比较')\nplt.legend()\nplt.grid(alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# 7. 阈值分析 (以最佳模型为例)\nprint(\"\\n=== 预测阈值分析 ===\")\n# 获取最佳模型的预测概率\nbest_model = results_best_rf\ny_prob = best_model['y_prob']\n\n# 测试不同的阈值\nthresholds = np.arange(0.1, 1.0, 0.1)\nthreshold_metrics = []\n\nfor threshold in thresholds:\n    # 根据阈值生成预测\n    y_pred_threshold = (y_prob >= threshold).astype(int)\n    \n    # 计算指标\n    accuracy = accuracy_score(y_test, y_pred_threshold)\n    precision = precision_score(y_test, y_pred_threshold)\n    recall = recall_score(y_test, y_pred_threshold)\n    f1 = f1_score(y_test, y_pred_threshold)\n    \n    # 计算成本\n    cm = confusion_matrix(y_test, y_pred_threshold)\n    tn, fp, fn, tp = cm.ravel()\n    cost_fn = 5000  # 假负例成本\n    cost_fp = 1000  # 假正例成本\n    total_cost = (fn * cost_fn) + (fp * cost_fp)\n    \n    threshold_metrics.append({\n        '阈值': threshold,\n        '准确率': accuracy,\n        '精确率': precision,\n        '召回率': recall,\n        'F1分数': f1,\n        '假正例(FP)': fp,\n        '假负例(FN)': fn,\n        '错误成本': total_cost\n    })\n\n# 创建阈值分析表格\nthreshold_df = pd.DataFrame(threshold_metrics)\nprint(threshold_df)\n\n# 可视化阈值分析\nplt.figure(figsize=(15, 10))\n\n# 绘制准确率、精确率、召回率和F1随阈值的变化\nplt.subplot(2, 2, 1)\nplt.plot(threshold_df['阈值'], threshold_df['准确率'], marker='o', label='准确率')\nplt.plot(threshold_df['阈值'], threshold_df['精确率'], marker='s', label='精确率')\nplt.plot(threshold_df['阈值'], threshold_df['召回率'], marker='^', label='召回率')\nplt.plot(threshold_df['阈值'], threshold_df['F1分数'], marker='*', label='F1分数')\nplt.xlabel('预测阈值')\nplt.ylabel('指标值')\nplt.title('性能指标与预测阈值的关系')\nplt.legend()\nplt.grid(alpha=0.3)\n\n# 绘制FP和FN随阈值的变化\nplt.subplot(2, 2, 2)\nplt.plot(threshold_df['阈值'], threshold_df['假正例(FP)'], marker='o', label='假正例(FP)')\nplt.plot(threshold_df['阈值'], threshold_df['假负例(FN)'], marker='s', label='假负例(FN)')\nplt.xlabel('预测阈值')\nplt.ylabel('错误计数')\nplt.title('错误类型与预测阈值的关系')\nplt.legend()\nplt.grid(alpha=0.3)\n\n# 绘制错误成本随阈值的变化\nplt.subplot(2, 2, 3)\nplt.plot(threshold_df['阈值'], threshold_df['错误成本'], marker='o', color='red')\nplt.xlabel('预测阈值')\nplt.ylabel('错误成本(元)')\nplt.title('错误成本与预测阈值的关系')\nplt.grid(alpha=0.3)\n\n# 找出成本最低的阈值\nmin_cost_threshold = threshold_df.loc[threshold_df['错误成本'].idxmin(), '阈值']\nmin_cost = threshold_df['错误成本'].min()\nplt.axvline(min_cost_threshold, color='green', linestyle='--', label=f'最优阈值={min_cost_threshold}')\nplt.legend()\n\n# 显示不同阈值下的混淆矩阵热图\nplt.subplot(2, 2, 4)\nopt_index = threshold_df['阈值'].tolist().index(min_cost_threshold)\ny_pred_opt = (y_prob >= min_cost_threshold).astype(int)\ncm_opt = confusion_matrix(y_test, y_pred_opt)\nsns.heatmap(cm_opt, annot=True, fmt='d', cmap='Blues')\nplt.title(f'最优阈值({min_cost_threshold})下的混淆矩阵')\nplt.xlabel('预测标签')\nplt.ylabel('真实标签')\n\nplt.tight_layout()\nplt.show()\n\n# 8. 特征重要性分析（针对随机森林模型）\nprint(\"\\n=== 特征重要性分析 ===\")\n\n# 获取最佳随机森林模型的特征重要性\nbest_rf_classifier = best_rf.named_steps['classifier']\nfeature_importances = best_rf_classifier.feature_importances_\n\n# 创建特征重要性数据框\nimportance_df = pd.DataFrame({\n    '特征': important_features,\n    '重要性': feature_importances\n})\nimportance_df = importance_df.sort_values('重要性', ascending=False)\n\n# 显示特征重要性\nprint(\"随机森林特征重要性:\")\nprint(importance_df)\n\n# 可视化特征重要性\nplt.figure(figsize=(12, 8))\nsns.barplot(x='重要性', y='特征', data=importance_df)\nplt.title('随机森林特征重要性')\nplt.xlabel('重要性')\nplt.tight_layout()\nplt.show()\n\n# 9. 最终模型保存和使用示例\nprint(\"\\n=== 最终模型 ===\")\n\n# 使用最优阈值\noptimal_threshold = min_cost_threshold\nprint(f\"最优预测阈值: {optimal_threshold}\")\n\n# 应用最终模型进行预测的示例\ndef predict_churn_risk(model, threshold, customer_data):\n    \"\"\"使用训练好的模型和优化的阈值预测客户流失风险\"\"\"\n    # 确保数据格式正确\n    if isinstance(customer_data, pd.DataFrame):\n        # 确保包含所有需要的特征\n        missing_cols = set(important_features) - set(customer_data.columns)\n        if missing_cols:\n            raise ValueError(f\"客户数据缺少以下特征: {missing_cols}\")\n        \n        # 仅选择模型所需特征\n        X = customer_data[important_features]\n    else:\n        raise ValueError(\"客户数据必须是pandas DataFrame格式\")\n    \n    # 预测概率\n    churn_proba = model.predict_proba(X)[:, 1]\n    \n    # 根据阈值确定预测结果\n    predictions = (churn_proba >= threshold).astype(int)\n    \n    # 返回预测结果和概率\n    results = pd.DataFrame({\n        'customer_id': customer_data.index if customer_data.index.name else range(len(X)),\n        'churn_probability': churn_proba,\n        'churn_prediction': predictions\n    })\n    \n    # 添加风险等级\n    results['risk_level'] = pd.cut(\n        results['churn_probability'],\n        bins=[0, 0.3, 0.6, 0.8, 1.0],\n        labels=['低风险', '中风险', '高风险', '极高风险']\n    )\n    \n    return results\n\n# 演示最终模型预测\nprint(\"\\n预测示例:\")\n# 使用测试集的前10行作为示例\nsample_customers = X_test.head(10).copy()\n\n# 预测这些客户的流失风险\nprediction_results = predict_churn_risk(best_rf, optimal_threshold, sample_customers)\n\n# 添加实际标签用于比较\nprediction_results['actual_churn'] = y_test.head(10).values\n\n# 显示预测结果\nprint(prediction_results[['churn_probability', 'churn_prediction', 'risk_level', 'actual_churn']])\n\n# 10. 模型总结与业务建议\nprint(\"\\n=== 模型总结与业务建议 ===\")\nprint(\"客户流失预警模型总结:\")\nprint(f\"1. 最佳模型: {best_model['name']}\")\nprint(f\"2. 模型性能: F1分数 = {best_model['f1']:.4f}, ROC AUC = {best_model['auc']:.4f}\")\nprint(f\"3. 最优预测阈值: {optimal_threshold} (基于错误成本最小化)\")\nprint(\"4. 关键预警特征:\")\nfor i, row in importance_df.head(5).iterrows():\n    print(f\"   - {row['特征']}: {row['重要性']:.4f}\")\n\nprint(\"\\n客户挽留策略建议:\")\nprint(\"1. 对于高风险和极高风险客户，实施主动挽留措施\")\nprint(\"2. 针对不同风险等级的客户制定差异化的挽留策略\")\nprint(\"3. 定期更新和评估模型，调整预测阈值以适应业务变化\")\nprint(\"4. 结合客户价值，优先挽留高价值且高流失风险的客户\")\nprint(\"5. 将流失风险预测集成到客户关系管理系统中，支持业务决策\")"
        },
        {
          "id": 5,
          "title": "模型验证",
          "description": "使用ROC曲线、混淆矩阵等方法评估模型效果",
          "example_code": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\nfrom sklearn.metrics import confusion_matrix, classification_report, roc_curve, precision_recall_curve\nfrom sklearn.calibration import calibration_curve\nfrom sklearn.pipeline import Pipeline\nfrom imblearn.over_sampling import SMOTE\nimport matplotlib as mpl\nfrom app.services.docker_matplotlib_fix import configure_matplotlib_fonts\n\n# 配置字体以支持中文显示\nconfigure_matplotlib_fonts()\nfrom app.services.bank_analysis import bank_service\n\n# 从上一步骤中加载已训练的模型和相关数据\n# 在实际应用中，可能需要重新加载数据和模型或者从上一步骤中获取\n\n# 加载客户数据\ndf = bank_service.load_customer_churn_data()\nprint(f\"加载数据集，共{df.shape[0]}行，{df.shape[1]}列\")\n\n# 确定重要特征\nimportant_features = [\n    'DaysSinceLastTransaction', 'TransactionFrequencyDecline', 'TransactionAmountDecline',\n    'Tenure', 'CustomerValue', 'TransactionFrequency', 'TransactionAmount',\n    'SupportCallsCount', 'ProductCount', 'Age', 'ComplaintsCount',\n    'IsDigitalActive', 'IsMobileActive', 'HasCreditCard', 'HasLoan'\n]\n\n# 检查所有重要特征是否在数据集中\nmissing_features = [feature for feature in important_features if feature not in df.columns]\nif missing_features:\n    print(f\"警告: 以下特征不在数据集中: {missing_features}\")\n    important_features = [feature for feature in important_features if feature in df.columns]\n\n# 准备特征和目标变量\nX = df[important_features]\ny = df['Churn']\n\n# 划分训练集和测试集\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=42, stratify=y\n)\n\n# 1. 模型训练 (重新训练简化版模型，用于演示验证过程)\nprint(\"\\n=== 模型训练 ===\")\n\n# 使用随机森林作为示例模型\nfrom sklearn.ensemble import RandomForestClassifier\n\n# 应用SMOTE处理类别不平衡\nsmote = SMOTE(random_state=42)\nX_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n\n# 创建数据处理和模型训练管道\npipeline = Pipeline([\n    ('scaler', StandardScaler()),\n    ('classifier', RandomForestClassifier(n_estimators=100, max_depth=20, random_state=42))\n])\n\n# 训练模型\npipeline.fit(X_train_smote, y_train_smote)\n\n# 在测试集上进行预测\ny_pred = pipeline.predict(X_test)\ny_prob = pipeline.predict_proba(X_test)[:, 1]\n\n# 2. 基本性能评估\nprint(\"\\n=== 基本性能评估 ===\")\n\n# 计算常用评估指标\naccuracy = accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred)\nrecall = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\nauc = roc_auc_score(y_test, y_prob)\n\nprint(f\"准确率: {accuracy:.4f}\")\nprint(f\"精确率: {precision:.4f}\")\nprint(f\"召回率: {recall:.4f}\")\nprint(f\"F1分数: {f1:.4f}\")\nprint(f\"ROC AUC: {auc:.4f}\")\n\n# 3. 混淆矩阵分析\nprint(\"\\n=== 混淆矩阵分析 ===\")\n\n# 计算混淆矩阵\ncm = confusion_matrix(y_test, y_pred)\ntn, fp, fn, tp = cm.ravel()\n\nprint(f\"真负例(TN): {tn}\")\nprint(f\"假正例(FP): {fp}\")\nprint(f\"假负例(FN): {fn}\")\nprint(f\"真正例(TP): {tp}\")\n\n# 计算更详细的指标\nsensitivity = recall  # 敏感度 = 召回率 = TP/(TP+FN)\nspecificity = tn / (tn + fp)  # 特异度 = TN/(TN+FP)\nprecision = tp / (tp + fp)  # 精确率 = TP/(TP+FP)\nnpv = tn / (tn + fn)  # 负预测值 = TN/(TN+FN)\nfallout = fp / (fp + tn)  # 误诊率 = FP/(FP+TN)\n\nprint(f\"敏感度(Sensitivity/Recall): {sensitivity:.4f}\")\nprint(f\"特异度(Specificity): {specificity:.4f}\")\nprint(f\"精确率(Precision): {precision:.4f}\")\nprint(f\"负预测值(NPV): {npv:.4f}\")\nprint(f\"误诊率(Fall-out): {fallout:.4f}\")\n\n# 可视化混淆矩阵\nplt.figure(figsize=(10, 8))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\nplt.title('混淆矩阵')\nplt.xlabel('预测标签')\nplt.ylabel('真实标签')\nplt.xticks([0.5, 1.5], ['未流失(0)', '已流失(1)'])\nplt.yticks([0.5, 1.5], ['未流失(0)', '已流失(1)'])\nplt.tight_layout()\nplt.show()\n\n# 4. ROC曲线分析\nprint(\"\\n=== ROC曲线分析 ===\")\n\n# 计算ROC曲线\nfpr, tpr, thresholds = roc_curve(y_test, y_prob)\n\n# 计算每个阈值下的特异度\nspecificity = 1 - fpr\n\n# 可视化ROC曲线\nplt.figure(figsize=(10, 8))\nplt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC曲线 (AUC = {auc:.3f})')\nplt.plot([0, 1], [0, 1], color='gray', linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('假正例率 (1 - 特异度)')\nplt.ylabel('真正例率 (敏感度)')\nplt.title('ROC曲线')\nplt.legend(loc='lower right')\nplt.grid(alpha=0.3)\n\n# 在ROC曲线上标记几个关键点\nidx_max_youden = np.argmax(tpr - fpr)  # Youden指数最大点\nidx_min_dist = np.argmin((1-tpr)**2 + fpr**2)  # 距离左上角最近点\n\nplt.scatter(fpr[idx_max_youden], tpr[idx_max_youden], marker='o', color='red', \n           label=f'最大Youden指数 (阈值={thresholds[idx_max_youden]:.2f})')\nplt.scatter(fpr[idx_min_dist], tpr[idx_min_dist], marker='s', color='green', \n           label=f'最佳平衡点 (阈值={thresholds[idx_min_dist]:.2f})')\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n# 5. 精确率-召回率曲线分析\nprint(\"\\n=== 精确率-召回率曲线分析 ===\")\n\n# 计算精确率-召回率曲线\nprecision_curve, recall_curve, pr_thresholds = precision_recall_curve(y_test, y_prob)\n\n# 可视化精确率-召回率曲线\nplt.figure(figsize=(10, 8))\nplt.plot(recall_curve, precision_curve, color='green', lw=2, label='PR曲线')\n\n# 计算平均精确率\navg_precision = np.mean(precision_curve)\nplt.axhline(y=avg_precision, color='gray', linestyle='--', label=f'平均精确率 = {avg_precision:.3f}')\n\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('召回率')\nplt.ylabel('精确率')\nplt.title('精确率-召回率曲线')\nplt.legend(loc='lower left')\nplt.grid(alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n# 6. 阈值与不同指标的关系分析\nprint(\"\\n=== 阈值分析 ===\")\n\n# 收集不同阈值下的各种指标\nthreshold_metrics = []\n\nfor i in range(len(thresholds)):\n    # 使用当前阈值生成预测\n    y_pred_threshold = (y_prob >= thresholds[i]).astype(int)\n    \n    # 计算指标\n    acc = accuracy_score(y_test, y_pred_threshold)\n    prec = precision_score(y_test, y_pred_threshold, zero_division=0)\n    rec = recall_score(y_test, y_pred_threshold)\n    f1 = f1_score(y_test, y_pred_threshold)\n    \n    # 记录结果\n    threshold_metrics.append({\n        '阈值': thresholds[i],\n        '准确率': acc,\n        '精确率': prec,\n        '召回率': rec,\n        'F1分数': f1,\n        '特异度': specificity[i]\n    })\n\n# 转换为DataFrame\nthreshold_df = pd.DataFrame(threshold_metrics)\n\n# 可视化阈值与各指标的关系\nplt.figure(figsize=(12, 8))\nplt.plot(threshold_df['阈值'], threshold_df['准确率'], marker='o', markersize=4, label='准确率')\nplt.plot(threshold_df['阈值'], threshold_df['精确率'], marker='s', markersize=4, label='精确率')\nplt.plot(threshold_df['阈值'], threshold_df['召回率'], marker='^', markersize=4, label='召回率')\nplt.plot(threshold_df['阈值'], threshold_df['F1分数'], marker='*', markersize=4, label='F1分数')\nplt.plot(threshold_df['阈值'], threshold_df['特异度'], marker='d', markersize=4, label='特异度')\n\nplt.axvline(x=thresholds[idx_max_youden], color='red', linestyle='--', label=f'最大Youden指数阈值 = {thresholds[idx_max_youden]:.2f}')\nplt.axvline(x=thresholds[idx_min_dist], color='green', linestyle='--', label=f'最佳平衡点阈值 = {thresholds[idx_min_dist]:.2f}')\n\nplt.xlabel('预测阈值')\nplt.ylabel('指标值')\nplt.title('不同阈值下的模型性能指标')\nplt.legend(loc='center right')\nplt.grid(alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n# 7. 校准曲线分析\nprint(\"\\n=== 概率校准分析 ===\")\n\n# 计算校准曲线\nprob_true, prob_pred = calibration_curve(y_test, y_prob, n_bins=10)\n\n# 可视化校准曲线\nplt.figure(figsize=(10, 8))\nplt.plot([0, 1], [0, 1], 'k--', label='完美校准')\nplt.plot(prob_pred, prob_true, 's-', label='模型校准曲线')\nplt.xlabel('预测概率')\nplt.ylabel('实际频率')\nplt.title('校准曲线 - 预测概率与实际频率的对比')\nplt.legend()\nplt.grid(alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n# 8. 交叉验证评估\nprint(\"\\n=== 交叉验证评估 ===\")\n\n# 定义交叉验证策略\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n# 执行交叉验证，针对不同评估指标\nscoring_metrics = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']\ncv_results = {}\n\nprint(\"执行5折交叉验证...\")\nfor metric in scoring_metrics:\n    cv_scores = cross_val_score(pipeline, X, y, cv=cv, scoring=metric)\n    cv_results[metric] = {\n        'mean': cv_scores.mean(),\n        'std': cv_scores.std(),\n        'scores': cv_scores\n    }\n    print(f\"{metric}: {cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")\n\n# 可视化交叉验证结果\nplt.figure(figsize=(12, 6))\nmean_scores = [cv_results[m]['mean'] for m in scoring_metrics]\nstd_scores = [cv_results[m]['std'] for m in scoring_metrics]\n\n# 创建条形图\nbars = plt.bar(scoring_metrics, mean_scores, yerr=std_scores, capsize=10, color='skyblue')\n\n# 在条形上添加具体数值\nfor i, bar in enumerate(bars):\n    height = bar.get_height()\n    plt.text(bar.get_x() + bar.get_width()/2., height + 0.01, f'{mean_scores[i]:.4f}', \n             ha='center', va='bottom', rotation=0, fontsize=9)\n\nplt.xlabel('评估指标')\nplt.ylabel('分数')\nplt.title('交叉验证评估结果')\nplt.ylim(0, 1.05)\nplt.grid(axis='y', alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n# 9. 特征贡献分析 (SHAP值)\nprint(\"\\n=== 特征贡献分析 ===\")\nprint(\"注: 完整的SHAP分析可能需要安装shap库，此处仅展示随机森林的特征重要性\")\n\n# 获取模型的特征重要性\nfeature_importance = pipeline.named_steps['classifier'].feature_importances_\n\n# 创建特征重要性数据框\nimportance_df = pd.DataFrame({\n    '特征': important_features,\n    '重要性': feature_importance\n})\nimportance_df = importance_df.sort_values('重要性', ascending=False)\n\n# 可视化特征重要性\nplt.figure(figsize=(12, 8))\nsns.barplot(x='重要性', y='特征', data=importance_df)\nplt.title('特征重要性分析')\nplt.xlabel('重要性')\nplt.tight_layout()\nplt.show()\n\n# 10. 结论与建议\nprint(\"\\n=== 模型验证结论 ===\")\n\nprint(\"基于模型验证的结论:\")\nprint(f\"1. 模型的整体性能: ROC AUC = {auc:.4f}, F1分数 = {f1:.4f}\")\n\n# 确定最佳阈值\nbest_f1_idx = threshold_df['F1分数'].idxmax()\nbest_f1_threshold = threshold_df.loc[best_f1_idx, '阈值']\n\n# Youden指数阈值\nyouden_threshold = thresholds[idx_max_youden]\n\nprint(f\"2. 最佳预测阈值:\")\nprint(f\"   - 基于F1分数: {best_f1_threshold:.4f}\")\nprint(f\"   - 基于Youden指数: {youden_threshold:.4f}\")\n\nprint(\"3. 模型优势:\")\nif sensitivity > 0.7:\n    print(f\"   - 较高的敏感度({sensitivity:.4f})，能够有效识别潜在流失客户\")\nif specificity > 0.7:\n    print(f\"   - 较高的特异度({specificity:.4f})，减少误报\")\nif auc > 0.8:\n    print(f\"   - 优秀的区分能力(AUC = {auc:.4f})\")\n\nprint(\"4. 模型不足:\")\nif sensitivity < 0.7:\n    print(f\"   - 敏感度较低({sensitivity:.4f})，可能漏检部分流失客户\")\nif specificity < 0.7:\n    print(f\"   - 特异度较低({specificity:.4f})，可能产生过多误报\")\nif np.abs(prob_true - prob_pred).mean() > 0.1:\n    print(\"   - 预测概率校准不足，需要进一步调整\")\n\nprint(\"\\n5. 建议:\")\nprint(\"   - 针对业务需求选择合适的预测阈值\")\nprint(\"   - 若优先考虑流失客户的召回，可选择较低阈值\")\nprint(\"   - 若优先考虑干预措施的精确性，可选择较高阈值\")\nprint(\"   - 在应用中结合客户价值等因素确定最终干预策略\")"
        },
        {
          "id": 6,
          "title": "客户挽留策略",
          "description": "基于模型结果设计针对性的客户挽留方案",
          "example_code": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nimport matplotlib as mpl\nfrom datetime import datetime, timedelta\nfrom app.services.docker_matplotlib_fix import configure_matplotlib_fonts\n\n# 配置字体以支持中文显示\nconfigure_matplotlib_fonts()\nfrom app.services.bank_analysis import bank_service\n\n# 从前面步骤加载预测结果和客户数据\n# 在实际应用中，需要加载前面步骤中的模型预测结果\n\n# 加载客户数据\ndf = bank_service.load_customer_churn_data()\nprint(f\"加载数据集，共{df.shape[0]}行，{df.shape[1]}列\")\n\n# 1. 创建模拟的流失风险预测结果\nprint(\"\\n=== 生成模拟的流失风险预测结果 ===\")\n\n# 假设我们有一个随机森林模型预测的流失概率\n# 这里我们模拟一个预测结果\nnp.random.seed(42)\ndf['churn_probability'] = np.clip(df['Churn'].astype(float) + np.random.normal(0, 0.2, size=len(df)), 0, 1)\n\n# 根据概率分配风险等级\ndf['risk_level'] = pd.cut(\n    df['churn_probability'], \n    bins=[0, 0.3, 0.6, 0.8, 1.0], \n    labels=['低风险', '中风险', '高风险', '极高风险']\n)\n\n# 查看风险分布情况\nrisk_counts = df['risk_level'].value_counts().sort_index()\nrisk_percentage = risk_counts / len(df) * 100\n\nprint(\"客户流失风险分布:\")\nfor level, count in risk_counts.items():\n    percentage = risk_percentage[level]\n    print(f\"{level}: {count}人 ({percentage:.2f}%)\")\n\n# 可视化风险分布\nplt.figure(figsize=(12, 6))\nsns.countplot(x='risk_level', data=df, palette='YlOrRd')\nplt.title('客户流失风险等级分布')\nplt.xlabel('风险等级')\nplt.ylabel('客户数量')\nplt.tight_layout()\nplt.show()\n\n# 2. 客户价值分析\nprint(\"\\n=== 客户价值分析 ===\")\n\n# 假设CustomerValue字段代表客户价值分数\nif 'CustomerValue' in df.columns:\n    # 将客户按价值划分为高、中、低三组\n    df['value_segment'] = pd.qcut(\n        df['CustomerValue'],\n        q=3,\n        labels=['低价值', '中价值', '高价值']\n    )\n    \n    # 查看价值分布情况\n    value_counts = df['value_segment'].value_counts().sort_index()\n    value_percentage = value_counts / len(df) * 100\n    \n    print(\"客户价值分布:\")\n    for level, count in value_counts.items():\n        percentage = value_percentage[level]\n        print(f\"{level}: {count}人 ({percentage:.2f}%)\")\n    \n    # 查看风险等级与价值等级的交叉分布\n    risk_value_cross = pd.crosstab(\n        df['risk_level'], \n        df['value_segment'], \n        normalize='all'*100\n    )\n    \n    print(\"\\n风险等级与价值等级交叉分布 (占总客户百分比%):\")\n    print(risk_value_cross)\n    \n    # 可视化风险-价值矩阵\n    plt.figure(figsize=(12, 8))\n    sns.heatmap(risk_value_cross, annot=True, fmt='.2f', cmap='YlGnBu', cbar_kws={'label': '占比(%)'}, vmin=0)\n    plt.title('客户风险-价值矩阵 (占总体客户百分比)')\n    plt.tight_layout()\n    plt.show()\nelse:\n    print(\"数据集中缺少客户价值信息\")\n    # 创建模拟的客户价值\n    df['CustomerValue'] = np.random.uniform(100, 10000, size=len(df))\n    df['value_segment'] = pd.qcut(\n        df['CustomerValue'],\n        q=3,\n        labels=['低价值', '中价值', '高价值']\n    )\n\n# 3. 客户细分与画像\nprint(\"\\n=== 客户细分与画像 ===\")\n\n# 选择用于聚类的特征\nclustering_features = ['Age', 'Tenure', 'CustomerValue', 'TransactionFrequency']\nfor feature in clustering_features:\n    if feature not in df.columns:\n        print(f\"警告: 特征 {feature} 不在数据集中\")\n\n# 使用可用的特征进行聚类\navailable_features = [f for f in clustering_features if f in df.columns]\nif not available_features:\n    # 如果没有可用特征，创建一些模拟特征\n    print(\"使用模拟特征进行客户细分\")\n    df['Age'] = np.random.randint(18, 80, size=len(df))\n    df['Tenure'] = np.random.randint(1, 20, size=len(df))\n    df['TransactionFrequency'] = np.random.randint(1, 50, size=len(df))\n    available_features = ['Age', 'Tenure', 'CustomerValue', 'TransactionFrequency']\n\n# 准备用于聚类的数据\nX_cluster = df[available_features].copy()\n\n# 标准化特征\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X_cluster)\n\n# 使用K-means进行聚类 (假设分为4个客户群体)\nkmeans = KMeans(n_clusters=4, random_state=42, n_init=10)\ndf['cluster'] = kmeans.fit_predict(X_scaled)\n\n# 分析各客户群体的特征\ncluster_summary = df.groupby('cluster').agg({\n    'churn_probability': 'mean',\n    'CustomerValue': 'mean',\n    'risk_level': lambda x: x.value_counts().index[0],  # 最常见的风险等级\n    'value_segment': lambda x: x.value_counts().index[0],  # 最常见的价值等级\n})\n\n# 添加其他可用的特征到摘要中\nfor feature in available_features:\n    if feature != 'CustomerValue':  # 已经包含了\n        cluster_summary[feature] = df.groupby('cluster')[feature].mean()\n\n# 为每个群体定义一个标签\ncluster_labels = {\n    0: '群体A',\n    1: '群体B', \n    2: '群体C',\n    3: '群体D'\n}\ncluster_summary['群体标签'] = [cluster_labels[i] for i in cluster_summary.index]\nprint(\"客户群体特征摘要:\")\nprint(cluster_summary)\n\n# 可视化客户群体\n# 使用PCA降维以便可视化\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X_scaled)\n\n# 添加降维结果到数据框\ndf['PCA1'] = X_pca[:, 0]\ndf['PCA2'] = X_pca[:, 1]\n\n# 可视化聚类结果\nplt.figure(figsize=(14, 10))\n\n# 散点图\nplt.subplot(2, 2, 1)\nsns.scatterplot(x='PCA1', y='PCA2', hue='cluster', data=df, palette='viridis', s=50, alpha=0.7)\nplt.title('客户群体聚类可视化 (PCA降维)')\nplt.legend(title='客户群体')\n\n# 风险等级分布\nplt.subplot(2, 2, 2)\nrisk_by_cluster = pd.crosstab(df['cluster'], df['risk_level'])\nrisk_by_cluster.plot(kind='bar', stacked=True, colormap='YlOrRd', ax=plt.gca())\nplt.title('各客户群体的风险等级分布')\nplt.xlabel('客户群体')\nplt.ylabel('客户数量')\nplt.legend(title='风险等级')\n\n# 客户价值分布\nplt.subplot(2, 2, 3)\nvalue_by_cluster = pd.crosstab(df['cluster'], df['value_segment'])\nvalue_by_cluster.plot(kind='bar', stacked=True, colormap='YlGnBu', ax=plt.gca())\nplt.title('各客户群体的价值等级分布')\nplt.xlabel('客户群体')\nplt.ylabel('客户数量')\nplt.legend(title='价值等级')\n\n# 流失概率分布\nplt.subplot(2, 2, 4)\nsns.boxplot(x='cluster', y='churn_probability', data=df, palette='viridis')\nplt.title('各客户群体的流失概率分布')\nplt.xlabel('客户群体')\nplt.ylabel('流失概率')\n\nplt.tight_layout()\nplt.show()\n\n# 4. 设计针对性的挽留策略矩阵\nprint(\"\\n=== 客户挽留策略矩阵 ===\")\n\n# 定义基于风险和价值的挽留策略矩阵\nretention_matrix = pd.DataFrame(\n    index=['低风险', '中风险', '高风险', '极高风险'],\n    columns=['低价值', '中价值', '高价值']\n)\n\n# 填充策略矩阵\nretention_matrix.loc['低风险', '低价值'] = \"常规服务维护\"\nretention_matrix.loc['低风险', '中价值'] = \"定期回访+产品推荐\"\nretention_matrix.loc['低风险', '高价值'] = \"VIP专属服务+增值服务\"\n\nretention_matrix.loc['中风险', '低价值'] = \"电子邮件活动+优惠券\"\nretention_matrix.loc['中风险', '中价值'] = \"个性化推荐+专属优惠\"\nretention_matrix.loc['中风险', '高价值'] = \"客户经理专访+金融咨询\"\n\nretention_matrix.loc['高风险', '低价值'] = \"短信提醒+小额优惠\"\nretention_matrix.loc['高风险', '中价值'] = \"电话回访+专属优惠包\"\nretention_matrix.loc['高风险', '高价值'] = \"1对1沟通+专属解决方案\"\n\nretention_matrix.loc['极高风险', '低价值'] = \"退出调查+社交媒体互动\"\nretention_matrix.loc['极高风险', '中价值'] = \"紧急挽留方案+产品升级\"\nretention_matrix.loc['极高风险', '高价值'] = \"总监级拜访+定制化挽留计划\"\n\nprint(\"客户挽留策略矩阵:\")\nprint(retention_matrix)\n\n# 可视化策略矩阵\nplt.figure(figsize=(12, 8))\nax = plt.subplot(111)\n\n# 创建表格\ntable = ax.table(\n    cellText=retention_matrix.values,\n    rowLabels=retention_matrix.index,\n    colLabels=retention_matrix.columns,\n    cellLoc='center',\n    rowLoc='center',\n    loc='center'\n)\n\n# 设置表格样式\ntable.auto_set_font_size(False)\ntable.set_fontsize(10)\ntable.scale(1, 2)\n\n# 设置表格颜色\ncmap = plt.cm.get_cmap('YlGnBu')\n\nfor i in range(len(retention_matrix.index)):\n    for j in range(len(retention_matrix.columns)):\n        # 根据位置设置颜色深浅\n        intensity = (i+1) * (j+1) / ((len(retention_matrix.index)) * (len(retention_matrix.columns)))\n        table[(i+1, j)].set_facecolor(cmap(intensity))\n\nax.set_title('客户挽留策略矩阵', fontsize=16)\nax.axis('off')\nplt.tight_layout()\nplt.show()\n\n# 5. 针对不同客户群体的挽留策略\nprint(\"\\n=== 客户群体挽留策略 ===\")\n\n# 定义各客户群体的挽留策略\ncluster_strategies = {\n    0: {\n        'name': cluster_labels[0],\n        'description': '以下策略针对' + cluster_labels[0] + '客户群体',\n        'strategies': [\n            \"策略1: 提供便捷的数字化服务体验\",\n            \"策略2: 简化产品使用流程，降低使用门槛\",\n            \"策略3: 提供专属的教育内容和使用指南\",\n            \"策略4: 设计阶梯式成长奖励计划\"\n        ]\n    },\n    1: {\n        'name': cluster_labels[1],\n        'description': '以下策略针对' + cluster_labels[1] + '客户群体',\n        'strategies': [\n            \"策略1: 开发专属的增值服务包\",\n            \"策略2: 提供个性化的财务顾问服务\",\n            \"策略3: 建立会员积分兑换制度\",\n            \"策略4: 定期举办客户交流活动\"\n        ]\n    },\n    2: {\n        'name': cluster_labels[2],\n        'description': '以下策略针对' + cluster_labels[2] + '客户群体',\n        'strategies': [\n            \"策略1: 定制专属的高端服务方案\",\n            \"策略2: 提供一站式财富管理服务\",\n            \"策略3: 邀请参与新产品开发意见征集\",\n            \"策略4: 建立专属客户经理对接机制\"\n        ]\n    },\n    3: {\n        'name': cluster_labels[3],\n        'description': '以下策略针对' + cluster_labels[3] + '客户群体',\n        'strategies': [\n            \"策略1: 开展特定主题的金融知识讲座\",\n            \"策略2: 提供针对性的产品组合推荐\",\n            \"策略3: 优化服务流程，提升响应速度\",\n            \"策略4: 建立客户反馈闭环处理机制\"\n        ]\n    }\n}\n\n# 打印每个群体的策略\nfor cluster_id, strategy in cluster_strategies.items():\n    print(f\"\\n{strategy['name']} ({strategy['description']})\")\n    avg_prob = df[df['cluster'] == cluster_id]['churn_probability'].mean()\n    print(f\"平均流失概率: {avg_prob:.4f}\")\n    \n    # 获取该群体的主要风险等级和价值等级\n    main_risk = df[df['cluster'] == cluster_id]['risk_level'].mode()[0]\n    main_value = df[df['cluster'] == cluster_id]['value_segment'].mode()[0]\n    print(f\"主要风险等级: {main_risk}, 主要价值等级: {main_value}\")\n    \n    print(\"挽留策略:\")\n    for s in strategy['strategies']:\n        print(f\"- {s}\")\n\n# 6. 挽留策略的实施计划\nprint(\"\\n=== 挽留策略实施计划 ===\")\n\n# 示例的策略实施计划时间表\ntoday = datetime.now()\nstart_date = today + timedelta(days=7)  # 从下周开始\n\n# 定义实施时间表\nimplementation_plan = [\n    {\n        'phase': '第一阶段: 紧急挽留',\n        'start_date': start_date,\n        'end_date': start_date + timedelta(days=30),\n        'target': '极高风险客户 (所有价值等级)',\n        'actions': [\n            \"1. 建立紧急挽留响应团队\",\n            \"2. 设计针对极高风险客户的挽留话术和优惠方案\",\n            \"3. 实施1对1沟通和专属解决方案\",\n            \"4. 跟踪并分析客户反馈和挽留效果\"\n        ]\n    },\n    {\n        'phase': '第二阶段: 高风险干预',\n        'start_date': start_date + timedelta(days=15),\n        'end_date': start_date + timedelta(days=60),\n        'target': '高风险客户 (中高价值)',\n        'actions': [\n            \"1. 客户经理主动联系和回访\",\n            \"2. 提供定制化的产品优化方案\",\n            \"3. 针对性解决客户痛点问题\",\n            \"4. 建立客户关系强化计划\"\n        ]\n    },\n    {\n        'phase': '第三阶段: 中风险预防',\n        'start_date': start_date + timedelta(days=30),\n        'end_date': start_date + timedelta(days=90),\n        'target': '中风险客户 (所有价值等级)',\n        'actions': [\n            \"1. 推出客户忠诚度提升活动\",\n            \"2. 开展产品使用教育和培训\",\n            \"3. 针对不同客户群体开展个性化营销\",\n            \"4. 完善客户服务体验流程\"\n        ]\n    },\n    {\n        'phase': '第四阶段: 低风险维护',\n        'start_date': start_date + timedelta(days=45),\n        'end_date': start_date + timedelta(days=120),\n        'target': '低风险客户 (中高价值)',\n        'actions': [\n            \"1. 定期客户满意度调查\",\n            \"2. 提供增值服务和专属福利\",\n            \"3. 建立长期客户关系维护机制\",\n            \"4. 发展交叉销售和向上销售策略\"\n        ]\n    }\n]\n\n# 打印实施计划\nfor phase in implementation_plan:\n    print(f\"\\n{phase['phase']}\")\n    print(f\"实施时间: {phase['start_date'].strftime('%Y-%m-%d')} 至 {phase['end_date'].strftime('%Y-%m-%d')}\")\n    print(f\"目标客户: {phase['target']}\")\n    print(\"主要行动:\")\n    for action in phase['actions']:\n        print(f\"- {action}\")\n\n# 7. 挽留效果评估指标设计\nprint(\"\\n=== 挽留效果评估指标 ===\")\n\n# 定义评估指标\nevaluation_metrics = {\n    '流失率指标': [\n        {'name': '整体流失率', 'description': '全行客户的整体流失率变化'},\n        {'name': '目标群体流失率', 'description': '各干预群体的流失率变化'},\n        {'name': '高价值客户流失率', 'description': '高价值客户群体的流失率变化'},\n        {'name': '新增流失率', 'description': '新增客户的流失率'}\n    ],\n    '客户行为指标': [\n        {'name': '交易频率', 'description': '客户交易频率的变化'},\n        {'name': '交易金额', 'description': '客户交易金额的变化'},\n        {'name': '产品持有数', 'description': '客户持有产品数量的变化'},\n        {'name': '渠道活跃度', 'description': '客户在各渠道的活跃程度变化'}\n    ],\n    '客户关系指标': [\n        {'name': '客户满意度', 'description': 'NPS或满意度调查结果'},\n        {'name': '投诉率', 'description': '客户投诉率的变化'},\n        {'name': '问题解决率', 'description': '客户问题的解决效率和质量'},\n        {'name': '客户互动率', 'description': '客户对营销活动的响应率'}\n    ],\n    '财务绩效指标': [\n        {'name': '挽留成本', 'description': '每挽留一个客户的平均成本'},\n        {'name': '挽留ROI', 'description': '挽留投资的回报率'},\n        {'name': '客户终身价值', 'description': '被挽留客户的预期终身价值变化'},\n        {'name': '交叉销售收入', 'description': '通过挽留活动产生的额外产品销售收入'}\n    ]\n}\n\n# 打印评估指标\nfor category, metrics in evaluation_metrics.items():\n    print(f\"\\n{category}:\")\n    for metric in metrics:\n        print(f\"- {metric['name']}: {metric['description']}\")\n\n# 8. 模拟挽留效果\nprint(\"\\n=== 挽留效果模拟预测 ===\")\n\n# 模拟不同挽留策略的效果\nretention_scenarios = {\n    '无干预': {'retention_rate': 0.0, 'cost_per_customer': 0},\n    '基础干预': {'retention_rate': 0.2, 'cost_per_customer': 200},\n    '标准干预': {'retention_rate': 0.4, 'cost_per_customer': 500},\n    '强化干预': {'retention_rate': 0.6, 'cost_per_customer': 1000},\n    '全方位干预': {'retention_rate': 0.7, 'cost_per_customer': 2000}\n}\n\n# 假设的客户价值和挽留成本\navg_customer_value = 5000  # 平均客户年价值\n\n# 高风险及以上客户数量\nhigh_risk_customers = df[df['risk_level'].isin(['高风险', '极高风险'])].shape[0]\n\n# 计算不同情景下的效果\nresults = []\nfor scenario, params in retention_scenarios.items():\n    # 被挽留的客户数\n    retained_customers = high_risk_customers * params['retention_rate']\n    \n    # 挽留总成本\n    total_cost = high_risk_customers * params['cost_per_customer']\n    \n    # 挽留带来的价值\n    retained_value = retained_customers * avg_customer_value\n    \n    # 净收益\n    net_benefit = retained_value - total_cost\n    \n    # ROI\n    roi = (net_benefit / total_cost) if total_cost > 0 else float('inf')\n    \n    results.append({\n        '情景': scenario,\n        '挽留率': params['retention_rate'] * 100,\n        '挽留客户数': retained_customers,\n        '总成本(元)': total_cost,\n        '挽留价值(元)': retained_value,\n        '净收益(元)': net_benefit,\n        'ROI': roi\n    })\n\n# 转换为DataFrame\nresults_df = pd.DataFrame(results)\nprint(\"不同挽留情景的效果比较:\")\nprint(results_df[['情景', '挽留率', '挽留客户数', '总成本(元)', '挽留价值(元)', '净收益(元)', 'ROI']])\n\n# 可视化挽留效果\nplt.figure(figsize=(14, 8))\n\n# 净收益比较\nplt.subplot(1, 2, 1)\nsns.barplot(x='情景', y='净收益(元)', data=results_df, palette='viridis')\nplt.title('不同情景下的净收益比较')\nplt.xticks(rotation=45)\nplt.ylabel('净收益(元)')\n\n# ROI比较\nplt.subplot(1, 2, 2)\nsns.barplot(x='情景', y='ROI', data=results_df[results_df['情景'] != '无干预'], palette='viridis')  # 排除无干预情景，避免无穷大ROI\nplt.title('不同情景下的ROI比较')\nplt.xticks(rotation=45)\nplt.ylabel('ROI(倍)')\n\nplt.tight_layout()\nplt.show()\n\n# 9. 长期客户关系管理建议\nprint(\"\\n=== 长期客户关系管理建议 ===\")\n\nlong_term_recommendations = [\n    {\n        'category': '客户洞察与理解',\n        'recommendations': [\n            \"1. 建立客户360度视图，整合多渠道数据\",\n            \"2. 定期进行客户需求调研和满意度调查\",\n            \"3. 利用高级分析技术预测客户生命周期变化\",\n            \"4. 建立客户行为模式库，支持精准营销\"\n        ]\n    },\n    {\n        'category': '产品与服务优化',\n        'recommendations': [\n            \"1. 基于客户反馈持续优化产品功能与体验\",\n            \"2. 开发针对不同客户群体的差异化产品\",\n            \"3. 简化服务流程，提升客户便捷性\",\n            \"4. 建立产品使用辅导和教育机制\"\n        ]\n    },\n    {\n        'category': '沟通与互动策略',\n        'recommendations': [\n            \"1. 建立多渠道、个性化的客户沟通机制\",\n            \"2. 优化触点管理，确保一致的品牌体验\",\n            \"3. 发展基于价值的内容营销策略\",\n            \"4. 建立客户社区，促进客户间交流与分享\"\n        ]\n    },\n    {\n        'category': '组织能力建设',\n        'recommendations': [\n            \"1. 建立以客户为中心的文化和考核机制\",\n            \"2. 提升员工客户服务技能和意识\",\n            \"3. 优化数据和技术基础设施\",\n            \"4. 建立客户体验管理团队和流程\"\n        ]\n    }\n]\n\n# 打印长期建议\nfor category in long_term_recommendations:\n    print(f\"\\n{category['category']}:\")\n    for recommendation in category['recommendations']:\n        print(f\"- {recommendation}\")\n\n# 10. 挽留策略的总结\nprint(\"\\n=== 客户挽留策略总结 ===\")\nprint(\"基于预警模型的客户挽留策略框架:\")\nprint(\"1. 通过机器学习预警模型，及时识别有流失风险的客户\")\nprint(\"2. 结合客户价值，制定差异化的挽留优先级矩阵\")\nprint(\"3. 针对不同客户群体，设计个性化的挽留措施\")\nprint(\"4. 分阶段实施挽留计划，优先处理高价值高风险客户\")\nprint(\"5. 建立科学的评估指标体系，持续监控挽留效果\")\nprint(\"6. 将短期挽留措施与长期客户关系管理相结合\")\nprint(\"7. 通过数据驱动决策，持续优化挽留策略和投入\")\n\nprint(\"\\n挽留策略成功的关键因素:\")\nprint(\"1. 精准的流失风险预测能力\")\nprint(\"2. 客户价值与风险的综合评估\")\nprint(\"3. 个性化的挽留方案设计\")\nprint(\"4. 多部门协作的实施机制\")\nprint(\"5. 科学的效果评估和持续优化\")\nprint(\"6. 高管层的支持与资源投入\")\n\nprint(\"\\n通过实施科学的客户挽留策略，银行可以有效降低客户流失率，提升客户忠诚度和终身价值，增强市场竞争力。\")"
        }
      ]
    },
    {
      "id": 3,
      "title": "银行信用欺诈数据分析",
      "description": "分析并检测银行信用卡交易中的欺诈行为",
      "category": "bank",
      "difficulty_level": 3,
      "estimated_duration": 130,
      "prerequisites": "数据挖掘和异常检测基础知识",
      "data_source": "银行信用卡交易数据",
      "steps": [
        {
          "id": 1,
          "title": "欺诈类型分析",
          "description": "了解不同类型的银行信用欺诈手段和特征",
          "example_code": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime, timedelta\nimport matplotlib as mpl\nfrom app.services.docker_matplotlib_fix import configure_matplotlib_fonts\n\n# 配置字体以支持中文显示\nconfigure_matplotlib_fonts()\nfrom app.services.bank_analysis import bank_service\n\n# 加载信用卡欺诈数据\ndf = bank_service.load_credit_fraud_data()\nprint(f\"加载数据集，共{df.shape[0]}行，{df.shape[1]}列\")\n\n# 1. 基本数据探索\nprint(\"\\n=== 基本数据探索 ===\")\n\n# 查看数据的基本信息\nprint(\"\\n数据集的基本信息:\")\nprint(df.info())\n\n# 查看数据的统计摘要\nprint(\"\\n数据的统计摘要:\")\nprint(df.describe())\n\n# 检查是否有缺失值\nprint(\"\\n缺失值统计:\")\nmissing_values = df.isnull().sum()\nprint(missing_values[missing_values > 0])\n\n# 查看目标变量分布\nprint(\"\\n欺诈与非欺诈交易比例:\")\nfraud_counts = df['Class'].value_counts()\nfraud_percentage = fraud_counts / len(df) * 100\n\nfor label, count in fraud_counts.items():\n    percentage = fraud_percentage[label]\n    is_fraud = \"欺诈\" if label == 1 else \"正常\"\n    print(f\"{is_fraud}交易: {count}笔 ({percentage:.4f}%)\")\n\n# 可视化欺诈与非欺诈交易比例\nplt.figure(figsize=(10, 6))\nsns.countplot(x='Class', data=df, palette=['#2ecc71', '#e74c3c'])\nplt.title('欺诈与非欺诈交易数量比较')\nplt.xlabel('交易类型')\nplt.ylabel('交易数量')\nplt.xticks([0, 1], ['正常交易', '欺诈交易'])\n\n# 添加文本标签\nfor i, count in enumerate(fraud_counts):\n    percentage = fraud_percentage[i if i in fraud_percentage.index else fraud_counts.index[i]]\n    plt.text(i, count + 100, f'{count}\\n({percentage:.4f}%)', ha='center')\n\nplt.tight_layout()\nplt.show()\n\n# 2. 欺诈交易金额分析\nprint(\"\\n=== 欺诈交易金额分析 ===\")\n\n# 分析正常和欺诈交易的金额分布\nprint(\"\\n正常交易金额统计:\")\nprint(df[df['Class'] == 0]['Amount'].describe())\n\nprint(\"\\n欺诈交易金额统计:\")\nprint(df[df['Class'] == 1]['Amount'].describe())\n\n# 可视化正常和欺诈交易金额分布\nplt.figure(figsize=(14, 6))\n\n# 正常交易金额分布\nplt.subplot(1, 2, 1)\nsns.histplot(df[df['Class'] == 0]['Amount'], bins=50, kde=True, color='#2ecc71')\nplt.title('正常交易金额分布')\nplt.xlabel('交易金额')\nplt.ylabel('交易次数')\n\n# 欺诈交易金额分布\nplt.subplot(1, 2, 2)\nsns.histplot(df[df['Class'] == 1]['Amount'], bins=50, kde=True, color='#e74c3c')\nplt.title('欺诈交易金额分布')\nplt.xlabel('交易金额')\nplt.ylabel('交易次数')\n\nplt.tight_layout()\nplt.show()\n\n# 使用箱线图比较欺诈和正常交易的金额差异\nplt.figure(figsize=(10, 6))\nsns.boxplot(x='Class', y='Amount', data=df, palette=['#2ecc71', '#e74c3c'])\nplt.title('欺诈与正常交易金额箱线图比较')\nplt.xlabel('交易类型')\nplt.ylabel('交易金额')\nplt.xticks([0, 1], ['正常交易', '欺诈交易'])\nplt.tight_layout()\nplt.show()\n\n# 3. 欺诈交易时间模式分析\nprint(\"\\n=== 欺诈交易时间模式分析 ===\")\n\n# 假设数据中有Time字段表示时间（以秒为单位）\nif 'Time' in df.columns:\n    # 将Time转换为小时\n    df['Hour'] = (df['Time'] / 3600) % 24\n    \n    # 分析欺诈交易在不同时间段的分布\n    plt.figure(figsize=(14, 6))\n    \n    # 欺诈交易的小时分布\n    plt.subplot(1, 2, 1)\n    sns.histplot(df[df['Class'] == 1]['Hour'], bins=24, kde=True, color='#e74c3c')\n    plt.title('欺诈交易的小时分布')\n    plt.xlabel('小时')\n    plt.ylabel('交易次数')\n    plt.xticks(range(0, 24, 2))\n    \n    # 正常交易的小时分布\n    plt.subplot(1, 2, 2)\n    sns.histplot(df[df['Class'] == 0]['Hour'], bins=24, kde=True, color='#2ecc71')\n    plt.title('正常交易的小时分布')\n    plt.xlabel('小时')\n    plt.ylabel('交易次数')\n    plt.xticks(range(0, 24, 2))\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # 分析各小时欺诈率\n    hour_fraud_rate = df.groupby('Hour')['Class'].mean() * 100\n    \n    plt.figure(figsize=(12, 6))\n    hour_fraud_rate.plot(kind='line', marker='o', color='#3498db')\n    plt.title('各小时的欺诈率')\n    plt.xlabel('小时')\n    plt.ylabel('欺诈率 (%)')\n    plt.xticks(range(0, 24, 2))\n    plt.grid(True, alpha=0.3)\n    plt.tight_layout()\n    plt.show()\n    \n# 4. 欺诈类型聚类分析\nprint(\"\\n=== 欺诈类型聚类分析 ===\")\n\n# 使用特征变量进行欺诈交易的聚类分析\nfraud_data = df[df['Class'] == 1].copy()\n\n# 选择特征变量（不包括Class和Amount）\nfeature_columns = [col for col in df.columns if col.startswith('V')]\n\n# 如果数据量较大，可以随机抽样\nif len(fraud_data) > 1000:\n    fraud_data = fraud_data.sample(1000, random_state=42)\n\n# 标准化特征\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nfraud_data_scaled = scaler.fit_transform(fraud_data[feature_columns])\n\n# 使用t-SNE进行降维可视化\nfrom sklearn.manifold import TSNE\ntsne = TSNE(n_components=2, random_state=42, perplexity=30)\nfraud_data_tsne = tsne.fit_transform(fraud_data_scaled)\n\n# 使用K-means进行聚类\nfrom sklearn.cluster import KMeans\n\n# 确定最佳聚类数（用肘部法则）\ndistortions = []\nK = range(1, 10)\nfor k in K:\n    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n    kmeans.fit(fraud_data_scaled)\n    distortions.append(kmeans.inertia_)\n\n# 可视化肘部法则\nplt.figure(figsize=(10, 6))\nplt.plot(K, distortions, 'bx-')\nplt.xlabel('聚类数量k')\nplt.ylabel('畸变')\nplt.title('肘部法则确定最佳聚类数')\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n# 假设最佳聚类数为4\nbest_k = 4  # 根据肘部法则确定\nkmeans = KMeans(n_clusters=best_k, random_state=42, n_init=10)\nfraud_clusters = kmeans.fit_predict(fraud_data_scaled)\n\n# 添加聚类标签到数据\nfraud_data['Cluster'] = fraud_clusters\n\n# 可视化聚类结果\nplt.figure(figsize=(12, 10))\n\n# t-SNE可视化\nplt.subplot(2, 2, 1)\nplt.scatter(fraud_data_tsne[:, 0], fraud_data_tsne[:, 1], c=fraud_clusters, \n            cmap='viridis', s=50, alpha=0.8)\nplt.title('欺诈交易的聚类分析 (t-SNE降维)')\nplt.xlabel('t-SNE维度1')\nplt.ylabel('t-SNE维度2')\nplt.colorbar(label='聚类')\n\n# 各聚类的交易金额分布\nplt.subplot(2, 2, 2)\nsns.boxplot(x='Cluster', y='Amount', data=fraud_data, palette='viridis')\nplt.title('各欺诈类型的交易金额分布')\nplt.xlabel('欺诈类型聚类')\nplt.ylabel('交易金额')\n\n# 如果有Time字段，分析各聚类的时间分布\nif 'Hour' in fraud_data.columns:\n    plt.subplot(2, 2, 3)\n    sns.boxplot(x='Cluster', y='Hour', data=fraud_data, palette='viridis')\n    plt.title('各欺诈类型的时间分布')\n    plt.xlabel('欺诈类型聚类')\n    plt.ylabel('小时')\n\n# 各聚类的大小\nplt.subplot(2, 2, 4)\ncluster_counts = fraud_data['Cluster'].value_counts().sort_index()\nplt.bar(cluster_counts.index, cluster_counts.values, color='viridis')\nplt.title('各欺诈类型的数量分布')\nplt.xlabel('欺诈类型聚类')\nplt.ylabel('交易数量')\nplt.xticks(range(best_k))\n\n# 在柱状图上添加数值标签\nfor i, count in enumerate(cluster_counts):\n    plt.text(i, count + 5, str(count), ha='center')\n\nplt.tight_layout()\nplt.show()\n\n# 5. 各欺诈类型的特征分析\nprint(\"\\n=== 各欺诈类型的特征分析 ===\")\n\n# 分析各聚类的特征均值\ncluster_features = []\nfor cluster in range(best_k):\n    cluster_mean = fraud_data[fraud_data['Cluster'] == cluster][feature_columns].mean()\n    cluster_features.append(cluster_mean)\n\ncluster_features_df = pd.DataFrame(cluster_features)\n\n# 计算全局特征均值（非欺诈交易）\nnon_fraud_mean = df[df['Class'] == 0][feature_columns].mean()\n\n# 分析每个聚类的特征偏离情况\nfor cluster in range(best_k):\n    # 计算与非欺诈交易的特征差异\n    feature_diff = cluster_features_df.iloc[cluster] - non_fraud_mean\n    \n    # 找出差异最大的10个特征\n    top_features = feature_diff.abs().nlargest(10).index.tolist()\n    \n    print(f\"\\n欺诈类型{cluster}的特征分析:\")\n    print(f\"该类型共有{cluster_counts[cluster]}笔交易\")\n    print(\"与正常交易差异最大的特征:\")\n    \n    for feature in top_features:\n        diff_value = feature_diff[feature]\n        diff_dir = \"高于\" if diff_value > 0 else \"低于\"\n        print(f\"{feature}: {diff_dir}正常交易 {abs(diff_value):.4f}单位\")\n\n# 可视化特征重要性\nplt.figure(figsize=(14, 10))\n\n# 为每个聚类绘制重要特征\nfor cluster in range(min(4, best_k)):  # 最多显示4个聚类\n    plt.subplot(2, 2, cluster + 1)\n    \n    # 计算与非欺诈交易的特征差异\n    feature_diff = cluster_features_df.iloc[cluster] - non_fraud_mean\n    \n    # 找出差异最大的10个特征\n    top_features = feature_diff.abs().nlargest(10)\n    \n    # 创建条形图\n    colors = ['#3498db' if val > 0 else '#e74c3c' for val in top_features]\n    plt.barh(top_features.index, top_features.values, color=colors)\n    plt.title(f'欺诈类型{cluster}的特征重要性')\n    plt.xlabel('与正常交易的差异')\n    plt.grid(True, axis='x', alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# 6. 欺诈类型总结\nprint(\"\\n=== 欺诈类型总结 ===\")\n\n# 根据分析结果，总结各种欺诈类型的特点\nfraud_types = [\n    {\n        \"type\": \"金额小、高频率欺诈\",\n        \"characteristics\": [\n            \"交易金额通常较小\",\n            \"在短时间内可能有多次交易\",\n            \"通常发生在特定的时间段\",\n            \"主要针对商业或零售账户\"\n        ],\n        \"examples\": [\n            \"测试被盗信用卡有效性的小额交易\",\n            \"自动化欺诈工具批量处理的交易\",\n            \"通过多次小额交易规避风控系统\"\n        ]\n    },\n    {\n        \"type\": \"金额大、低频率欺诈\",\n        \"characteristics\": [\n            \"交易金额通常较大\",\n            \"交易频率较低\",\n            \"可能针对高价值商品或服务\",\n            \"通常在被发现之前只有几次交易\"\n        ],\n        \"examples\": [\n            \"购买高价值电子产品或奢侈品\",\n            \"大额现金提取或转账\",\n            \"购买可快速转售的商品\"\n        ]\n    },\n    {\n        \"type\": \"账户接管型欺诈\",\n        \"characteristics\": [\n            \"交易模式与用户历史模式显著不同\",\n            \"可能先有小额测试交易，然后是大额交易\",\n            \"交易地点或商户类型异常\",\n            \"短时间内多种类型交易\"\n        ],\n        \"examples\": [\n            \"通过网络钓鱼获取账户后的欺诈交易\",\n            \"身份盗窃后的非授权使用\",\n            \"账户信息泄露后的欺诈行为\"\n        ]\n    },\n    {\n        \"type\": \"商户欺诈\",\n        \"characteristics\": [\n            \"通常涉及特定商户或商户类型\",\n            \"交易金额可能各不相同\",\n            \"可能有不规则的交易模式\",\n            \"可能涉及虚假退款或重复收费\"\n        ],\n        \"examples\": [\n            \"虚假商户进行的刷卡套现\",\n            \"商户与犯罪分子共谋的欺诈行为\",\n            \"POS终端被篡改后的欺诈交易\"\n        ]\n    }\n]\n\n# 打印欺诈类型总结\nfor i, fraud_type in enumerate(fraud_types):\n    print(f\"\\n欺诈类型{i+1}: {fraud_type['type']}\")\n    print(\"特征:\")\n    for char in fraud_type['characteristics']:\n        print(f\"- {char}\")\n    \n    print(\"\\n典型案例:\")\n    for example in fraud_type['examples']:\n        print(f\"- {example}\")\n    \n    print(\"\\n防范建议:\")\n    if i == 0:  # 金额小、高频率欺诈\n        print(\"- 设置短时间内小额交易次数限制\")\n        print(\"- 对多次小额交易进行风险聚合评估\")\n        print(\"- 建立小额高频交易的行为模型\")\n    elif i == 1:  # 金额大、低频率欺诈\n        print(\"- 对大额交易实施强验证机制\")\n        print(\"- 基于消费者行为分析大额交易风险\")\n        print(\"- 设置基于风险的交易限额\")\n    elif i == 2:  # 账户接管型欺诈\n        print(\"- 建立用户行为基线，检测异常活动\")\n        print(\"- 对异地或异常时间的交易添加验证步骤\")\n        print(\"- 使用多因素认证保护账户安全\")\n    else:  # 商户欺诈\n        print(\"- 对商户进行严格的尽职调查和监控\")\n        print(\"- 分析商户级别的交易模式异常\")\n        print(\"- 建立商户风险评分机制\")\n\n# 7. 反欺诈策略建议\nprint(\"\\n=== 反欺诈策略建议 ===\")\n\n# 分层防御策略\ndefense_layers = {\n    \"数据层\": [\n        \"1. 建立全面的数据收集机制，包括交易、设备、行为等信息\",\n        \"2. 实施实时数据质量检查，确保数据完整性\",\n        \"3. 整合内部和外部数据源，提高欺诈检测能力\"\n    ],\n    \"分析层\": [\n        \"1. 部署规则引擎，实施基本的欺诈检测规则\",\n        \"2. 应用高级分析技术，如机器学习模型检测复杂欺诈模式\",\n        \"3. 建立异常检测系统，识别偏离正常行为的交易\"\n    ],\n    \"操作层\": [\n        \"1. 实施实时交易监控和预警系统\",\n        \"2. 建立分级响应机制，根据风险级别采取相应措施\",\n        \"3. 配置自适应认证策略，对高风险交易增加验证步骤\"\n    ],\n    \"优化层\": [\n        \"1. 定期评估欺诈模式变化和策略有效性\",\n        \"2. 持续优化模型和规则，适应新出现的欺诈手段\",\n        \"3. 建立欺诈案例库，促进知识共享和学习\"\n    ]\n}\n\n# 打印反欺诈策略建议\nfor layer, strategies in defense_layers.items():\n    print(f\"\\n{layer}:\")\n    for strategy in strategies:\n        print(f\"- {strategy}\")\n\n# 总结\nprint(\"\\n=== 总结 ===\")\nprint(\"信用卡欺诈分析的关键发现:\")\nprint(\"1. 欺诈交易在整体交易中占比很小，数据极不平衡\")\nprint(\"2. 欺诈交易的金额分布与正常交易存在显著差异\")\nprint(\"3. 欺诈交易在某些时间段可能更为集中\")\nprint(\"4. 通过聚类分析，可以识别多种不同类型的欺诈模式\")\nprint(\"5. 不同类型的欺诈具有不同的特征表现和处理策略\")\n\nprint(\"\\n下一步建议:\")\nprint(\"1. 基于识别的欺诈类型，设计有针对性的特征工程\")\nprint(\"2. 开发专门针对各类欺诈行为的检测模型\")\nprint(\"3. 实施多层次防御策略，提高欺诈检测效果\")\nprint(\"4. 建立实时监控系统，及时发现和响应欺诈活动\")"
        },
        {
          "id": 2,
          "title": "交易数据处理",
          "description": "清洗和准备交易数据，处理时间特征",
          "example_code": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime, timedelta\nimport matplotlib as mpl\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\nfrom app.services.docker_matplotlib_fix import configure_matplotlib_fonts\n\n# 配置字体以支持中文显示\nconfigure_matplotlib_fonts()\nfrom app.services.bank_analysis import bank_service\n\n# 加载信用卡欺诈数据\ndf = bank_service.load_credit_fraud_data()\nprint(f\"加载数据集，共{df.shape[0]}行，{df.shape[1]}列\")\n\n# 1. 数据概览\nprint(\"\\n=== 数据概览 ===\")\nprint(\"数据集前5行:\")\nprint(df.head())\n\nprint(\"\\n数据集的基本信息:\")\nprint(df.info())\n\n# 检查是否有缺失值\nprint(\"\\n缺失值检查:\")\nmissing_values = df.isnull().sum()\nif missing_values.sum() > 0:\n    print(missing_values[missing_values > 0])\n    print(\"\\n需要处理缺失值...\")\nelse:\n    print(\"数据集中没有缺失值，无需填充处理\")\n\n# 检查重复值\nduplicates = df.duplicated().sum()\nprint(f\"\\n重复行数: {duplicates}\")\nif duplicates > 0:\n    print(\"需要处理重复数据...\")\n    \n# 查看数据集基本统计\nprint(\"\\n数据集的基本统计:\")\nprint(df.describe())\n\n# 2. 处理交易金额\nprint(\"\\n=== 处理交易金额 ===\")\n\n# 检查金额是否有异常值（如负值）\nnegative_amounts = (df['Amount'] < 0).sum()\nprint(f\"负值金额数量: {negative_amounts}\")\n\n# 检查金额为0的交易\nzero_amounts = (df['Amount'] == 0).sum()\nprint(f\"金额为0的交易数量: {zero_amounts}\")\n\n# 可视化金额分布\nplt.figure(figsize=(12, 5))\n\n# 原始金额分布\nplt.subplot(1, 2, 1)\nsns.histplot(df['Amount'], bins=50, kde=True)\nplt.title('原始交易金额分布')\nplt.xlabel('交易金额')\nplt.ylabel('频率')\n\n# 对数变换后的金额分布\ndf['LogAmount'] = np.log1p(df['Amount'])  # log1p处理金额为0的情况\nplt.subplot(1, 2, 2)\nsns.histplot(df['LogAmount'], bins=50, kde=True)\nplt.title('对数变换后的交易金额分布')\nplt.xlabel('log(交易金额 + 1)')\nplt.ylabel('频率')\n\nplt.tight_layout()\nplt.show()\n\n# 3. 处理时间特征\nprint(\"\\n=== 处理时间特征 ===\")\n\n# Time特征处理（通常表示自某个起始时间点的秒数）\nif 'Time' in df.columns:\n    # 基本统计\n    print(\"时间特征的基本统计:\")\n    print(df['Time'].describe())\n    \n    # 确定数据时间跨度（假设Time是按秒计）\n    time_span_days = (df['Time'].max() - df['Time'].min()) / (60 * 60 * 24)\n    print(f\"数据时间跨度约为: {time_span_days:.2f} 天\")\n    \n    # 转换为更有意义的时间特征\n    df['Hour'] = (df['Time'] / 3600) % 24  # 小时（0-23）\n    df['Day'] = (df['Time'] / (3600 * 24)) % 7  # 星期几（0-6，假设0代表星期一）\n    \n    # 创建其他有用的时间特征\n    # 将小时分为早上、下午、晚上和深夜\n    df['TimeOfDay'] = pd.cut(\n        df['Hour'], \n        bins=[0, 6, 12, 18, 24], \n        labels=['深夜', '早上', '下午', '晚上']\n    )\n    \n    # 将星期几划分为工作日和周末\n    df['IsWeekend'] = df['Day'].apply(lambda x: 1 if x >= 5 else 0)  # 假设5,6代表周末\n    \n    # 可视化时间特征\n    plt.figure(figsize=(15, 10))\n    \n    # 小时分布\n    plt.subplot(2, 2, 1)\n    sns.countplot(x='Hour', data=df, palette='viridis')\n    plt.title('交易小时分布')\n    plt.xlabel('小时')\n    plt.ylabel('交易数量')\n    plt.xticks(range(0, 24, 2))\n    \n    # 星期几分布\n    plt.subplot(2, 2, 2)\n    sns.countplot(x='Day', data=df, palette='viridis')\n    plt.title('交易星期分布')\n    plt.xlabel('星期')\n    plt.ylabel('交易数量')\n    plt.xticks(range(7), ['周一', '周二', '周三', '周四', '周五', '周六', '周日'])\n    \n    # 时间段分布\n    plt.subplot(2, 2, 3)\n    sns.countplot(x='TimeOfDay', data=df, palette='viridis')\n    plt.title('交易时间段分布')\n    plt.xlabel('时间段')\n    plt.ylabel('交易数量')\n    \n    # 工作日与周末对比\n    plt.subplot(2, 2, 4)\n    sns.countplot(x='IsWeekend', data=df, palette='viridis')\n    plt.title('工作日与周末交易对比')\n    plt.xlabel('是否周末')\n    plt.ylabel('交易数量')\n    plt.xticks([0, 1], ['工作日', '周末'])\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # 分析不同时间段的欺诈率\n    time_features = ['Hour', 'Day', 'TimeOfDay', 'IsWeekend']\n    plt.figure(figsize=(15, 10))\n    \n    for i, feature in enumerate(time_features):\n        plt.subplot(2, 2, i+1)\n        fraud_rate = df.groupby(feature)['Class'].mean() * 100\n        fraud_rate.plot(kind='bar', color='#3498db')\n        plt.title(f'{feature}的欺诈率')\n        plt.ylabel('欺诈率 (%)')\n        plt.grid(True, alpha=0.3, axis='y')\n        \n        # 为星期几和时间段添加自定义标签\n        if feature == 'Day':\n            plt.xticks(range(7), ['周一', '周二', '周三', '周四', '周五', '周六', '周日'])\n    \n    plt.tight_layout()\n    plt.show()\n\n# 4. 处理特征（V1-V28）\nprint(\"\\n=== 处理特征变量 ===\")\n\n# 假设V1-V28是PCA或其他变换后的特征\nfeature_columns = [col for col in df.columns if col.startswith('V')]\nprint(f\"特征变量数量: {len(feature_columns)}\")\n\n# 检查特征的分布\nplt.figure(figsize=(15, 10))\n\n# 随机选择6个特征进行可视化\nselected_features = np.random.choice(feature_columns, 6, replace=False)\n\nfor i, feature in enumerate(selected_features):\n    plt.subplot(2, 3, i+1)\n    sns.histplot(df[feature], bins=50, kde=True)\n    plt.title(f'{feature}分布')\n    plt.xlabel(feature)\n\nplt.tight_layout()\nplt.show()\n\n# 检查特征相关性\ncorr_matrix = df[feature_columns].corr()\n\n# 绘制相关性热图\nplt.figure(figsize=(15, 12))\nsns.heatmap(corr_matrix, cmap='coolwarm', center=0, annot=False)\nplt.title('特征变量相关性热图')\nplt.tight_layout()\nplt.show()\n\n# 检查是否有高度相关的特征对\nhighly_correlated = []\ncorr_threshold = 0.8\n\nfor i in range(len(feature_columns)):\n    for j in range(i+1, len(feature_columns)):\n        corr = corr_matrix.iloc[i, j]\n        if abs(corr) > corr_threshold:\n            highly_correlated.append((feature_columns[i], feature_columns[j], corr))\n\nif highly_correlated:\n    print(\"高度相关的特征对 (|相关系数| > 0.8):\")\n    for feat1, feat2, corr in highly_correlated:\n        print(f\"{feat1} 和 {feat2}: {corr:.4f}\")\nelse:\n    print(\"没有发现高度相关的特征对\")\n\n# 5. 特征标准化\nprint(\"\\n=== 特征标准化 ===\")\n\n# 使用StandardScaler和RobustScaler进行对比\nfeatures_to_scale = feature_columns + ['Amount']\n\n# 创建一个包含原始数据的副本\ndf_scaled = df.copy()\n\n# 应用StandardScaler\nstd_scaler = StandardScaler()\ndf_scaled[features_to_scale] = std_scaler.fit_transform(df[features_to_scale])\n\n# 应用RobustScaler（对异常值具有更强的鲁棒性）\nrobust_scaler = RobustScaler()\ndf_robust = df.copy()\ndf_robust[features_to_scale] = robust_scaler.fit_transform(df[features_to_scale])\n\n# 比较不同缩放方法的结果\nplt.figure(figsize=(15, 10))\n\n# 随机选择3个特征进行对比可视化\nselected_features_for_scaling = np.random.choice(feature_columns, 3, replace=False)\n\nfor i, feature in enumerate(selected_features_for_scaling):\n    # 原始数据分布\n    plt.subplot(3, 3, i*3+1)\n    sns.histplot(df[feature], bins=50, kde=True)\n    plt.title(f'原始 {feature}')\n    plt.xlabel(feature)\n    \n    # StandardScaler后的分布\n    plt.subplot(3, 3, i*3+2)\n    sns.histplot(df_scaled[feature], bins=50, kde=True)\n    plt.title(f'StandardScaler {feature}')\n    plt.xlabel(feature)\n    \n    # RobustScaler后的分布\n    plt.subplot(3, 3, i*3+3)\n    sns.histplot(df_robust[feature], bins=50, kde=True)\n    plt.title(f'RobustScaler {feature}')\n    plt.xlabel(feature)\n\nplt.tight_layout()\nplt.show()\n\n# 6. 检测和处理异常值\nprint(\"\\n=== 检测和处理异常值 ===\")\n\n# 使用箱线图检测异常值\nplt.figure(figsize=(15, 10))\n\n# 随机选择6个特征进行箱线图可视化\nselected_features_boxplot = np.random.choice(feature_columns, 6, replace=False)\n\nfor i, feature in enumerate(selected_features_boxplot):\n    plt.subplot(2, 3, i+1)\n    sns.boxplot(y=df[feature])\n    plt.title(f'{feature}的箱线图')\n    plt.ylabel(feature)\n\nplt.tight_layout()\nplt.show()\n\n# 计算并显示异常值比例\nprint(\"\\n使用IQR方法识别异常值的比例:\")\n\nfor feature in selected_features_boxplot:\n    q1 = df[feature].quantile(0.25)\n    q3 = df[feature].quantile(0.75)\n    iqr = q3 - q1\n    lower_bound = q1 - 1.5 * iqr\n    upper_bound = q3 + 1.5 * iqr\n    outliers = df[(df[feature] < lower_bound) | (df[feature] > upper_bound)]\n    outlier_percentage = len(outliers) / len(df) * 100\n    print(f\"{feature}: {outlier_percentage:.2f}% 的数据点被识别为异常值\")\n\n# 7. 处理类别不平衡问题\nprint(\"\\n=== 处理类别不平衡问题 ===\")\n\n# 显示类别分布\nclass_counts = df['Class'].value_counts()\nfraud_percentage = class_counts[1] / len(df) * 100\n\nprint(f\"非欺诈交易数量: {class_counts[0]}\")\nprint(f\"欺诈交易数量: {class_counts[1]}\")\nprint(f\"欺诈率: {fraud_percentage:.4f}%\")\n\n# 计算欺诈类别的权重（用于处理不平衡问题）\nfraud_weight = class_counts[0] / class_counts[1]\nprint(f\"欺诈类别的权重: {fraud_weight:.4f}\")\n\n# 8. 数据处理前后对比\nprint(\"\\n=== 数据处理前后对比 ===\")\n\n# 创建表格显示处理前后的数据形状和特征\noriginal_shape = df.shape\nprocessed_shape = df_scaled.shape\nnew_features = [col for col in df_scaled.columns if col not in df.columns]\nmodified_features = ['Amount']  # LogAmount, 标准化的特征等\n\nprint(f\"原始数据: {original_shape[0]}行, {original_shape[1]}列\")\nprint(f\"处理后的数据: {processed_shape[0]}行, {processed_shape[1]}列\")\nprint(f\"新增特征: {', '.join(new_features)}\")\nprint(f\"修改的特征: LogAmount, 标准化的特征等\")\n\n# 9. 最终数据集准备\nprint(\"\\n=== 最终数据集准备 ===\")\n\n# 选择处理后的特征\nprocessed_features = feature_columns + ['LogAmount', 'Hour', 'Day', 'IsWeekend']\n\n# 准备最终数据集\nX = df_robust[processed_features]  # 使用RobustScaler处理的数据集\ny = df['Class']\n\nprint(f\"特征数据(X)形状: {X.shape}\")\nprint(f\"目标变量(y)形状: {y.shape}\")\n\n# 将数据集划分为训练集和测试集\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n\nprint(f\"训练集: X_train: {X_train.shape}, y_train: {y_train.shape}\")\nprint(f\"测试集: X_test: {X_test.shape}, y_test: {y_test.shape}\")\n\n# 检查划分后的类别分布\nprint(\"\\n训练集类别分布:\")\nprint(y_train.value_counts())\nprint(\"\\n测试集类别分布:\")\nprint(y_test.value_counts())\n\n# 10. 数据处理总结\nprint(\"\\n=== 数据处理总结 ===\")\nprint(\"数据处理和准备完成，包含以下步骤:\")\nprint(\"1. 数据加载和初步检查\")\nprint(\"2. 处理交易金额（对数变换）\")\nprint(\"3. 处理时间特征（小时、星期、时间段等）\")\nprint(\"4. 分析特征变量的分布和相关性\")\nprint(\"5. 使用StandardScaler和RobustScaler进行特征标准化\")\nprint(\"6. 检测和分析异常值\")\nprint(\"7. 了解并处理类别不平衡问题\")\nprint(\"8. 准备最终处理后的数据集\")\nprint(\"9. 划分训练集和测试集\")\n\nprint(\"\\n下一步工作:\")\nprint(\"1. 进行更深入的特征工程，如提取异常检测特征\")\nprint(\"2. 应用不平衡数据处理技术如SMOTE、类别权重等\")\nprint(\"3. 构建和训练欺诈检测模型\")"
        },
        {
          "id": 3,
          "title": "异常特征提取",
          "description": "提取能够识别欺诈交易的关键特征",
          "example_code": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif\nfrom sklearn.ensemble import RandomForestClassifier, IsolationForest\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom app.services.docker_matplotlib_fix import configure_matplotlib_fonts\n\n# 配置字体以支持中文显示\nconfigure_matplotlib_fonts()\nfrom app.services.bank_analysis import bank_service\n\n# 加载已经预处理的欺诈数据\ndf = bank_service.load_credit_fraud_data()\nprint(f\"加载数据集，共{df.shape[0]}行，{df.shape[1]}列\")\n\n# 假设已经进行了基本的数据清洗和准备（如前面步骤所示）\n# 这里我们将重点放在特征提取和选择上\n\n# 分离特征和目标变量\nfeature_columns = [col for col in df.columns if col.startswith('V')] + ['Amount', 'Time']\nX = df[feature_columns]\ny = df['Class']\n\n# 1. 基于统计学的特征重要性\nprint(\"\\n=== 基于统计学的特征重要性 ===\")\n\n# 使用ANOVA F-value 评估特征重要性\nk_best = 15  # 选择前15个最重要的特征\nselector = SelectKBest(score_func=f_classif, k=k_best)\nX_selected = selector.fit_transform(X, y)\n\n# 获取每个特征的重要性得分\nfeature_scores = selector.scores_\nfeature_scores_dict = dict(zip(feature_columns, feature_scores))\n\n# 按重要性排序\nsorted_features = sorted(feature_scores_dict.items(), key=lambda x: x[1], reverse=True)\nprint(\"\\n基于F值的前15个重要特征:\")\nfor feature, score in sorted_features[:15]:\n    print(f\"{feature}: {score:.4f}\")\n\n# 可视化特征重要性\nplt.figure(figsize=(12, 8))\ntop_features = [f[0] for f in sorted_features[:15]]\ntop_scores = [f[1] for f in sorted_features[:15]]\n\nplt.barh(range(len(top_features)), top_scores, align='center')\nplt.yticks(range(len(top_features)), top_features)\nplt.xlabel('F值重要性分数')\nplt.title('特征重要性排名 (ANOVA F-test)')\nplt.tight_layout()\nplt.show()\n\n# 2. 基于机器学习的特征重要性\nprint(\"\\n=== 基于机器学习的特征重要性 ===\")\n\n# 数据标准化\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# 使用随机森林评估特征重要性\nrf = RandomForestClassifier(n_estimators=100, random_state=42)\nrf.fit(X_scaled, y)\n\n# 提取特征重要性\nrf_importances = rf.feature_importances_\nrf_feature_importance = dict(zip(feature_columns, rf_importances))\nsorted_rf_features = sorted(rf_feature_importance.items(), key=lambda x: x[1], reverse=True)\n\nprint(\"\\n基于随机森林的前15个重要特征:\")\nfor feature, importance in sorted_rf_features[:15]:\n    print(f\"{feature}: {importance:.6f}\")\n\n# 可视化特征重要性\nplt.figure(figsize=(12, 8))\ntop_rf_features = [f[0] for f in sorted_rf_features[:15]]\ntop_rf_importances = [f[1] for f in sorted_rf_features[:15]]\n\nplt.barh(range(len(top_rf_features)), top_rf_importances, align='center')\nplt.yticks(range(len(top_rf_features)), top_rf_features)\nplt.xlabel('随机森林特征重要性')\nplt.title('特征重要性排名 (随机森林)')\nplt.tight_layout()\nplt.show()\n\n# 3. 异常检测特征提取\nprint(\"\\n=== 异常检测特征提取 ===\")\n\n# 仅使用非欺诈交易数据训练异常检测模型\nnormal_data = df[df['Class'] == 0]\nX_normal = normal_data[feature_columns]\nX_normal_scaled = scaler.transform(X_normal)\n\n# 使用Isolation Forest检测异常\nisolation_forest = IsolationForest(n_estimators=100, contamination=0.01, random_state=42)\nisolation_forest.fit(X_normal_scaled)\n\n# 对所有数据进行预测\ndf['IF_score'] = isolation_forest.decision_function(X_scaled) * -1  # 越高越异常\ndf['IF_anomaly'] = isolation_forest.predict(X_scaled) == -1  # True表示异常\n\n# 使用Local Outlier Factor (LOF) 检测异常\nlof = LocalOutlierFactor(n_neighbors=20, contamination=0.01)\ndf['LOF_score'] = lof.fit_predict(X_scaled) == -1  # -1表示异常\ndf['LOF_score'] = df['LOF_score'].astype(int)  # 转为0/1\n\n# 使用DBSCAN进行聚类，检测异常点\ndbscan = DBSCAN(eps=0.5, min_samples=5)\ndf['DBSCAN_cluster'] = dbscan.fit_predict(X_scaled)\ndf['DBSCAN_anomaly'] = df['DBSCAN_cluster'] == -1  # -1表示异常\ndf['DBSCAN_anomaly'] = df['DBSCAN_anomaly'].astype(int)  # 转为0/1\n\n# 分析异常检测结果与欺诈的相关性\nprint(\"\\n异常检测与欺诈的关联性:\")\nfor method in ['IF_anomaly', 'LOF_score', 'DBSCAN_anomaly']:\n    corr = df[method].corr(df['Class'])\n    accuracy = (df[method] == df['Class']).mean()\n    print(f\"{method} 与欺诈的相关系数: {corr:.4f}, 一致率: {accuracy:.4f}\")\n\n# 可视化异常得分与欺诈的关系\nplt.figure(figsize=(12, 6))\n\nplt.subplot(1, 2, 1)\nsns.boxplot(x='Class', y='IF_score', data=df)\nplt.title('Isolation Forest 异常得分 vs 欺诈类别')\nplt.xlabel('欺诈类别 (0=正常, 1=欺诈)')\nplt.ylabel('异常得分 (越高越异常)')\n\nplt.subplot(1, 2, 2)\nsns.countplot(x='IF_anomaly', hue='Class', data=df, palette=['#2ecc71', '#e74c3c'])\nplt.title('Isolation Forest 预测异常 vs 欺诈类别')\nplt.xlabel('是否异常 (0=正常, 1=异常)')\nplt.ylabel('计数')\nplt.legend(['正常交易', '欺诈交易'])\n\nplt.tight_layout()\nplt.show()\n\n# 4. 特征工程 - 创建新特征\nprint(\"\\n=== 特征工程 - 创建新特征 ===\")\n\n# 创建一些可能与欺诈行为相关的新特征\n\n# 1. 时间相关特征（假设Time是以秒为单位的时间）\nif 'Time' in df.columns:\n    # 将Time转换为更有意义的特征\n    df['Hour'] = (df['Time'] / 3600) % 24\n    \n    # 将一天分为不同时段\n    df['TimeSegment'] = pd.cut(df['Hour'], \n                          bins=[0, 6, 12, 18, 24], \n                          labels=['深夜', '上午', '下午', '晚上'],\n                          include_lowest=True)\n    \n    # 计算每个时间段的欺诈率\n    time_segment_fraud_rate = df.groupby('TimeSegment')['Class'].mean()\n    print(\"\\n不同时间段的欺诈率:\")\n    print(time_segment_fraud_rate)\n    \n    # 基于时间段的欺诈风险评分\n    time_risk_mapping = time_segment_fraud_rate.to_dict()\n    df['TimeRiskScore'] = df['TimeSegment'].map(time_risk_mapping)\n\n# 2. 金额相关特征\n# 计算金额的各种变换以捕捉不同范围的模式\ndf['LogAmount'] = np.log1p(df['Amount'])  # 对数变换\ndf['AmountSqrt'] = np.sqrt(df['Amount'])  # 平方根变换\ndf['AmountBin'] = pd.qcut(df['Amount'], q=10, labels=False, duplicates='drop')  # 十分位数\n\n# 计算每个金额分箱的欺诈率\namount_bin_fraud_rate = df.groupby('AmountBin')['Class'].mean()\nprint(\"\\n不同金额区间的欺诈率:\")\nprint(amount_bin_fraud_rate)\n\n# 映射欺诈风险评分\namount_risk_mapping = amount_bin_fraud_rate.to_dict()\ndf['AmountRiskScore'] = df['AmountBin'].map(amount_risk_mapping)\n\n# 3. 异常检测特征组合\n# 组合多个异常检测算法的结果\ndf['AnomalyScore'] = df['IF_score'] + df['LOF_score'] + df['DBSCAN_anomaly']\n\n# 4. 聚合特征 - PCA分量\n# 使用PCA创建新的特征表示\npca = PCA(n_components=5)\npca_features = pca.fit_transform(X_scaled)\n\n# 将PCA特征添加到数据集\nfor i in range(pca_features.shape[1]):\n    df[f'PCA_{i+1}'] = pca_features[:, i]\n\n# 分析PCA特征与欺诈的关系\npca_corr = df[[f'PCA_{i+1}' for i in range(5)] + ['Class']].corr()['Class'].drop('Class')\nprint(\"\\nPCA特征与欺诈的相关性:\")\nprint(pca_corr)\n\n# 5. 特征选择 - 选择最终特征集\nprint(\"\\n=== 最终特征选择 ===\")\n\n# 基于前面的分析选择特征\nimportance_threshold = 0.01  # 随机森林重要性阈值\nselected_rf_features = [f for f, imp in sorted_rf_features if imp > importance_threshold]\n\n# 新创建的特征\nengineered_features = ['IF_score', 'LOF_score', 'DBSCAN_anomaly', 'AnomalyScore']\nif 'TimeRiskScore' in df.columns:\n    engineered_features.append('TimeRiskScore')\nengineered_features.extend(['AmountRiskScore', 'LogAmount'])\nengineered_features.extend([f'PCA_{i+1}' for i in range(3)])  # 前3个PCA分量\n\n# 合并选择的特征\nfinal_features = selected_rf_features + engineered_features\n\n# 去除重复\nfinal_features = list(dict.fromkeys(final_features))\n\nprint(f\"\\n最终选择的特征集 ({len(final_features)}个特征):\")\nfor i, feature in enumerate(final_features):\n    print(f\"{i+1}. {feature}\")\n\n# 准备最终数据集\nX_final = df[final_features]\ny_final = df['Class']\n\nprint(f\"\\n最终数据集形状: {X_final.shape}\")\n\n# 6. 特征重要性可视化\nprint(\"\\n=== 最终特征重要性可视化 ===\")\n\n# 使用最终特征集训练随机森林\nrf_final = RandomForestClassifier(n_estimators=100, random_state=42)\nrf_final.fit(X_final, y_final)\n\n# 提取特征重要性\nfinal_importances = rf_final.feature_importances_\nfinal_importance_dict = dict(zip(final_features, final_importances))\nsorted_final_features = sorted(final_importance_dict.items(), key=lambda x: x[1], reverse=True)\n\n# 可视化最终特征重要性\nplt.figure(figsize=(14, 10))\ntop_n = min(20, len(final_features))  # 最多显示20个特征\ntop_final_features = [f[0] for f in sorted_final_features[:top_n]]\ntop_final_importances = [f[1] for f in sorted_final_features[:top_n]]\n\nplt.barh(range(len(top_final_features)), top_final_importances, align='center')\nplt.yticks(range(len(top_final_features)), top_final_features)\nplt.xlabel('最终特征重要性')\nplt.title('最终选择特征的重要性排名')\nplt.tight_layout()\nplt.show()\n\n# 7. 特征相关性分析\nprint(\"\\n=== 特征相关性分析 ===\")\n\n# 分析最终特征之间的相关性\ncorr_matrix = X_final.corr()\n\n# 绘制相关性热图\nplt.figure(figsize=(16, 14))\nsns.heatmap(corr_matrix, annot=False, cmap='coolwarm', center=0, \n            vmin=-1, vmax=1, square=True, linewidths=.5, cbar_kws={\"shrink\": .8})\nplt.title('最终特征之间的相关性热图')\nplt.tight_layout()\nplt.show()\n\n# 8. 总结\nprint(\"\\n=== 异常特征提取总结 ===\")\nprint(\"完成了以下特征工程和选择步骤:\")\nprint(\"1. 使用ANOVA F-test评估原始特征的统计学重要性\")\nprint(\"2. 使用随机森林评估特征重要性\")\nprint(\"3. 应用多种异常检测算法创建异常分数特征\")\nprint(\"4. 创建时间和金额相关的风险评分特征\")\nprint(\"5. 使用PCA创建降维特征\")\nprint(\"6. 整合多种技术选择最终特征集\")\n\nprint(\"\\n这些特征将用于后续的欺诈检测模型构建。特征工程是欺诈检测中至关重要的一步，\")\nprint(\"因为它能捕捉欺诈行为的微妙模式，提高模型性能。\")"
        },
        {
          "id": 4,
          "title": "欺诈检测建模",
          "description": "构建欺诈检测模型，解决不平衡数据问题",
           "example_code": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, StratifiedKFold\nfrom sklearn.metrics import classification_report, confusion_matrix, precision_recall_curve, auc\nfrom sklearn.metrics import roc_curve, roc_auc_score, precision_score, recall_score, f1_score, accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neural_network import MLPClassifier\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.pipeline import Pipeline as ImbPipeline\nfrom sklearn.pipeline import Pipeline\nfrom app.services.docker_matplotlib_fix import configure_matplotlib_fonts\n\n# 配置字体以支持中文显示\nconfigure_matplotlib_fonts()\nfrom app.services.bank_analysis import bank_service\n\n# 加载已处理好的欺诈数据（假设前面步骤已完成特征工程）\ndf = bank_service.load_credit_fraud_data()\nprint(f\"加载数据集，共{df.shape[0]}行，{df.shape[1]}列\")\n\n# 1. 数据准备\nprint(\"\\n=== 数据准备 ===\")\n\n# 检查类别分布\nclass_counts = df['Class'].value_counts()\nprint(\"类别分布:\")\nfor label, count in class_counts.items():\n    percentage = count / len(df) * 100\n    fraud_label = \"欺诈交易\" if label == 1 else \"正常交易\"\n    print(f\"{fraud_label}: {count} ({percentage:.4f}%)\")\n\n# 基于前一步骤选择特征\n# 这里演示使用，假设我们选择了原始V特征和一些工程特征\nbase_features = [col for col in df.columns if col.startswith('V')]\nengineered_features = ['Amount', 'LogAmount']\n\n# 如果前一步创建了这些特征，则使用它们\nif 'Hour' in df.columns:\n    engineered_features.append('Hour')\nif 'AnomalyScore' in df.columns:\n    engineered_features.append('AnomalyScore')\nif 'TimeRiskScore' in df.columns:\n    engineered_features.append('TimeRiskScore')\n\n# 组合所有特征\nfeatures = base_features + engineered_features\nprint(f\"\\n使用{len(features)}个特征训练模型\")\n\n# 分离特征和目标变量\nX = df[features]\ny = df['Class']\n\n# 拆分训练集和测试集\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n\nprint(f\"训练集: {X_train.shape[0]}行, 测试集: {X_test.shape[0]}行\")\nprint(f\"训练集欺诈比例: {y_train.mean()*100:.4f}%\")\nprint(f\"测试集欺诈比例: {y_test.mean()*100:.4f}%\")\n\n# 2. 处理不平衡数据问题\nprint(\"\\n=== 处理不平衡数据 ===\")\n\n# 2.1 类别加权\nweight_for_0 = 1.0\nweight_for_1 = class_counts[0] / class_counts[1]\nclass_weights = {0: weight_for_0, 1: weight_for_1}\nprint(f\"类别权重 - 正常交易: {weight_for_0}, 欺诈交易: {weight_for_1:.4f}\")\n\n# 2.2 SMOTE过采样\nprint(\"\\n应用SMOTE过采样技术...\")\nsmote = SMOTE(random_state=42)\nX_resampled, y_resampled = smote.fit_resample(X_train, y_train)\nprint(f\"SMOTE后 - 训练集: {X_resampled.shape[0]}行\")\nprint(f\"SMOTE后 - 欺诈比例: {y_resampled.mean()*100:.4f}%\")\n\n# 2.3 随机欠采样\nprint(\"\\n应用随机欠采样技术...\")\nunder_sampler = RandomUnderSampler(random_state=42)\nX_undersampled, y_undersampled = under_sampler.fit_resample(X_train, y_train)\nprint(f\"欠采样后 - 训练集: {X_undersampled.shape[0]}行\")\nprint(f\"欠采样后 - 欺诈比例: {y_undersampled.mean()*100:.4f}%\")\n\n# 3. 模型评估函数\nprint(\"\\n=== 定义模型评估函数 ===\")\n\ndef evaluate_model(model, X_test, y_test, model_name=\"模型\", threshold=0.5):\n    \"\"\"评估模型性能\"\"\"\n    # 预测概率\n    if hasattr(model, \"predict_proba\"):\n        y_proba = model.predict_proba(X_test)[:, 1]\n    else:  # 如果模型不支持predict_proba，使用决策函数\n        y_proba = model.decision_function(X_test)\n        y_proba = (y_proba - y_proba.min()) / (y_proba.max() - y_proba.min())\n    \n    # 基于概率阈值的预测\n    y_pred = (y_proba >= threshold).astype(int)\n    \n    # 计算评估指标\n    accuracy = accuracy_score(y_test, y_pred)\n    precision = precision_score(y_test, y_pred)\n    recall = recall_score(y_test, y_pred)\n    f1 = f1_score(y_test, y_pred)\n    auc_score = roc_auc_score(y_test, y_proba)\n    \n    # 混淆矩阵\n    cm = confusion_matrix(y_test, y_pred)\n    tn, fp, fn, tp = cm.ravel()\n    \n    # 输出结果\n    print(f\"\\n{model_name}评估结果:\")\n    print(f\"准确率: {accuracy:.4f}\")\n    print(f\"精确率: {precision:.4f}\")\n    print(f\"召回率: {recall:.4f}\")\n    print(f\"F1分数: {f1:.4f}\")\n    print(f\"AUC: {auc_score:.4f}\")\n    \n    print(\"\\n混淆矩阵:\")\n    print(f\"真负例(TN): {tn}  |  假正例(FP): {fp}\")\n    print(f\"假负例(FN): {fn}  |  真正例(TP): {tp}\")\n    \n    # 成本计算（假设欺诈交易未检出的成本远高于误报的成本）\n    cost_fn = 10  # 假设漏检一起欺诈交易的成本是误报10倍\n    cost_fp = 1\n    total_cost = fn * cost_fn + fp * cost_fp\n    print(f\"总成本: {total_cost} (FN成本: {fn*cost_fn}, FP成本: {fp*cost_fp})\")\n    \n    return {\n        \"model\": model,\n        \"name\": model_name,\n        \"accuracy\": accuracy,\n        \"precision\": precision,\n        \"recall\": recall,\n        \"f1\": f1,\n        \"auc\": auc_score,\n        \"confusion_matrix\": cm,\n        \"y_proba\": y_proba,\n        \"y_pred\": y_pred,\n        \"threshold\": threshold,\n        \"cost\": total_cost\n    }\n\n# 4. 训练多种基础模型\nprint(\"\\n=== 训练基础模型 ===\")\n\n# 4.1 标准化数据\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\nX_resampled_scaled = scaler.transform(X_resampled)\nX_undersampled_scaled = scaler.transform(X_undersampled)\n\n# 4.2 创建不同的模型 - 使用原始不平衡数据 + 类别权重\nprint(\"\\n使用原始不平衡数据 + 类别权重训练模型\")\n\n# 逻辑回归\nlogistic = LogisticRegression(class_weight=class_weights, max_iter=1000, random_state=42)\nlogistic.fit(X_train_scaled, y_train)\nresults_logistic = evaluate_model(logistic, X_test_scaled, y_test, \"逻辑回归 (类别权重)\")\n\n# 随机森林\nrf = RandomForestClassifier(class_weight=class_weights, n_estimators=100, random_state=42)\nrf.fit(X_train_scaled, y_train)\nresults_rf = evaluate_model(rf, X_test_scaled, y_test, \"随机森林 (类别权重)\")\n\n# 4.3 使用SMOTE过采样数据训练模型\nprint(\"\\n使用SMOTE过采样数据训练模型\")\n\n# 支持向量机\nsvm = SVC(gamma='auto', probability=True, random_state=42)\nsvm.fit(X_resampled_scaled, y_resampled)\nresults_svm = evaluate_model(svm, X_test_scaled, y_test, \"SVM (SMOTE)\")\n\n# 梯度提升树\ngbc = GradientBoostingClassifier(n_estimators=100, random_state=42)\ngbc.fit(X_resampled_scaled, y_resampled)\nresults_gbc = evaluate_model(gbc, X_test_scaled, y_test, \"梯度提升树 (SMOTE)\")\n\n# 4.4 使用欠采样数据训练模型\nprint(\"\\n使用欠采样数据训练模型\")\n\n# 多层感知机\nmlp = MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=300, random_state=42)\nmlp.fit(X_undersampled_scaled, y_undersampled)\nresults_mlp = evaluate_model(mlp, X_test_scaled, y_test, \"多层感知机 (欠采样)\")\n\n# 5. 集成多个模型\nprint(\"\\n=== 集成多个模型 ===\")\n\n# 创建投票分类器\nvoting_clf = VotingClassifier(\n    estimators=[\n        ('logistic', logistic),\n        ('rf', rf),\n        ('svm', svm),\n        ('gbc', gbc)\n    ],\n    voting='soft'  # 使用概率而不是硬投票\n)\n\n# 在原始训练数据上拟合\nvoting_clf.fit(X_train_scaled, y_train)\nresults_voting = evaluate_model(voting_clf, X_test_scaled, y_test, \"投票分类器 (集成)\")\n\n# 6. 特征重要性分析\nprint(\"\\n=== 特征重要性分析 ===\")\n\n# 获取随机森林的特征重要性\nfeature_importances = rf.feature_importances_\n\n# 创建特征重要性数据框\nimportance_df = pd.DataFrame({\n    '特征': features,\n    '重要性': feature_importances\n})\nimportance_df = importance_df.sort_values('重要性', ascending=False)\n\n# 显示前15个最重要的特征\nprint(\"\\n随机森林特征重要性 (前15个):\")\nprint(importance_df.head(15))\n\n# 可视化特征重要性\nplt.figure(figsize=(12, 8))\nsns.barplot(x='重要性', y='特征', data=importance_df.head(15))\nplt.title('欺诈检测模型 - 特征重要性')\nplt.tight_layout()\nplt.show()\n\n# 7. 交叉验证评估最佳模型\nprint(\"\\n=== 交叉验证评估最佳模型 ===\")\n\n# 假设随机森林是最佳模型（根据前面的评估）\nbest_model = rf\n\n# 定义交叉验证策略 - 使用分层K折交叉验证\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n# 对精确率、召回率和F1分数进行交叉验证\ncv_precision = cross_val_score(best_model, X_train_scaled, y_train, cv=skf, scoring='precision')\ncv_recall = cross_val_score(best_model, X_train_scaled, y_train, cv=skf, scoring='recall')\ncv_f1 = cross_val_score(best_model, X_train_scaled, y_train, cv=skf, scoring='f1')\ncv_auc = cross_val_score(best_model, X_train_scaled, y_train, cv=skf, scoring='roc_auc')\n\nprint(\"随机森林交叉验证结果:\")\nprint(f\"精确率: {cv_precision.mean():.4f} ± {cv_precision.std():.4f}\")\nprint(f\"召回率: {cv_recall.mean():.4f} ± {cv_recall.std():.4f}\")\nprint(f\"F1分数: {cv_f1.mean():.4f} ± {cv_f1.std():.4f}\")\nprint(f\"AUC: {cv_auc.mean():.4f} ± {cv_auc.std():.4f}\")\n\n# 8. 成本敏感学习\nprint(\"\\n=== 成本敏感学习 ===\")\n\n# 创建一个自定义的成本敏感管道\n# 先使用SMOTE过采样，然后应用随机森林分类器\ncost_sensitive_pipeline = ImbPipeline([\n    ('sampling', SMOTE(random_state=42)),\n    ('scaler', StandardScaler()),\n    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))\n])\n\n# 拟合管道\ncost_sensitive_pipeline.fit(X_train, y_train)\n\n# 评估成本敏感模型\nresults_cost = evaluate_model(\n    cost_sensitive_pipeline, X_test, y_test, \"成本敏感随机森林 (SMOTE)\"\n)\n\n# 9. 可视化模型比较\nprint(\"\\n=== 可视化模型比较 ===\")\n\n# 收集所有模型结果\nmodels = [results_logistic, results_rf, results_svm, results_gbc, \n          results_mlp, results_voting, results_cost]\nmodel_names = [model['name'] for model in models]\n\n# 准备比较数据\nmetrics = ['precision', 'recall', 'f1', 'auc']\nmetric_names = ['精确率', '召回率', 'F1分数', 'AUC']\ncomparison_data = []\n\nfor model in models:\n    for metric, metric_name in zip(metrics, metric_names):\n        comparison_data.append({\n            '模型': model['name'],\n            '指标': metric_name,\n            '值': model[metric]\n        })\n\ncomparison_df = pd.DataFrame(comparison_data)\n\n# 可视化比较\nplt.figure(figsize=(14, 8))\nsns.barplot(x='模型', y='值', hue='指标', data=comparison_df)\nplt.title('欺诈检测模型性能比较')\nplt.xticks(rotation=45)\nplt.ylim(0, 1)\nplt.tight_layout()\nplt.show()\n\n# 比较各模型的成本\ncosts = [model['cost'] for model in models]\n\nplt.figure(figsize=(12, 6))\nsns.barplot(x=model_names, y=costs)\nplt.title('欺诈检测模型成本比较 (越低越好)')\nplt.ylabel('成本 (FN * 10 + FP)')\nplt.xlabel('模型')\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n\n# 10. ROC曲线比较\nplt.figure(figsize=(12, 8))\n\n# 为每个模型绘制ROC曲线\nfor model in models:\n    fpr, tpr, _ = roc_curve(y_test, model['y_proba'])\n    plt.plot(fpr, tpr, label=f\"{model['name']} (AUC = {model['auc']:.4f})\")\n\n# 添加随机猜测的基准线\nplt.plot([0, 1], [0, 1], 'k--', label='随机猜测')\nplt.xlabel('假正例率 (FPR)')\nplt.ylabel('真正例率 (TPR)')\nplt.title('ROC曲线比较')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n\n# 11. 精确率-召回率曲线比较\nplt.figure(figsize=(12, 8))\n\n# 为每个模型绘制PR曲线\nfor model in models:\n    precision_curve, recall_curve, _ = precision_recall_curve(y_test, model['y_proba'])\n    plt.plot(recall_curve, precision_curve, label=f\"{model['name']} (F1 = {model['f1']:.4f})\")\n\n# 添加基准线（数据集中正例的比例）\nbaseline = y_test.mean()\nplt.axhline(y=baseline, color='k', linestyle='--', label=f'基准 (正例比例 = {baseline:.4f})')\n\nplt.xlabel('召回率')\nplt.ylabel('精确率')\nplt.title('精确率-召回率曲线比较')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n\n# 12. 总结最佳模型\nprint(\"\\n=== 欺诈检测模型总结 ===\")\n\n# 确定最佳模型（基于F1分数）\nbest_model_idx = np.argmax([model['f1'] for model in models])\nbest_model_result = models[best_model_idx]\n\nprint(f\"最佳模型: {best_model_result['name']}\")\nprint(f\"F1分数: {best_model_result['f1']:.4f}\")\nprint(f\"精确率: {best_model_result['precision']:.4f}\")\nprint(f\"召回率: {best_model_result['recall']:.4f}\")\nprint(f\"AUC: {best_model_result['auc']:.4f}\")\nprint(f\"总成本: {best_model_result['cost']}\")\n\nprint(\"\\n欺诈检测模型构建的关键点:\")\nprint(\"1. 数据极度不平衡，需要特殊处理技术\")\nprint(\"2. 成本敏感学习对欺诈检测至关重要\")\nprint(\"3. 模型的选择应基于业务需求（如成本最小化或高召回率）\")\nprint(\"4. 集成多个模型可以提高整体性能\")\nprint(\"5. 特征工程和选择对模型性能有显著影响\")\n\nprint(\"\\n下一步工作:\")\nprint(\"1. 进一步调优最佳模型的超参数\")\nprint(\"2. 探索不同阈值对模型性能的影响\")\nprint(\"3. 分析误分类的案例，找出改进方向\")\nprint(\"4. 实施在线学习策略，适应欺诈模式的变化\")"
        },
        {
          "id": 5,
          "title": "模型调优与评估",
          "description": "使用精确率-召回率和AUC评估模型性能",
          "example_code": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, cross_val_score, StratifiedKFold\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\nfrom sklearn.metrics import confusion_matrix, classification_report, roc_curve, precision_recall_curve\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.svm import SVC\nfrom imblearn.over_sampling import SMOTE\nimport matplotlib as mpl\nfrom app.services.docker_matplotlib_fix import configure_matplotlib_fonts\n\n# 配置字体以支持中文显示\nconfigure_matplotlib_fonts()\nfrom app.services.bank_analysis import bank_service\n\n# 加载欺诈数据\ndf = bank_service.load_credit_fraud_data()\nprint(f\"加载数据集，共{df.shape[0]}行，{df.shape[1]}列\")\n\n# 1. 数据准备\nprint(\"\\n=== 数据准备 ===\")\n\n# 选择特征\nfeature_columns = [col for col in df.columns if col.startswith('V')] + ['Amount']\nX = df[feature_columns]\ny = df['Class']\n\n# 标准化特征\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# 划分训练集和测试集\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42, stratify=y)\n\n# 处理类别不平衡\nsmote = SMOTE(random_state=42)\nX_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n\n# 2. 模型优化 - 逻辑回归\nprint(\"\\n=== 逻辑回归模型优化 ===\")\n\n# 定义参数网格\nparam_grid_lr = {\n    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n    'penalty': ['l1', 'l2'],\n    'solver': ['liblinear', 'saga'],\n    'class_weight': [None, 'balanced']\n}\n\n# 使用网格搜索找到最佳参数\ngrid_lr = GridSearchCV(\n    LogisticRegression(max_iter=1000, random_state=42),\n    param_grid_lr,\n    cv=5,\n    scoring='f1',\n    n_jobs=-1\n)\ngrid_lr.fit(X_train_resampled, y_train_resampled)\n\n# 输出最佳参数\nprint(f\"最佳参数: {grid_lr.best_params_}\")\nprint(f\"最佳F1分数: {grid_lr.best_score_:.4f}\")\n\n# 使用最佳参数的模型\nbest_lr = grid_lr.best_estimator_\n\n# 3. 模型优化 - 随机森林\nprint(\"\\n=== 随机森林模型优化 ===\")\n\n# 使用随机搜索而不是网格搜索来加速\nparam_dist_rf = {\n    'n_estimators': [50, 100, 200],\n    'max_depth': [None, 10, 20, 30],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 4],\n    'max_features': ['auto', 'sqrt'],\n    'class_weight': [None, 'balanced', 'balanced_subsample']\n}\n\nrandom_search_rf = RandomizedSearchCV(\n    RandomForestClassifier(random_state=42),\n    param_distributions=param_dist_rf,\n    n_iter=20,\n    cv=5,\n    scoring='f1',\n    n_jobs=-1,\n    random_state=42\n)\nrandom_search_rf.fit(X_train_resampled, y_train_resampled)\n\n# 输出最佳参数\nprint(f\"最佳参数: {random_search_rf.best_params_}\")\nprint(f\"最佳F1分数: {random_search_rf.best_score_:.4f}\")\n\n# 使用最佳参数的模型\nbest_rf = random_search_rf.best_estimator_\n\n# 4. 交叉验证评估\nprint(\"\\n=== 交叉验证评估 ===\")\n\n# 定义要评估的模型\nmodels = {\n    '逻辑回归': best_lr,\n    '随机森林': best_rf,\n    '梯度提升树': GradientBoostingClassifier(random_state=42)\n}\n\n# 定义交叉验证策略\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n# 评估每个模型\ncv_results = {}\nfor name, model in models.items():\n    cv_scores = cross_val_score(model, X_train_resampled, y_train_resampled, cv=skf, scoring='f1')\n    cv_results[name] = {\n        'mean': cv_scores.mean(),\n        'std': cv_scores.std(),\n        'scores': cv_scores\n    }\n    print(f\"{name}: F1 = {cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")\n\n# 5. 最终模型评估\nprint(\"\\n=== 最终模型评估 ===\")\n\n# 在测试集上评估每个模型\ntest_results = {}\nfor name, model in models.items():\n    # 在重采样的训练集上训练模型\n    model.fit(X_train_resampled, y_train_resampled)\n    \n    # 在测试集上预测\n    y_pred = model.predict(X_test)\n    y_prob = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else None\n    \n    # 计算评估指标\n    accuracy = accuracy_score(y_test, y_pred)\n    precision = precision_score(y_test, y_pred)\n    recall = recall_score(y_test, y_pred)\n    f1 = f1_score(y_test, y_pred)\n    auc = roc_auc_score(y_test, y_prob) if y_prob is not None else None\n    \n    # 保存结果\n    test_results[name] = {\n        'accuracy': accuracy,\n        'precision': precision,\n        'recall': recall,\n        'f1': f1,\n        'auc': auc,\n        'y_pred': y_pred,\n        'y_prob': y_prob\n    }\n    \n    # 输出评估结果\n    print(f\"\\n{name}评估结果:\")\n    print(f\"准确率: {accuracy:.4f}\")\n    print(f\"精确率: {precision:.4f}\")\n    print(f\"召回率: {recall:.4f}\")\n    print(f\"F1分数: {f1:.4f}\")\n    if auc:\n        print(f\"ROC AUC: {auc:.4f}\")\n    \n    # 混淆矩阵\n    cm = confusion_matrix(y_test, y_pred)\n    tn, fp, fn, tp = cm.ravel()\n    print(f\"\\n混淆矩阵:\")\n    print(f\"真负例(TN): {tn}\")\n    print(f\"假正例(FP): {fp}\")\n    print(f\"假负例(FN): {fn}\")\n    print(f\"真正例(TP): {tp}\")\n\n# 6. 可视化比较\nprint(\"\\n=== 模型性能比较 ===\")\n\n# 创建比较表格\ncomparison_df = pd.DataFrame({\n    '模型': list(models.keys()),\n    '准确率': [test_results[name]['accuracy'] for name in models],\n    '精确率': [test_results[name]['precision'] for name in models],\n    '召回率': [test_results[name]['recall'] for name in models],\n    'F1分数': [test_results[name]['f1'] for name in models],\n    'ROC AUC': [test_results[name]['auc'] for name in models if test_results[name]['auc'] is not None]\n})\nprint(comparison_df)\n\n# 绘制ROC曲线\nplt.figure(figsize=(12, 6))\n\n# ROC曲线\nplt.subplot(1, 2, 1)\nfor name in models.keys():\n    if test_results[name]['y_prob'] is not None:\n        fpr, tpr, _ = roc_curve(y_test, test_results[name]['y_prob'])\n        auc = test_results[name]['auc']\n        plt.plot(fpr, tpr, label=f'{name} (AUC = {auc:.3f})')\n\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlabel('假正例率 (FPR)')\nplt.ylabel('真正例率 (TPR)')\nplt.title('ROC曲线比较')\nplt.legend()\n\n# 精确率-召回率曲线\nplt.subplot(1, 2, 2)\nfor name in models.keys():\n    if test_results[name]['y_prob'] is not None:\n        precision_values, recall_values, _ = precision_recall_curve(y_test, test_results[name]['y_prob'])\n        plt.plot(recall_values, precision_values, label=f'{name}')\n\nplt.xlabel('召回率')\nplt.ylabel('精确率')\nplt.title('精确率-召回率曲线比较')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n\n# 7. 成本敏感评估\nprint(\"\\n=== 成本敏感评估 ===\")\n\n# 假设错误分类的成本\nfn_cost = 100  # 漏检欺诈的成本\nfp_cost = 10   # 误报的成本\n\n# 计算每个模型的总成本\nfor name in models.keys():\n    cm = confusion_matrix(y_test, test_results[name]['y_pred'])\n    tn, fp, fn, tp = cm.ravel()\n    total_cost = (fn * fn_cost) + (fp * fp_cost)\n    avg_cost_per_transaction = total_cost / len(y_test)\n    print(f\"{name}的总成本: {total_cost}，平均每笔交易成本: {avg_cost_per_transaction:.2f}\")\n\n# 8. 不同阈值下的模型性能\n# 选择最佳模型进行阈值分析\nbest_model_name = max(test_results, key=lambda x: test_results[x]['f1'])\nbest_model = models[best_model_name]\nbest_y_prob = test_results[best_model_name]['y_prob']\n\nif best_y_prob is not None:\n    thresholds = np.arange(0.1, 1.0, 0.1)\n    threshold_results = []\n    \n    for threshold in thresholds:\n        y_pred_threshold = (best_y_prob >= threshold).astype(int)\n        precision = precision_score(y_test, y_pred_threshold)\n        recall = recall_score(y_test, y_pred_threshold)\n        f1 = f1_score(y_test, y_pred_threshold)\n        cm = confusion_matrix(y_test, y_pred_threshold)\n        tn, fp, fn, tp = cm.ravel()\n        cost = (fn * fn_cost) + (fp * fp_cost)\n        \n        threshold_results.append({\n            '阈值': threshold,\n            '精确率': precision,\n            '召回率': recall,\n            'F1分数': f1,\n            '成本': cost\n        })\n    \n    threshold_df = pd.DataFrame(threshold_results)\n    print(f\"\\n{best_model_name}在不同阈值下的性能:\")\n    print(threshold_df)\n    \n    # 找出F1分数最高的阈值\n    best_f1_threshold = threshold_df.loc[threshold_df['F1分数'].idxmax(), '阈值']\n    # 找出成本最低的阈值\n    best_cost_threshold = threshold_df.loc[threshold_df['成本'].idxmin(), '阈值']\n    \n    print(f\"\\nF1分数最高的阈值: {best_f1_threshold}\")\n    print(f\"成本最低的阈值: {best_cost_threshold}\")\n\n# 9. 模型调优总结\nprint(\"\\n=== 模型调优总结 ===\")\n\n# 性能最好的模型\nbest_model_name = max(test_results, key=lambda x: test_results[x]['f1'])\nbest_f1 = test_results[best_model_name]['f1']\nbest_recall = test_results[best_model_name]['recall']\nbest_precision = test_results[best_model_name]['precision']\n\nprint(f\"性能最佳的模型是: {best_model_name}，F1分数: {best_f1:.4f}\")\nprint(f\"其精确率: {best_precision:.4f}，召回率: {best_recall:.4f}\")\n\n# 调优建议\nprint(\"\\n模型调优建议:\")\nprint(\"1. 特征工程是提升欺诈检测模型性能的关键\")\nprint(\"2. 类别不平衡处理对于提高模型的召回率至关重要\")\nprint(\"3. 阈值选择应基于业务目标，权衡精确率和召回率\")\nprint(\"4. 定期重新训练模型以适应欺诈模式的变化\")\nprint(\"5. 考虑使用集成方法，综合多个模型的优势\");"
        },
        {
          "id": 6,
          "title": "欺诈预警系统",
          "description": "设计实时欺诈交易监控与预警系统架构"
        }
      ]
    },
    {
      "id": 4,
      "title": "股票技术指标分析",
      "description": "计算和分析股票技术指标，包括移动平均线、MACD、RSI等",
      "category": "security",
      "difficulty_level": 2,
      "estimated_duration": 90,
      "prerequisites": "基本的金融和股票知识",
      "data_source": "股票历史价格数据",
      "steps": [
        {
          "id": 1,
          "title": "数据获取",
          "description": "获取股票历史价格数据"
        },
        {
          "id": 2,
          "title": "趋势指标计算",
          "description": "计算移动平均线、MACD等趋势指标"
        },
        {
          "id": 3,
          "title": "摆动指标计算",
          "description": "计算RSI、KDJ等摆动指标"
        },
        {
          "id": 4,
          "title": "指标可视化",
          "description": "绘制技术指标和价格图表"
        },
        {
          "id": 5,
          "title": "交易信号分析",
          "description": "分析指标产生的交易信号"
        },
        {
          "id": 6,
          "title": "回测策略",
          "description": "回测基于技术指标的交易策略"
        }
      ]
    },
    {
      "id": 5,
      "title": "投资组合优化",
      "description": "使用马科维茨模型和夏普比率优化股票投资组合",
      "category": "security",
      "difficulty_level": 4,
      "estimated_duration": 150,
      "prerequisites": "投资理论和基础统计知识",
      "data_source": "多只股票历史收益率数据",
      "steps": [
        {
          "id": 1,
          "title": "数据准备",
          "description": "获取多只股票的历史价格和收益率"
        },
        {
          "id": 2,
          "title": "风险和收益计算",
          "description": "计算各股票的预期收益和风险"
        },
        {
          "id": 3,
          "title": "相关性分析",
          "description": "分析股票间的相关性"
        },
        {
          "id": 4,
          "title": "效率前沿构建",
          "description": "构建投资组合的效率前沿"
        },
        {
          "id": 5,
          "title": "最优组合确定",
          "description": "确定最优夏普比率的投资组合"
        },
        {
          "id": 6,
          "title": "投资组合评估",
          "description": "评估最优投资组合的表现"
        }
      ]
    },
    {
      "id": 6,
      "title": "车险索赔率影响因素分析",
      "description": "分析影响车险索赔率的因素，建立预测模型",
      "category": "insurance",
      "difficulty_level": 3,
      "estimated_duration": 120,
      "prerequisites": "基本的统计分析和机器学习知识",
      "data_source": "车险客户和索赔数据",
      "steps": [
        {
          "id": 1,
          "title": "数据探索",
          "description": "探索车险客户和索赔数据"
        },
        {
          "id": 2,
          "title": "索赔率计算",
          "description": "计算不同因素下的索赔率"
        },
        {
          "id": 3,
          "title": "特征重要性分析",
          "description": "分析影响索赔率的重要因素"
        },
        {
          "id": 4,
          "title": "预测模型构建",
          "description": "构建索赔金额预测模型"
        },
        {
          "id": 5,
          "title": "风险分群",
          "description": "对客户进行风险分群"
        },
        {
          "id": 6,
          "title": "保险定价策略",
          "description": "基于分析结果制定保险定价策略"
        }
      ]
    },
    {
      "id": 7,
      "title": "医疗保险精准营销分析",
      "description": "使用客户分群和预测模型进行医疗保险精准营销",
      "category": "insurance",
      "difficulty_level": 3,
      "estimated_duration": 120,
      "prerequisites": "基本的数据分析和机器学习知识",
      "data_source": "医疗保险客户数据",
      "steps": [
        {
          "id": 1,
          "title": "数据探索",
          "description": "探索医疗保险客户数据"
        },
        {
          "id": 2,
          "title": "客户分群",
          "description": "使用K-means对客户进行分群"
        },
        {
          "id": 3,
          "title": "购买意向预测",
          "description": "构建购买意向预测模型"
        },
        {
          "id": 4,
          "title": "分群特征分析",
          "description": "分析各群体的特征和行为"
        },
        {
          "id": 5,
          "title": "营销建议生成",
          "description": "为各客户群体生成营销建议"
        },
        {
          "id": 6,
          "title": "营销效果评估",
          "description": "评估针对不同群体的营销策略效果"
        }
      ]
    }
  ]
}